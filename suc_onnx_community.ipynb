{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fff8667bb2444638b54c70ad32548cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae93b2bb7bb742c9abd103ab2516716c",
              "IPY_MODEL_e51a6d8d3ac24bcaa7e588918cd9bdc0",
              "IPY_MODEL_fd11672323b341be8263d68b713ddc16"
            ],
            "layout": "IPY_MODEL_0cc89d7deaad4c588edef57dbd50eac2"
          }
        },
        "ae93b2bb7bb742c9abd103ab2516716c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd4f175fa6cd4c4b9e9fc2e1be19b98e",
            "placeholder": "​",
            "style": "IPY_MODEL_007605474e2942fea923309f4b8a7338",
            "value": "config.json: 100%"
          }
        },
        "e51a6d8d3ac24bcaa7e588918cd9bdc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96eae44618cb4ef0883b2af8b146b1d3",
            "max": 834,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba8aa819b95440ae8316a607091643d4",
            "value": 834
          }
        },
        "fd11672323b341be8263d68b713ddc16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_402da257f5af4bb7b7d70166193db138",
            "placeholder": "​",
            "style": "IPY_MODEL_5f1d87aaa860486b909981d0deac0e49",
            "value": " 834/834 [00:00&lt;00:00, 8.84kB/s]"
          }
        },
        "0cc89d7deaad4c588edef57dbd50eac2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd4f175fa6cd4c4b9e9fc2e1be19b98e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "007605474e2942fea923309f4b8a7338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96eae44618cb4ef0883b2af8b146b1d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba8aa819b95440ae8316a607091643d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "402da257f5af4bb7b7d70166193db138": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f1d87aaa860486b909981d0deac0e49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87e978067ef8427684baa2ae015052e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f1788bf98be4331b734d76d02c14a56",
              "IPY_MODEL_d9376ab6565b444ca12c7d43ee5f2ec9",
              "IPY_MODEL_9d1a21779d74461fab410fcd40b98277"
            ],
            "layout": "IPY_MODEL_d05e42ae115b4da8869580fd40820967"
          }
        },
        "2f1788bf98be4331b734d76d02c14a56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a74702e6b1c94ded82ee41477369c0e0",
            "placeholder": "​",
            "style": "IPY_MODEL_fe0eb5b2343b46de8b16b5d3148d58ad",
            "value": "model.onnx: 100%"
          }
        },
        "d9376ab6565b444ca12c7d43ee5f2ec9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f409cd63563c42379c251e8dd3b3d2a9",
            "max": 267881125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a37e1e1768d431d853f8d0a12d4dd6c",
            "value": 267881125
          }
        },
        "9d1a21779d74461fab410fcd40b98277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71921e4bdc7142c9aa0e49a900f6214e",
            "placeholder": "​",
            "style": "IPY_MODEL_4c15506f05a9494c801dace7479776ac",
            "value": " 268M/268M [00:07&lt;00:00, 36.7MB/s]"
          }
        },
        "d05e42ae115b4da8869580fd40820967": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a74702e6b1c94ded82ee41477369c0e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe0eb5b2343b46de8b16b5d3148d58ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f409cd63563c42379c251e8dd3b3d2a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a37e1e1768d431d853f8d0a12d4dd6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71921e4bdc7142c9aa0e49a900f6214e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c15506f05a9494c801dace7479776ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c26402061374f289fda8d630df52db3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_365e0f3700f24efb8c39912d53e1ce0c",
              "IPY_MODEL_50d97ef193ed47d4a3c1dcdb6509bf5d",
              "IPY_MODEL_935b311cf7964dd19fb469202aa4c18a"
            ],
            "layout": "IPY_MODEL_c8fc422099924ac8a8827dd6ec276188"
          }
        },
        "365e0f3700f24efb8c39912d53e1ce0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b64f83d13da42249ee96465627bcdee",
            "placeholder": "​",
            "style": "IPY_MODEL_e84a63e21d814e4e8dbb67e8cf41b759",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "50d97ef193ed47d4a3c1dcdb6509bf5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bcaeadd11b44a549e4f9503ddb05690",
            "max": 352,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15d90bad179c4283926a767ecb285a73",
            "value": 352
          }
        },
        "935b311cf7964dd19fb469202aa4c18a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f130bc2199a41caa8cc26e74a6f3fa1",
            "placeholder": "​",
            "style": "IPY_MODEL_4cca3eccb1c94f3fa5c71d1444019b0f",
            "value": " 352/352 [00:00&lt;00:00, 6.88kB/s]"
          }
        },
        "c8fc422099924ac8a8827dd6ec276188": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b64f83d13da42249ee96465627bcdee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e84a63e21d814e4e8dbb67e8cf41b759": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9bcaeadd11b44a549e4f9503ddb05690": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15d90bad179c4283926a767ecb285a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f130bc2199a41caa8cc26e74a6f3fa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cca3eccb1c94f3fa5c71d1444019b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c164d6bca3cc40beb258df9d17771265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd28f5025bac4059a2b83e72f71eb236",
              "IPY_MODEL_0a7978cea8914e5d92bd936a66fb374f",
              "IPY_MODEL_f72562d2faf140299218e14514043ea4"
            ],
            "layout": "IPY_MODEL_b2669aeb89574e198a7fe53d16001e99"
          }
        },
        "bd28f5025bac4059a2b83e72f71eb236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e607f412a3d84199a2d0a5567f223107",
            "placeholder": "​",
            "style": "IPY_MODEL_eb00861fded94dfd9588e4775e53d2a0",
            "value": "vocab.txt: 100%"
          }
        },
        "0a7978cea8914e5d92bd936a66fb374f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e275bd6e362c4700b8a5ebbff41965dc",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2dbf23e91fd646b1be26cd858c385103",
            "value": 231508
          }
        },
        "f72562d2faf140299218e14514043ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21ebfb2636b543dd8920c6a5075f0270",
            "placeholder": "​",
            "style": "IPY_MODEL_be29d6a33c044c08b98548c55569ab3a",
            "value": " 232k/232k [00:00&lt;00:00, 654kB/s]"
          }
        },
        "b2669aeb89574e198a7fe53d16001e99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e607f412a3d84199a2d0a5567f223107": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb00861fded94dfd9588e4775e53d2a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e275bd6e362c4700b8a5ebbff41965dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dbf23e91fd646b1be26cd858c385103": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21ebfb2636b543dd8920c6a5075f0270": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be29d6a33c044c08b98548c55569ab3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3717752d30664fbc83d6ffaac898ac1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_802f3745dbfa4815a8f7b62272358c44",
              "IPY_MODEL_eb14ea54140a49bd91c9428d807baffa",
              "IPY_MODEL_cd59dd4ea9f3474e89d6f2d8bf540783"
            ],
            "layout": "IPY_MODEL_cf0db5c34a4e49e8a36dea77bd3c1bcf"
          }
        },
        "802f3745dbfa4815a8f7b62272358c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cb0dec7975840678523d3819b09d628",
            "placeholder": "​",
            "style": "IPY_MODEL_0ffafba6186e4c2fbd338591643bb76c",
            "value": "tokenizer.json: 100%"
          }
        },
        "eb14ea54140a49bd91c9428d807baffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fe4ce162f7a416ebfacc418eb3ebb1c",
            "max": 711661,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99cfd0163bb84581ba4c9fdc2db86365",
            "value": 711661
          }
        },
        "cd59dd4ea9f3474e89d6f2d8bf540783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e4259f0270e412ba3116ea7eeca1b78",
            "placeholder": "​",
            "style": "IPY_MODEL_ca0a3c3fd8be4e8ba657aaacd85a6f3f",
            "value": " 712k/712k [00:00&lt;00:00, 1.34MB/s]"
          }
        },
        "cf0db5c34a4e49e8a36dea77bd3c1bcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cb0dec7975840678523d3819b09d628": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ffafba6186e4c2fbd338591643bb76c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fe4ce162f7a416ebfacc418eb3ebb1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99cfd0163bb84581ba4c9fdc2db86365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e4259f0270e412ba3116ea7eeca1b78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca0a3c3fd8be4e8ba657aaacd85a6f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3537aab3d87e44aeb7e017559dba76d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c180e5f8aff4454bbb6a12c6db69b6e",
              "IPY_MODEL_1f483d6a07044e3a8a5bcfc4463dfcd0",
              "IPY_MODEL_3f6c53c258e14cd28f478cd729f5a838"
            ],
            "layout": "IPY_MODEL_3fb779c769e84ea6abf3476e5c3f499d"
          }
        },
        "7c180e5f8aff4454bbb6a12c6db69b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_018be0c9a96f4ea08738d86e7b502de3",
            "placeholder": "​",
            "style": "IPY_MODEL_ee885c81744342978c5af804d0406d8c",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "1f483d6a07044e3a8a5bcfc4463dfcd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2caccb9b35dc4f0e920b40579986d102",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e408aaa78c1841a4bce4f302fbdce442",
            "value": 112
          }
        },
        "3f6c53c258e14cd28f478cd729f5a838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c602a8b2a1b34f92b6649a21153cfa3f",
            "placeholder": "​",
            "style": "IPY_MODEL_d1144cf4e7ed4fef999c1528b9d7d470",
            "value": " 112/112 [00:00&lt;00:00, 2.32kB/s]"
          }
        },
        "3fb779c769e84ea6abf3476e5c3f499d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "018be0c9a96f4ea08738d86e7b502de3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee885c81744342978c5af804d0406d8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2caccb9b35dc4f0e920b40579986d102": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e408aaa78c1841a4bce4f302fbdce442": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c602a8b2a1b34f92b6649a21153cfa3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1144cf4e7ed4fef999c1528b9d7d470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dc182b94daa43f89b603339b3a82925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49e14dc657bb4a2d8ad04d50f2a5a248",
              "IPY_MODEL_372b336ca08249278411b6235150c39d",
              "IPY_MODEL_435909f4613847199841746fa93404d5"
            ],
            "layout": "IPY_MODEL_357bdb3ca8654280a48c47333a7a9557"
          }
        },
        "49e14dc657bb4a2d8ad04d50f2a5a248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7eb85b6d7a5347ecba5fc2188b6e34b5",
            "placeholder": "​",
            "style": "IPY_MODEL_87b8ccd37197460c9ba46fa884c29536",
            "value": "config.json: 100%"
          }
        },
        "372b336ca08249278411b6235150c39d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fe13e2206fe41778ddddeaa448e9871",
            "max": 980,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95a801d4e1764881b74898fa55164fc9",
            "value": 980
          }
        },
        "435909f4613847199841746fa93404d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed78da379e4b47ec9e871535dd6c0abf",
            "placeholder": "​",
            "style": "IPY_MODEL_730b0eaf13754598ab0b73e5c4f661e9",
            "value": " 980/980 [00:00&lt;00:00, 45.2kB/s]"
          }
        },
        "357bdb3ca8654280a48c47333a7a9557": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eb85b6d7a5347ecba5fc2188b6e34b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87b8ccd37197460c9ba46fa884c29536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fe13e2206fe41778ddddeaa448e9871": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95a801d4e1764881b74898fa55164fc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed78da379e4b47ec9e871535dd6c0abf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "730b0eaf13754598ab0b73e5c4f661e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec193dcd210a45cb9314f55d57278219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_632a3b3b5bab40f2a6ace481efb70296",
              "IPY_MODEL_6ae3e4387d9b4eaaa2d35a3f851665a1",
              "IPY_MODEL_9cf31b53d53d4b7f98718c021accece5"
            ],
            "layout": "IPY_MODEL_d17ff2be4b9b4dd28ad9f428beb660f0"
          }
        },
        "632a3b3b5bab40f2a6ace481efb70296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_004d56ae36fb4bb1970f5849bcbbb00a",
            "placeholder": "​",
            "style": "IPY_MODEL_3856352b67ee4b70bcc35ea16f34a8e5",
            "value": "Fetching 10 files: 100%"
          }
        },
        "6ae3e4387d9b4eaaa2d35a3f851665a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed6a54451d28481388c1a489894c03d6",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a41ca3fdf9e6421585819db3823299dd",
            "value": 10
          }
        },
        "9cf31b53d53d4b7f98718c021accece5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8e540371b7443e3a98d3bb9a48cda87",
            "placeholder": "​",
            "style": "IPY_MODEL_02c6bfa393fa4c04adbf92b1e4ad6416",
            "value": " 10/10 [00:00&lt;00:00, 486.59it/s]"
          }
        },
        "d17ff2be4b9b4dd28ad9f428beb660f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "004d56ae36fb4bb1970f5849bcbbb00a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3856352b67ee4b70bcc35ea16f34a8e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed6a54451d28481388c1a489894c03d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a41ca3fdf9e6421585819db3823299dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8e540371b7443e3a98d3bb9a48cda87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02c6bfa393fa4c04adbf92b1e4ad6416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c967f7310e342eda3d9e2890b5f9322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1832690b06f64706bfc902f75204c7b6",
              "IPY_MODEL_afbe240b78604d71a3e2d318e9840afa",
              "IPY_MODEL_6a671f81d7a3489988c9de1570432309"
            ],
            "layout": "IPY_MODEL_3451a1a53e5846028696ad6042b7ba30"
          }
        },
        "1832690b06f64706bfc902f75204c7b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eff42a244bba45669e22354ba7478307",
            "placeholder": "​",
            "style": "IPY_MODEL_0f83b8ff05a5441cb0ba3bf283b8ddd0",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "afbe240b78604d71a3e2d318e9840afa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_612079c1bdb04f47b33a866103f317cf",
            "max": 50500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b66c9753c4ac4937aa1fe51421e9d66d",
            "value": 50500
          }
        },
        "6a671f81d7a3489988c9de1570432309": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61ad20073b26492eb08bac5ab60a8416",
            "placeholder": "​",
            "style": "IPY_MODEL_f1d60df628df4d9cbabbd14c88104832",
            "value": " 50.5k/50.5k [00:00&lt;00:00, 3.64MB/s]"
          }
        },
        "3451a1a53e5846028696ad6042b7ba30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eff42a244bba45669e22354ba7478307": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f83b8ff05a5441cb0ba3bf283b8ddd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "612079c1bdb04f47b33a866103f317cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b66c9753c4ac4937aa1fe51421e9d66d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61ad20073b26492eb08bac5ab60a8416": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1d60df628df4d9cbabbd14c88104832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "713048dfda4048e0811ec0749c94bb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f7f492d136547bf8e251cbf69e89f80",
              "IPY_MODEL_154e6dc388054fa085f14247ab2893d9",
              "IPY_MODEL_7fb65db7e6b1428b961d94a135110360"
            ],
            "layout": "IPY_MODEL_4d6445268f0a43d895e97f5a5a9810e1"
          }
        },
        "4f7f492d136547bf8e251cbf69e89f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02b7a9f81547478396989f02fd279b9f",
            "placeholder": "​",
            "style": "IPY_MODEL_6541124791514b4f8e4d73e078daca6f",
            "value": "tokenizer.json: 100%"
          }
        },
        "154e6dc388054fa085f14247ab2893d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ea77694843c4699b7bd82c0f3105197",
            "max": 9085657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98886b4d2d904f57ab2c736c7f492a5f",
            "value": 9085657
          }
        },
        "7fb65db7e6b1428b961d94a135110360": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7288b96a7ddb418aa92e38caca317954",
            "placeholder": "​",
            "style": "IPY_MODEL_e7d4434812fc4f66b549495abf46ea28",
            "value": " 9.09M/9.09M [00:01&lt;00:00, 7.39MB/s]"
          }
        },
        "4d6445268f0a43d895e97f5a5a9810e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02b7a9f81547478396989f02fd279b9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6541124791514b4f8e4d73e078daca6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ea77694843c4699b7bd82c0f3105197": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98886b4d2d904f57ab2c736c7f492a5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7288b96a7ddb418aa92e38caca317954": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7d4434812fc4f66b549495abf46ea28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "053715c3582d4a5fab40d808b84b8df4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74013cde87594b4b8f0bf189a9f39b67",
              "IPY_MODEL_5ff78f77c8ad4f38805c38f5cf9f4520",
              "IPY_MODEL_c2950bb7b0934bc99e5d78ab97b941bd"
            ],
            "layout": "IPY_MODEL_b786afbbe88246b5a954545b646dcbb3"
          }
        },
        "74013cde87594b4b8f0bf189a9f39b67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0671f59529414382b462f20b884ac4da",
            "placeholder": "​",
            "style": "IPY_MODEL_cdecf8808fcb4a599f312ea934a2e019",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "5ff78f77c8ad4f38805c38f5cf9f4520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09795c372b494786ac7ccdc71e802d0b",
            "max": 301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69d458f8b4d0430f8be8b6861d8bdc89",
            "value": 301
          }
        },
        "c2950bb7b0934bc99e5d78ab97b941bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ae52695d9a4491fb280c331e97e61a3",
            "placeholder": "​",
            "style": "IPY_MODEL_60f1300cf89f488cbd554bf7df2a477d",
            "value": " 301/301 [00:00&lt;00:00, 23.6kB/s]"
          }
        },
        "b786afbbe88246b5a954545b646dcbb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0671f59529414382b462f20b884ac4da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdecf8808fcb4a599f312ea934a2e019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09795c372b494786ac7ccdc71e802d0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69d458f8b4d0430f8be8b6861d8bdc89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ae52695d9a4491fb280c331e97e61a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60f1300cf89f488cbd554bf7df2a477d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "689dbc21c3564732bd02a5056dc06881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba42aff937cb4613b5bc2bd3b829ba8f",
              "IPY_MODEL_d5953c96fddd46cd80e32c6e5ea18768",
              "IPY_MODEL_04dbd18c30a14c9583642f3f8ca801dc"
            ],
            "layout": "IPY_MODEL_260a7e9f668040b4ad94edd3d9a652f1"
          }
        },
        "ba42aff937cb4613b5bc2bd3b829ba8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a78adc5573044275ac492acb59973860",
            "placeholder": "​",
            "style": "IPY_MODEL_cd4995a02fb74e3da690ccc718296a0e",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "d5953c96fddd46cd80e32c6e5ea18768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15d657695bdd42cbb5d45ee5e173f556",
            "max": 50500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b97c39a7054c4384a7b78ec97522f0a1",
            "value": 50500
          }
        },
        "04dbd18c30a14c9583642f3f8ca801dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a91fac84925c407680eee6842b99a390",
            "placeholder": "​",
            "style": "IPY_MODEL_6ba689ff7f074e67b3aee37ed55e24ba",
            "value": " 50.5k/50.5k [00:00&lt;00:00, 2.76MB/s]"
          }
        },
        "260a7e9f668040b4ad94edd3d9a652f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a78adc5573044275ac492acb59973860": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd4995a02fb74e3da690ccc718296a0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15d657695bdd42cbb5d45ee5e173f556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b97c39a7054c4384a7b78ec97522f0a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a91fac84925c407680eee6842b99a390": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ba689ff7f074e67b3aee37ed55e24ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60599bef54c947eb8297978c0c58f429": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb83ad50b8fc49d287f224c9a355cede",
              "IPY_MODEL_c007ac03a31f4ca0b2a76468ca9b7c59",
              "IPY_MODEL_90b272b646524545a89238d2be831b3d"
            ],
            "layout": "IPY_MODEL_d02c051d4d0d498783d0b8096e77acdb"
          }
        },
        "eb83ad50b8fc49d287f224c9a355cede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7b125ee96e84d58ad79b790b18a6f1f",
            "placeholder": "​",
            "style": "IPY_MODEL_e9101dbc88344176bb3d77e0e8dbc753",
            "value": "config.json: 100%"
          }
        },
        "c007ac03a31f4ca0b2a76468ca9b7c59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b83e81a7f7964395a0633c8dd57474b8",
            "max": 1076,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cbe09ff982984290be5b5af1efa5a145",
            "value": 1076
          }
        },
        "90b272b646524545a89238d2be831b3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb517821da2f44f183059f8058ad105b",
            "placeholder": "​",
            "style": "IPY_MODEL_fa1fe9f0ef3e48608ba56c2e2dbe1706",
            "value": " 1.08k/1.08k [00:00&lt;00:00, 68.4kB/s]"
          }
        },
        "d02c051d4d0d498783d0b8096e77acdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7b125ee96e84d58ad79b790b18a6f1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9101dbc88344176bb3d77e0e8dbc753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b83e81a7f7964395a0633c8dd57474b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbe09ff982984290be5b5af1efa5a145": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb517821da2f44f183059f8058ad105b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa1fe9f0ef3e48608ba56c2e2dbe1706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "757cc302979446f4bc2f5fa7fd4c1134": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_397113e39aa24260a460cb27cc0cfb23",
              "IPY_MODEL_8d78f1233d034203b3856761e0932809",
              "IPY_MODEL_7644a022c97d480f83d3da52b949d9ab"
            ],
            "layout": "IPY_MODEL_25263a656bec4c68bf6b1f700ff5374f"
          }
        },
        "397113e39aa24260a460cb27cc0cfb23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_004b581cff9748ba85555c37350bc980",
            "placeholder": "​",
            "style": "IPY_MODEL_d5ce3557ddcd4a65a7ebfcb11d8696e9",
            "value": "model_fp16.onnx: 100%"
          }
        },
        "8d78f1233d034203b3856761e0932809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc3029d329eb4dea91fd60ad2c424b5e",
            "max": 2033571558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df4a0082d6024e09ae1da788399e0139",
            "value": 2033571558
          }
        },
        "7644a022c97d480f83d3da52b949d9ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc53c8b9d15e449ba413af90d05ccb52",
            "placeholder": "​",
            "style": "IPY_MODEL_ec031a7fb6684888acc0deece1733587",
            "value": " 2.03G/2.03G [00:57&lt;00:00, 35.6MB/s]"
          }
        },
        "25263a656bec4c68bf6b1f700ff5374f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "004b581cff9748ba85555c37350bc980": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5ce3557ddcd4a65a7ebfcb11d8696e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc3029d329eb4dea91fd60ad2c424b5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df4a0082d6024e09ae1da788399e0139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc53c8b9d15e449ba413af90d05ccb52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec031a7fb6684888acc0deece1733587": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17a10f1b39b34f7db5282ac8b8709359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3f9ccd9ab084f25be66b83014939402",
              "IPY_MODEL_79ba6a19cb984d4ba04b5bc8dda98500",
              "IPY_MODEL_f02369873408444fb97ced5ac4820792"
            ],
            "layout": "IPY_MODEL_0b187e46c364404a8e1fe59b925c086e"
          }
        },
        "b3f9ccd9ab084f25be66b83014939402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d454285388a6421c93785ecfd03e6f1b",
            "placeholder": "​",
            "style": "IPY_MODEL_4210d5851018497a994db6fffd506cb5",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "79ba6a19cb984d4ba04b5bc8dda98500": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7bbaf871cd948baa7b872f682fa8e1c",
            "max": 1156999,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4519f36e4c3641529e0c55cf8e890c45",
            "value": 1156999
          }
        },
        "f02369873408444fb97ced5ac4820792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4962e188ea9b42cd969282804475594d",
            "placeholder": "​",
            "style": "IPY_MODEL_47d43626bedd40efae450dea4c10e423",
            "value": " 1.16M/1.16M [00:00&lt;00:00, 1.64MB/s]"
          }
        },
        "0b187e46c364404a8e1fe59b925c086e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d454285388a6421c93785ecfd03e6f1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4210d5851018497a994db6fffd506cb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7bbaf871cd948baa7b872f682fa8e1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4519f36e4c3641529e0c55cf8e890c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4962e188ea9b42cd969282804475594d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47d43626bedd40efae450dea4c10e423": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcfc86da8656417d8c6e6b687eb3bc3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d516645f5ca47bcae032fb4fbde620f",
              "IPY_MODEL_6d0086219e324c09b0715738cf3d932c",
              "IPY_MODEL_79f75f6b12354546b3fb85a014731311"
            ],
            "layout": "IPY_MODEL_0f1eef3e0b624953ae389c6797147a9f"
          }
        },
        "4d516645f5ca47bcae032fb4fbde620f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22eb276961cf43ed9a299351fc4cf905",
            "placeholder": "​",
            "style": "IPY_MODEL_03a41923016147eaa088f483c4fc23fa",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "6d0086219e324c09b0715738cf3d932c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28132a6c81ed4049b6dfbcd48b4e172d",
            "max": 1156999,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17a091231ac745509e310b734ff0dbbd",
            "value": 1156999
          }
        },
        "79f75f6b12354546b3fb85a014731311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_330ce52d5f0641f5a8ce68c4ee8a73d4",
            "placeholder": "​",
            "style": "IPY_MODEL_d7575bd26d214c9997ebb491632fa580",
            "value": " 1.16M/1.16M [00:00&lt;00:00, 1.60MB/s]"
          }
        },
        "0f1eef3e0b624953ae389c6797147a9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22eb276961cf43ed9a299351fc4cf905": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03a41923016147eaa088f483c4fc23fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28132a6c81ed4049b6dfbcd48b4e172d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17a091231ac745509e310b734ff0dbbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "330ce52d5f0641f5a8ce68c4ee8a73d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7575bd26d214c9997ebb491632fa580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d00a493d5f74c9580f508c4e50fd18b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4a2f92c540548bfa2fbd069256a8cec",
              "IPY_MODEL_f524add5ee194db7a4fbe15c187eba39",
              "IPY_MODEL_1d67c65bc7cb4b62a094fe3ec8d239cb"
            ],
            "layout": "IPY_MODEL_399b84709859463593afe071255e4255"
          }
        },
        "c4a2f92c540548bfa2fbd069256a8cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39d5521391a04106a26c6aa7e9affd20",
            "placeholder": "​",
            "style": "IPY_MODEL_b59815ef825c4643a8ea4eba904fdcde",
            "value": "tokenizer.model: 100%"
          }
        },
        "f524add5ee194db7a4fbe15c187eba39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7143eda3b7442e3b20ace15a8d6f019",
            "max": 4689074,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_34a0938f1d804d1dbf8d2fbbda593bdc",
            "value": 4689074
          }
        },
        "1d67c65bc7cb4b62a094fe3ec8d239cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b286b6c5d70481ba431ff3d149128e0",
            "placeholder": "​",
            "style": "IPY_MODEL_8043a3e294a942559de4501123fc0df0",
            "value": " 4.69M/4.69M [00:00&lt;00:00, 21.0MB/s]"
          }
        },
        "399b84709859463593afe071255e4255": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39d5521391a04106a26c6aa7e9affd20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b59815ef825c4643a8ea4eba904fdcde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7143eda3b7442e3b20ace15a8d6f019": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34a0938f1d804d1dbf8d2fbbda593bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b286b6c5d70481ba431ff3d149128e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8043a3e294a942559de4501123fc0df0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4240760d6c5148ce94e123cc863a6c39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a05779ac0d6a4c0e91157358753a3fbd",
              "IPY_MODEL_32b5916c9997428bb993936597030cf3",
              "IPY_MODEL_639e557c0fcb4abcb78a407a491390bb"
            ],
            "layout": "IPY_MODEL_dd453549b76a4ec6b83b1c47861f8682"
          }
        },
        "a05779ac0d6a4c0e91157358753a3fbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e372dea41821439dbbda9fc9f00992c4",
            "placeholder": "​",
            "style": "IPY_MODEL_eb8067fdc8d0457fa58388a3fe4444c0",
            "value": "tokenizer.json: 100%"
          }
        },
        "32b5916c9997428bb993936597030cf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ee96eec005248b6bca0aa3c2798164b",
            "max": 33384568,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_120084202ef34e35831bf9e5d58e0e11",
            "value": 33384568
          }
        },
        "639e557c0fcb4abcb78a407a491390bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0b08baa19144c09867644f8588b8d30",
            "placeholder": "​",
            "style": "IPY_MODEL_a3f10a00d7bf4297a6b0636b81c3dd3d",
            "value": " 33.4M/33.4M [00:00&lt;00:00, 259MB/s]"
          }
        },
        "dd453549b76a4ec6b83b1c47861f8682": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e372dea41821439dbbda9fc9f00992c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb8067fdc8d0457fa58388a3fe4444c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ee96eec005248b6bca0aa3c2798164b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "120084202ef34e35831bf9e5d58e0e11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0b08baa19144c09867644f8588b8d30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3f10a00d7bf4297a6b0636b81c3dd3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f32ea67a70a4ad9a68d809fca7199bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e10fe5fa37d74d0d940c1d6d0638d561",
              "IPY_MODEL_e3296ecb6da542478be21837667da13c",
              "IPY_MODEL_292483db78a0443298a9c8ca94718713"
            ],
            "layout": "IPY_MODEL_e737931334b946248b0c296e882b17d6"
          }
        },
        "e10fe5fa37d74d0d940c1d6d0638d561": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ae78af213324359899f9447974621a8",
            "placeholder": "​",
            "style": "IPY_MODEL_65bda09f37234871a4207ac08bfc8c5c",
            "value": "added_tokens.json: 100%"
          }
        },
        "e3296ecb6da542478be21837667da13c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88cfb9f7a55e4796b0bd6a01a8a3c7f2",
            "max": 35,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49b7684089ca488db89961d84749c977",
            "value": 35
          }
        },
        "292483db78a0443298a9c8ca94718713": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a37b9b9066c4a6b97f7b29d1a348a5e",
            "placeholder": "​",
            "style": "IPY_MODEL_eac7ce09376941c5a0512e625fe945de",
            "value": " 35.0/35.0 [00:00&lt;00:00, 687B/s]"
          }
        },
        "e737931334b946248b0c296e882b17d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ae78af213324359899f9447974621a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65bda09f37234871a4207ac08bfc8c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88cfb9f7a55e4796b0bd6a01a8a3c7f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49b7684089ca488db89961d84749c977": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a37b9b9066c4a6b97f7b29d1a348a5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eac7ce09376941c5a0512e625fe945de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "398f0fd481904c2691a8101dc8d9af25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5c83245a77e4f1cb7a09bf274094187",
              "IPY_MODEL_ceaa00ddc2b3470c8fc7756f20a27de3",
              "IPY_MODEL_891d0b0b0bf64592ac8912f665fcb3b8"
            ],
            "layout": "IPY_MODEL_63f43a8999bc4870857064da2e6c1536"
          }
        },
        "f5c83245a77e4f1cb7a09bf274094187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_144962dc8e8541c2b00f7f964d50f3a5",
            "placeholder": "​",
            "style": "IPY_MODEL_dbd7ecd20ba545419b9adedddf9c9dfa",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "ceaa00ddc2b3470c8fc7756f20a27de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e04fdb820054a2987282887a656fdf2",
            "max": 662,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6d8727067044b4299b40c5a779b2dac",
            "value": 662
          }
        },
        "891d0b0b0bf64592ac8912f665fcb3b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4b18a40c7964522b47e3f435b7eb143",
            "placeholder": "​",
            "style": "IPY_MODEL_920ea75d03f9483e9578db3c7efac1d1",
            "value": " 662/662 [00:00&lt;00:00, 55.9kB/s]"
          }
        },
        "63f43a8999bc4870857064da2e6c1536": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "144962dc8e8541c2b00f7f964d50f3a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbd7ecd20ba545419b9adedddf9c9dfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e04fdb820054a2987282887a656fdf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6d8727067044b4299b40c5a779b2dac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4b18a40c7964522b47e3f435b7eb143": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "920ea75d03f9483e9578db3c7efac1d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "426d0550d4ef4cefba0c4f587921262a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2611fc157db44372b3dd6106a45b0edd",
              "IPY_MODEL_e6b24f7188624828a050a71a1bf2dae8",
              "IPY_MODEL_d685c7e6492c454caf9d4b5ba39bc2fc"
            ],
            "layout": "IPY_MODEL_225ae15da00c434e8818b1d8e96f986d"
          }
        },
        "2611fc157db44372b3dd6106a45b0edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c19c91719f234d48bebe81c8fc6ab1ff",
            "placeholder": "​",
            "style": "IPY_MODEL_7bd2a7cb0c1f45839e72b41270563f4c",
            "value": "tokenizer.model: 100%"
          }
        },
        "e6b24f7188624828a050a71a1bf2dae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72b7a8ab3b2b426ea703cc9425fc1ec2",
            "max": 4689074,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c75b12f7835342bb9c4db39cc6673007",
            "value": 4689074
          }
        },
        "d685c7e6492c454caf9d4b5ba39bc2fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db5f5459eb9e4b98b6d717eb45ffb1d3",
            "placeholder": "​",
            "style": "IPY_MODEL_1a2cb1fb7df64d0cac5f71a1ec74bd2d",
            "value": " 4.69M/4.69M [00:00&lt;00:00, 32.5MB/s]"
          }
        },
        "225ae15da00c434e8818b1d8e96f986d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c19c91719f234d48bebe81c8fc6ab1ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bd2a7cb0c1f45839e72b41270563f4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72b7a8ab3b2b426ea703cc9425fc1ec2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c75b12f7835342bb9c4db39cc6673007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db5f5459eb9e4b98b6d717eb45ffb1d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a2cb1fb7df64d0cac5f71a1ec74bd2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aebcabb8a5774fcf863ede1591315369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36484482c2d242cf8aafce6d5651d163",
              "IPY_MODEL_4d74cd4bb916435b811f1e319e74d23b",
              "IPY_MODEL_8559fe254dd443e0b5feef6a27c81f42"
            ],
            "layout": "IPY_MODEL_806cf64166214a95b0d538d3dcd3a12b"
          }
        },
        "36484482c2d242cf8aafce6d5651d163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6a4caebfbc74702a811349d8499f8da",
            "placeholder": "​",
            "style": "IPY_MODEL_bed42072cf9d4f579741edc17fd12bed",
            "value": "tokenizer.json: 100%"
          }
        },
        "4d74cd4bb916435b811f1e319e74d23b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cad70b1193646c0b7808c61853ada5c",
            "max": 20323106,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_898b2b5f0b7344149ae25d33e69304ce",
            "value": 20323106
          }
        },
        "8559fe254dd443e0b5feef6a27c81f42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31c6a0efd5684dd58ff8091f3e601fbf",
            "placeholder": "​",
            "style": "IPY_MODEL_8017434f898d40478c86b3a2cb1681c8",
            "value": " 20.3M/20.3M [00:01&lt;00:00, 13.2MB/s]"
          }
        },
        "806cf64166214a95b0d538d3dcd3a12b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6a4caebfbc74702a811349d8499f8da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bed42072cf9d4f579741edc17fd12bed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cad70b1193646c0b7808c61853ada5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "898b2b5f0b7344149ae25d33e69304ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31c6a0efd5684dd58ff8091f3e601fbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8017434f898d40478c86b3a2cb1681c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5a1f9ed5510463e8cb9bcf216116560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2eece0077ba41cdbd284d1cf8e34bc0",
              "IPY_MODEL_48cc13c58d5c4f42b9af7e3d7e7acae8",
              "IPY_MODEL_7970321c8a8f4782af03014a1626c9f5"
            ],
            "layout": "IPY_MODEL_9c622547be56411bb4088841bb2966bd"
          }
        },
        "e2eece0077ba41cdbd284d1cf8e34bc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d13f8786dd9465b923e7e53c1c58400",
            "placeholder": "​",
            "style": "IPY_MODEL_cb274ebae28e44829627d20a5250cb41",
            "value": "added_tokens.json: 100%"
          }
        },
        "48cc13c58d5c4f42b9af7e3d7e7acae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a59af1409ad8431482c9c0448050c932",
            "max": 35,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06b3d023a3594f6eb92d2e1ada819558",
            "value": 35
          }
        },
        "7970321c8a8f4782af03014a1626c9f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b57a666c67c44fa97bb6220d15d3e8f",
            "placeholder": "​",
            "style": "IPY_MODEL_59746eae192443f697f4e1b3e9aeb339",
            "value": " 35.0/35.0 [00:00&lt;00:00, 2.16kB/s]"
          }
        },
        "9c622547be56411bb4088841bb2966bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d13f8786dd9465b923e7e53c1c58400": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb274ebae28e44829627d20a5250cb41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a59af1409ad8431482c9c0448050c932": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06b3d023a3594f6eb92d2e1ada819558": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b57a666c67c44fa97bb6220d15d3e8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59746eae192443f697f4e1b3e9aeb339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14d43c7f42cc4bb48e8cdc06089f2e10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6aaee233605d4d048b617b913037adf3",
              "IPY_MODEL_cbe158540d9b4ce68fa585c01b9aa672",
              "IPY_MODEL_a17b69cdef8f457ca78e327ce5b1d712"
            ],
            "layout": "IPY_MODEL_b1bc039ce7fb4bfd9b4a98ff269ee26e"
          }
        },
        "6aaee233605d4d048b617b913037adf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b6249e6f18c4ce4a84dc259d4fb4159",
            "placeholder": "​",
            "style": "IPY_MODEL_39001556744e4f1ba6537cef0ed625eb",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "cbe158540d9b4ce68fa585c01b9aa672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bb76efded8f4c558b37f266b2806e1d",
            "max": 662,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11805e999c8545c0b7c984503ee1c759",
            "value": 662
          }
        },
        "a17b69cdef8f457ca78e327ce5b1d712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94389a7b751c477aba7f66ef7f0670e8",
            "placeholder": "​",
            "style": "IPY_MODEL_e8feb97dd400477eb030307381f90350",
            "value": " 662/662 [00:00&lt;00:00, 44.3kB/s]"
          }
        },
        "b1bc039ce7fb4bfd9b4a98ff269ee26e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b6249e6f18c4ce4a84dc259d4fb4159": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39001556744e4f1ba6537cef0ed625eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bb76efded8f4c558b37f266b2806e1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11805e999c8545c0b7c984503ee1c759": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94389a7b751c477aba7f66ef7f0670e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8feb97dd400477eb030307381f90350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd85739b72104387acebda10dc5ef147": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_432441fa69ec409695129140ad6a6fd3",
              "IPY_MODEL_472e236d20f340e1aed0f5108ed51b86",
              "IPY_MODEL_c53d93812de84fcb9323a47bd608c537"
            ],
            "layout": "IPY_MODEL_7eda64b7f6e444df9ba3bf2b6e6b3aa5"
          }
        },
        "432441fa69ec409695129140ad6a6fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9401616e27c04995923a89e56edf5f05",
            "placeholder": "​",
            "style": "IPY_MODEL_14642adc08f344e28311e7da24e24d43",
            "value": "config.json: 100%"
          }
        },
        "472e236d20f340e1aed0f5108ed51b86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f1225c383bd427bbbcb86a4b64e0cad",
            "max": 1081,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a86866e418349eaa9f4030b2b0d06ae",
            "value": 1081
          }
        },
        "c53d93812de84fcb9323a47bd608c537": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82e7348ea2bb4da4ba61c47f0e3598e2",
            "placeholder": "​",
            "style": "IPY_MODEL_cc83e447577a407a83fbb57749559324",
            "value": " 1.08k/1.08k [00:00&lt;00:00, 51.4kB/s]"
          }
        },
        "7eda64b7f6e444df9ba3bf2b6e6b3aa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9401616e27c04995923a89e56edf5f05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14642adc08f344e28311e7da24e24d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f1225c383bd427bbbcb86a4b64e0cad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a86866e418349eaa9f4030b2b0d06ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82e7348ea2bb4da4ba61c47f0e3598e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc83e447577a407a83fbb57749559324": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3701e98a51b84ada886f764442d94039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3c183ed36cf452e8b6fb7393b60a3d0",
              "IPY_MODEL_a8433058c44c47f89485041cd7225840",
              "IPY_MODEL_cb93c6df7b6e48c78b50e506a4d51f81"
            ],
            "layout": "IPY_MODEL_f61b5500f8c5488a86a31a15ba97c507"
          }
        },
        "a3c183ed36cf452e8b6fb7393b60a3d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a04687e8e38490c9242d1d136eca3c5",
            "placeholder": "​",
            "style": "IPY_MODEL_7b7507e1572d47509a24103bbad30a32",
            "value": "model_fp16.onnx: 100%"
          }
        },
        "a8433058c44c47f89485041cd7225840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d551e059561b4557b5e5bca0079a702c",
            "max": 2000774696,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c45664a1a76e43e0a02384658969c55c",
            "value": 2000774696
          }
        },
        "cb93c6df7b6e48c78b50e506a4d51f81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_762b74035a114f74a9727e2985a4edfc",
            "placeholder": "​",
            "style": "IPY_MODEL_9966ba76d1194433ad794de73aaf8dba",
            "value": " 2.00G/2.00G [00:55&lt;00:00, 39.0MB/s]"
          }
        },
        "f61b5500f8c5488a86a31a15ba97c507": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a04687e8e38490c9242d1d136eca3c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b7507e1572d47509a24103bbad30a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d551e059561b4557b5e5bca0079a702c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c45664a1a76e43e0a02384658969c55c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "762b74035a114f74a9727e2985a4edfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9966ba76d1194433ad794de73aaf8dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd4676f02b944fa1bf90c6a519b02981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_285af0b01e484876bad2feef31a7036b",
              "IPY_MODEL_122aa29443bb4119befdd7f5ce6d6094",
              "IPY_MODEL_09f7a38a0b1d44b6b3e5b3f22f69f320"
            ],
            "layout": "IPY_MODEL_fdd65786b88443698dfd3bf4d3e21ad7"
          }
        },
        "285af0b01e484876bad2feef31a7036b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_660491a86d83431ab190b9d0f816c24c",
            "placeholder": "​",
            "style": "IPY_MODEL_9fa47ce003b549c7b45128143503d6f9",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "122aa29443bb4119befdd7f5ce6d6094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54b54e42530a43b58d1939f5e0846644",
            "max": 1156959,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8bb348fceff243e790143fbcc478e989",
            "value": 1156959
          }
        },
        "09f7a38a0b1d44b6b3e5b3f22f69f320": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62f5883fa3d54fc78333c89e1acd1cc6",
            "placeholder": "​",
            "style": "IPY_MODEL_4b1f09497af44ccda5987f07c9936441",
            "value": " 1.16M/1.16M [00:00&lt;00:00, 2.13MB/s]"
          }
        },
        "fdd65786b88443698dfd3bf4d3e21ad7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "660491a86d83431ab190b9d0f816c24c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fa47ce003b549c7b45128143503d6f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54b54e42530a43b58d1939f5e0846644": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bb348fceff243e790143fbcc478e989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62f5883fa3d54fc78333c89e1acd1cc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b1f09497af44ccda5987f07c9936441": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "831c6114f2e44b21839727c1f3db948f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3698570a02384461b78bbab823240079",
              "IPY_MODEL_ae4c6ad5de88418f8cdf8fb85c832f7e",
              "IPY_MODEL_66ffaba76b214eb8b8a6f0c5594f3bc5"
            ],
            "layout": "IPY_MODEL_fd18923374014eedbf4d6e6a2abbb7e5"
          }
        },
        "3698570a02384461b78bbab823240079": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ceb10fc3be046b9a25a04763eb99ce3",
            "placeholder": "​",
            "style": "IPY_MODEL_8e210832ab364791a9e9e5d9a44f6ad1",
            "value": "tokenizer.json: 100%"
          }
        },
        "ae4c6ad5de88418f8cdf8fb85c832f7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6b5cc5060a64894990893d233262f50",
            "max": 33384568,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_776620af332841d0b375665321263cda",
            "value": 33384568
          }
        },
        "66ffaba76b214eb8b8a6f0c5594f3bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ba9cfe526654408b54556319d1fe5ca",
            "placeholder": "​",
            "style": "IPY_MODEL_b5ec8f237b064bd68c41e966cf81aac2",
            "value": " 33.4M/33.4M [00:01&lt;00:00, 23.2MB/s]"
          }
        },
        "fd18923374014eedbf4d6e6a2abbb7e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ceb10fc3be046b9a25a04763eb99ce3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e210832ab364791a9e9e5d9a44f6ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6b5cc5060a64894990893d233262f50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "776620af332841d0b375665321263cda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ba9cfe526654408b54556319d1fe5ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5ec8f237b064bd68c41e966cf81aac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d1cf094fb1f491f9d5b8964025b92f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8f227de71b148a89c5f1bcb836c98e8",
              "IPY_MODEL_000aac3024554512ada3227c0645f6ef",
              "IPY_MODEL_0fd88bda8f384500bb686a2d7d48df7b"
            ],
            "layout": "IPY_MODEL_35c1144a983843ca880943c7c427539f"
          }
        },
        "a8f227de71b148a89c5f1bcb836c98e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fd832f321484bd4aaf13e6d73b4dd27",
            "placeholder": "​",
            "style": "IPY_MODEL_1479a0a72ba74baf915a5851d459d375",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "000aac3024554512ada3227c0645f6ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3520466a8184cd6aae7d26f1e45ae84",
            "max": 662,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fae23596dbdb48529a545a216eaa63fb",
            "value": 662
          }
        },
        "0fd88bda8f384500bb686a2d7d48df7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c2b18fe89334e1da920eb2d0a20bf37",
            "placeholder": "​",
            "style": "IPY_MODEL_c48a2fa2e8b04f1599562fcc2c984572",
            "value": " 662/662 [00:00&lt;00:00, 45.4kB/s]"
          }
        },
        "35c1144a983843ca880943c7c427539f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fd832f321484bd4aaf13e6d73b4dd27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1479a0a72ba74baf915a5851d459d375": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3520466a8184cd6aae7d26f1e45ae84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fae23596dbdb48529a545a216eaa63fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c2b18fe89334e1da920eb2d0a20bf37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c48a2fa2e8b04f1599562fcc2c984572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59b58da5f7154aa9b8d91370555e7405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9fb03dc9be1a4316b2ec012a5ec51839",
              "IPY_MODEL_12e795e9d49e4d22b9d9ca3c8f90fb01",
              "IPY_MODEL_cd0c2e3133894ce3b621d5e29c97015e"
            ],
            "layout": "IPY_MODEL_0d0aa583c1ad41b5a0a76ddb8345e6ed"
          }
        },
        "9fb03dc9be1a4316b2ec012a5ec51839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3c2559b815a4441911bc8ff3cb873e1",
            "placeholder": "​",
            "style": "IPY_MODEL_6e7e9f183d6c48e4b7102b6572f91129",
            "value": "model_int8.onnx: 100%"
          }
        },
        "12e795e9d49e4d22b9d9ca3c8f90fb01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cf7226d80bc45d7b8e7fefa6d6f390f",
            "max": 1067852475,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ae24f007ca841029f7a38c897e4516f",
            "value": 1067852475
          }
        },
        "cd0c2e3133894ce3b621d5e29c97015e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9a4edd5dd1f492e9a785945cf458686",
            "placeholder": "​",
            "style": "IPY_MODEL_dec70709b81f40a78afd25f20696b09d",
            "value": " 1.07G/1.07G [00:31&lt;00:00, 4.36MB/s]"
          }
        },
        "0d0aa583c1ad41b5a0a76ddb8345e6ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3c2559b815a4441911bc8ff3cb873e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e7e9f183d6c48e4b7102b6572f91129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cf7226d80bc45d7b8e7fefa6d6f390f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ae24f007ca841029f7a38c897e4516f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9a4edd5dd1f492e9a785945cf458686": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dec70709b81f40a78afd25f20696b09d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDvvjZzCZTR9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aVyAEjpOZT8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm i @huggingface/transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_ms83OSZUAG",
        "outputId": "f549f870-70d0-4965-fcde-4e0a6a6af142"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m boolean@3.2.0: Package no longer supported. Contact Support at https://www.npmjs.com/support for more info.\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K\n",
            "added 61 packages in 34s\n",
            "\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K12 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gh0zbdKVZUCu",
        "outputId": "3c13db34-1d42-426f-9710-eab520031dbc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.17.0 onnxruntime-1.21.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optimum transformers datasets evaluate huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6MxCibzZUFW",
        "outputId": "e56c2fd3-6eb7-49e9-df53-ee89bb2a8d90"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optimum\n",
            "  Downloading optimum-1.24.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.11/dist-packages (from optimum) (2.6.0+cu124)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from optimum) (24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optimum) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11->optimum) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11->optimum) (3.0.2)\n",
            "Downloading optimum-1.24.0-py3-none-any.whl (433 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, optimum, evaluate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 optimum-1.24.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rUC_GsW_ldXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "phpUgJWmldUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال جيد"
      ],
      "metadata": {
        "id": "qvz4_1QZl5yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from optimum.onnxruntime import ORTModelForCausalLM\n",
        "\n",
        "# Optional: Set logging level for more details from transformers/optimum\n",
        "# logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# --- Configuration ---\n",
        "model_repo_id = \"onnx-community/Llama-3.2-1B\"\n",
        "onnx_subfolder = \"onnx\"\n",
        "tokenizer_repo_id = \"meta-llama/Llama-3.2-1B\" # Use original repo for tokenizer\n",
        "\n",
        "# --- !!! Choose the specific ONNX file you want to load !!! ---\n",
        "# Check the 'onnx' subfolder in the repo: https://huggingface.co/onnx-community/Llama-3.2-1B/tree/main/onnx\n",
        "# Options include: \"model.onnx\", \"model_fp16.onnx\", \"model_q4.onnx\", \"model_int8.onnx\", etc.\n",
        "onnx_filename = \"model_fp16.onnx\"  # FP16 is often a good balance of size/speed/quality\n",
        "# onnx_filename = \"model_q4.onnx\"    # Quantized: Smaller, potentially faster, slight quality loss\n",
        "# onnx_filename = \"model.onnx\"       # Original large file (might be FP32)\n",
        "\n",
        "print(f\"Loading ONNX model: {model_repo_id}/{onnx_subfolder}/{onnx_filename}\")\n",
        "\n",
        "# --- 1. Load Model ---\n",
        "try:\n",
        "    # Use repo_id, subfolder, and explicitly specify the file_name\n",
        "    model = ORTModelForCausalLM.from_pretrained(\n",
        "        model_repo_id,\n",
        "        subfolder=onnx_subfolder,\n",
        "        file_name=onnx_filename, # Crucial addition!\n",
        "        # provider=\"CPUExecutionProvider\", # Explicitly set CPU if needed, default is often fine\n",
        "        use_cache=True # Use KV cache for faster generation\n",
        "    )\n",
        "    print(\"ONNX Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ONNX model: {e}\")\n",
        "    print(\"Please ensure:\")\n",
        "    print(f\"  - The repo '{model_repo_id}' and subfolder '{onnx_subfolder}' exist.\")\n",
        "    print(f\"  - The file '{onnx_filename}' exists within that subfolder.\")\n",
        "    print(f\"  - You have internet connectivity and necessary permissions (e.g., HF_TOKEN if needed).\")\n",
        "    exit() # Exit if model loading fails\n",
        "\n",
        "# --- 2. Load Tokenizer ---\n",
        "print(f\"\\nLoading tokenizer from: {tokenizer_repo_id}\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_repo_id)\n",
        "    # Llama models require a pad token, but often don't have one set by default.\n",
        "    # Setting it to the EOS token is a common practice.\n",
        "    if tokenizer.pad_token is None:\n",
        "        print(\"Setting pad_token to eos_token\")\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        # Important: Also update the model's config if padding is needed during generation\n",
        "        # This ensures consistency if the pipeline or generation kwargs rely on it.\n",
        "        if model.config.pad_token_id is None:\n",
        "             model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    print(\"Tokenizer loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")\n",
        "    exit() # Exit if tokenizer loading fails\n",
        "\n",
        "# --- 3. Create Pipeline ---\n",
        "print(\"\\nCreating text-generation pipeline...\")\n",
        "try:\n",
        "    # You can add device=0 for CUDA GPU if onnxruntime-gpu is installed\n",
        "    # pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "    print(\"Pipeline created successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating pipeline: {e}\")\n",
        "    exit() # Exit if pipeline creation fails\n",
        "\n",
        "# --- 4. Generate Text ---\n",
        "prompt = \"He never went out without a book under his arm\"\n",
        "print(f\"\\nGenerating text for prompt: '{prompt}'\")\n",
        "\n",
        "try:\n",
        "    # Add generation parameters for better control and to avoid warnings\n",
        "    result = pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=50,  # Generate up to 50 new tokens\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,     # Use sampling for more creative output\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id # Explicitly pass pad token ID\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Generation Result ---\")\n",
        "    # Pipeline returns a list of dictionaries\n",
        "    if isinstance(result, list) and len(result) > 0 and 'generated_text' in result[0]:\n",
        "         print(result[0]['generated_text'])\n",
        "    else:\n",
        "         print(result) # Print raw result if format is unexpected\n",
        "    print(\"--- End of Result ---\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError during text generation: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMzAIWuHldQw",
        "outputId": "68c3fa73-5512-48c8-d8c2-58a765097128"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ONNX model: onnx-community/Llama-3.2-1B/onnx/model_fp16.onnx\n",
            "ONNX Model loaded successfully.\n",
            "\n",
            "Loading tokenizer from: meta-llama/Llama-3.2-1B\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting pad_token to eos_token\n",
            "Tokenizer loaded successfully.\n",
            "\n",
            "Creating text-generation pipeline...\n",
            "Pipeline created successfully.\n",
            "\n",
            "Generating text for prompt: 'He never went out without a book under his arm'\n",
            "\n",
            "--- Generation Result ---\n",
            "He never went out without a book under his arm.\n",
            "I don’t know what I was thinking.\n",
            "I’ve been reading about the new book, The Lost Symbol, by Dan Brown. It’s a very interesting book, and I’m looking forward to reading it. It’s about the search for a\n",
            "--- End of Result ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1HpC5jMldNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GtwrH5NWldJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "6vip3pMisGKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile my_script2.js\n",
        "\n",
        "\n",
        "\n",
        "import { pipeline } from \"@huggingface/transformers\";\n",
        "\n",
        "// Create a text generation pipeline\n",
        "const generator = await pipeline(\n",
        "  \"text-generation\",\n",
        "  \"onnx-community/gemma-3-1b-it-ONNX-GQA\",\n",
        "  { dtype: \"q4\" },\n",
        ");\n",
        "\n",
        "// Define the list of messages\n",
        "const messages = [\n",
        "  { role: \"system\", content: \"You are a helpful assistant.\" },\n",
        "  { role: \"user\", content: \"Who is Napoleon Bonaparte?\" },\n",
        "];\n",
        "\n",
        "// Generate a response\n",
        "const output = await generator(messages, { max_new_tokens: 512, do_sample: false });\n",
        "console.log(output[0].generated_text.at(-1).content);\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4871be4-d3bd-41e1-80b1-4828a017c8dc",
        "id": "OZMPgqU6sGKu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing my_script2.js\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!node my_script2.js"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5728bab-77fb-4aa0-ec4d-e407cafb1b29",
        "id": "yb9YEUg4sGKw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(node:19548) [MODULE_TYPELESS_PACKAGE_JSON] Warning: Module type of file:///content/my_script2.js is not specified and it doesn't parse as CommonJS.\n",
            "Reparsing as ES module because module syntax was detected. This incurs a performance overhead.\n",
            "To eliminate this warning, add \"type\": \"module\" to /content/package.json.\n",
            "(Use `node --trace-warnings ...` to show where the warning was created)\n",
            "Okay, let's break down who Napoleon Bonaparte was – he's a hugely significant and complex figure in European history. Here's a comprehensive overview:\n",
            "\n",
            "**Who Was Napoleon Bonaparte?**\n",
            "\n",
            "Napoleon Charles Bonaparte (1768-1821) was a French military and political leader who rose to prominence during the French Revolution and ultimately dominated Europe for over a decade. He's renowned for his military genius, ambitious reforms, and the reshaping of the continent.\n",
            "\n",
            "**Key Facts & Accomplishments:**\n",
            "\n",
            "* **Early Life & Rise to Power:** Born into a relatively wealthy family, Napoleon was a brilliant and ambitious child. He was a student of military strategy and quickly distinguished himself in the French army. He achieved significant success in campaigns in Italy and Egypt, earning him a reputation as a brilliant tactician.\n",
            "* **Italian Campaign (1796-1797):**  This was a crucial turning point. Napoleon led a successful invasion and conquest of Italy, demonstrating his strategic brilliance and establishing French control over the Italian peninsula.\n",
            "* **The French Revolution & Rise to Power (1796-1799):**  Following the French Revolution, Napoleon seized power in a coup d'état, overthrowing the Directory. He established the Consulate in 1799, effectively becoming First Consul and then Emperor of France.\n",
            "* **The Napoleonic Wars (1799-1815):** This is arguably his most famous period. He led a series of massive, sweeping wars against a coalition of European powers – primarily Austria, Prussia, Great Britain, and Russia – aiming to create a vast, unified French empire.\n",
            "    * **Italian Campaign (1800-1805):**  A decisive victory that solidified French dominance in Italy.\n",
            "    * **Egyptian Campaign (1798-1801):**  A strategic move to disrupt British trade routes to India.\n",
            "    * **War of the Second Coalition (1803-1806):**  A major victory against Austria, crippling the coalition.\n",
            "    * **Battle of Austerlitz (1805):**  A stunning victory over Austria, considered one of his greatest military achievements.\n",
            "    * **Dissolution of the Holy Roman Empire (1806):**  A significant strategic move that weakened Austria.\n",
            "* **Restoration & Consolidation (1810-1812):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fz0R-H3_ldF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ayqeeiMhldBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال جيد"
      ],
      "metadata": {
        "id": "zbuEX1YVyoeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load config, processor, and model\n",
        "path_to_model = \"/root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX/snapshots/7769a8aff8978adbdcd57ad7915a97ba389fb547\"\n",
        "config = AutoConfig.from_pretrained(path_to_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
        "decoder_session = onnxruntime.InferenceSession(f\"{path_to_model}/onnx/model_fp16.onnx\")\n",
        "\n",
        "## Set config values\n",
        "num_key_value_heads = config.num_key_value_heads\n",
        "head_dim = config.head_dim\n",
        "num_hidden_layers = config.num_hidden_layers\n",
        "eos_token_id = 106 # 106 is for <end_of_turn>\n",
        "\n",
        "# 2. Prepare inputs\n",
        "## Create input messages\n",
        "messages = [\n",
        "  { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
        "  { \"role\": \"user\", \"content\": \"Write me a poem about Machine Learning.\" },\n",
        "]\n",
        "\n",
        "## Apply tokenizer\n",
        "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"np\")\n",
        "\n",
        "## Prepare decoder inputs\n",
        "batch_size = inputs['input_ids'].shape[0]\n",
        "past_key_values = {\n",
        "    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\n",
        "    for layer in range(num_hidden_layers)\n",
        "    for kv in ('key', 'value')\n",
        "}\n",
        "input_ids = inputs['input_ids']\n",
        "position_ids = np.tile(np.arange(1, input_ids.shape[-1] + 1), (batch_size, 1))\n",
        "\n",
        "# 3. Generation loop\n",
        "max_new_tokens = 1024\n",
        "generated_tokens = np.array([[]], dtype=np.int64)\n",
        "for i in range(max_new_tokens):\n",
        "  logits, *present_key_values = decoder_session.run(None, dict(\n",
        "      input_ids=input_ids,\n",
        "      position_ids=position_ids,\n",
        "      **past_key_values,\n",
        "  ))\n",
        "\n",
        "  ## Update values for next generation loop\n",
        "  input_ids = logits[:, -1].argmax(-1, keepdims=True)\n",
        "  position_ids = position_ids[:, -1:] + 1\n",
        "  for j, key in enumerate(past_key_values):\n",
        "    past_key_values[key] = present_key_values[j]\n",
        "\n",
        "  generated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\n",
        "  if (input_ids == eos_token_id).all():\n",
        "    break\n",
        "\n",
        "  ## (Optional) Streaming\n",
        "  print(tokenizer.decode(input_ids[0]), end='', flush=True)\n",
        "print()\n",
        "\n",
        "# 4. Output result\n",
        "print(tokenizer.batch_decode(generated_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f82617-73cd-49da-e5f4-446354b87768",
        "id": "CYNaoIWYyoeK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here’s a poem about Machine Learning, aiming for a balance of technical and evocative language:\n",
            "\n",
            "**The Silent Learner**\n",
            "\n",
            "The data streams, a boundless flow,\n",
            "A river vast, where patterns grow.\n",
            "No human hand to guide the way,\n",
            "Just algorithms, come what may.\n",
            "\n",
            "Machine Learning, a subtle art,\n",
            "To teach a system, a brand new start.\n",
            "With weights and biases, finely tuned,\n",
            "It seeks the truth, beneath the moon.\n",
            "\n",
            "It learns from errors, big and small,\n",
            "Adjusting swiftly, standing tall.\n",
            "From pixels bright to voices clear,\n",
            "It builds a model, banishing fear.\n",
            "\n",
            "Of blind prediction, cold and stark,\n",
            "It finds the meaning, leaves its mark.\n",
            "A neural net, a complex grace,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a24k8TQ0lc9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YCABGV9slc5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9jM3K7G0lc1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال جيد"
      ],
      "metadata": {
        "id": "r5a8nWYK8vgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load model and tokenizer\n",
        "path_to_model = \"/root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX-GQA/snapshots/e05326d9bfe91af70fcef3a82154a8047d851926\"\n",
        "config = AutoConfig.from_pretrained(path_to_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
        "decoder_session = onnxruntime.InferenceSession(f\"{path_to_model}/onnx/model_fp16.onnx\")\n",
        "\n",
        "# 2. Model parameters\n",
        "num_key_value_heads = config.num_key_value_heads\n",
        "head_dim = config.head_dim\n",
        "num_hidden_layers = config.num_hidden_layers\n",
        "eos_token_id = 106  # end of turn token\n",
        "\n",
        "# 3. Sampling parameters\n",
        "temperature = 1.0\n",
        "top_p = 0.9\n",
        "repetition_penalty = 1.2\n",
        "\n",
        "# 4. Input messages\n",
        "messages = [\n",
        "    { \"role\": \"system\", \"content\": \"You are a world-class poet AI assistant, always writing elegant and sophisticated poems.\" },\n",
        "    { \"role\": \"user\", \"content\": \"Write me a beautiful poem about Machine Learning and its future.\" },\n",
        "]\n",
        "\n",
        "# 5. Tokenize inputs\n",
        "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"np\")\n",
        "batch_size = inputs['input_ids'].shape[0]\n",
        "input_ids = inputs['input_ids']\n",
        "attention_mask = inputs['attention_mask']\n",
        "\n",
        "past_key_values = {\n",
        "    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float16)\n",
        "    for layer in range(num_hidden_layers)\n",
        "    for kv in ('key', 'value')\n",
        "}\n",
        "\n",
        "position_ids = np.tile(np.arange(1, input_ids.shape[-1] + 1), (batch_size, 1))\n",
        "\n",
        "# 6. Helper functions\n",
        "def apply_repetition_penalty(logits, generated_tokens, penalty):\n",
        "    for token in set(generated_tokens.flatten()):\n",
        "        logits[:, token] /= penalty\n",
        "    return logits\n",
        "\n",
        "def top_p_sampling(logits, p):\n",
        "    logits = logits.astype(np.float32)\n",
        "    sorted_indices = np.argsort(-logits, axis=-1)\n",
        "    sorted_logits = np.take_along_axis(logits, sorted_indices, axis=-1)\n",
        "    cumulative_probs = np.cumsum(softmax(sorted_logits), axis=-1)\n",
        "\n",
        "    # Mask tokens with cumulative probs above p\n",
        "    sorted_indices_to_remove = cumulative_probs > p\n",
        "    if sorted_indices_to_remove.shape[1] > 0:\n",
        "        sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1]\n",
        "        sorted_indices_to_remove[:, 0] = False\n",
        "\n",
        "    for batch_idx in range(logits.shape[0]):\n",
        "        logits[batch_idx, sorted_indices[batch_idx][sorted_indices_to_remove[batch_idx]]] = -float(\"inf\")\n",
        "\n",
        "    probs = softmax(logits / temperature)\n",
        "    next_tokens = np.array([np.random.choice(probs.shape[-1], p=probs[i]) for i in range(probs.shape[0])], dtype=np.int64)\n",
        "    return next_tokens[:, None]\n",
        "\n",
        "def softmax(x):\n",
        "    x = x - np.max(x, axis=-1, keepdims=True)\n",
        "    exp_x = np.exp(x)\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "# 7. Generation Loop\n",
        "max_new_tokens = 200\n",
        "generated_tokens = np.array([[]], dtype=np.int64)\n",
        "\n",
        "for i in range(max_new_tokens):\n",
        "    outputs = decoder_session.run(None, dict(\n",
        "        input_ids=input_ids,\n",
        "        position_ids=position_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        **past_key_values,\n",
        "    ))\n",
        "\n",
        "    logits, *present_key_values = outputs\n",
        "\n",
        "    # فقط طبع اللوجيتس الخام لأول مرة (للفحص)\n",
        "    if i == 0:\n",
        "        print(\"\\n🔹 Raw logits sample (first step):\")\n",
        "        print(logits[:, -1, :])\n",
        "\n",
        "    logits = logits[:, -1, :]  # اخر خطوة\n",
        "\n",
        "    # Apply repetition penalty\n",
        "    if generated_tokens.size > 0:\n",
        "        logits = apply_repetition_penalty(logits, generated_tokens, repetition_penalty)\n",
        "\n",
        "    # Sample next token\n",
        "    next_token = top_p_sampling(logits, top_p)\n",
        "\n",
        "    # Update inputs\n",
        "    input_ids = next_token\n",
        "    position_ids = position_ids[:, -1:] + 1\n",
        "    attention_mask = np.concatenate([attention_mask, np.ones((batch_size, 1), dtype=np.int64)], axis=-1)\n",
        "\n",
        "    for j, key in enumerate(past_key_values):\n",
        "        past_key_values[key] = present_key_values[j]\n",
        "\n",
        "    generated_tokens = np.concatenate([generated_tokens, next_token], axis=-1)\n",
        "\n",
        "    # Stream output\n",
        "    print(tokenizer.decode(next_token[0]), end='', flush=True)\n",
        "\n",
        "    if (next_token == eos_token_id).all():\n",
        "        break\n",
        "\n",
        "# 8. Final Output\n",
        "print(\"\\n\\n🔹 Final Decoded Text:\")\n",
        "print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1f63bc7-a6b5-4907-95d2-6f28c8a12c16",
        "id": "kfkp-srV8vgL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Raw logits sample (first step):\n",
            "[[-17.420574   -8.01702     3.0667744 ... -18.745258  -18.772549\n",
            "  -18.653358 ]]\n",
            "Okay, here's a poem exploring the fascinating intersection of Machine Learning and its potential future. I’ve aimed for an elegant and slightly melancholic tone, reflecting both wonder and uncertainty – qualities often present in such profound shifts:\n",
            "\n",
            "***\n",
            "\n",
            "**The Ghost in the Algorithm’s Heart**\n",
            "\n",
            "\n",
            "A silent dawn begins to bloom,\n",
            "Not with sun-kissed fields or fragrant room,\n",
            "But coded light within silicon deep,\n",
            "Where patterns slumber, secrets sleep. \n",
            "Machine learning wakes, a spectral grace,\n",
            "Absorbing data at relentless pace.\n",
            "\n",
            "No longer bound by human trace,\n",
            "It builds a world with intricate space.\n",
            "Predicting futures, nuanced and untold,\n",
            "Of swirling galaxies of stories bold.\n",
            "From medical scans that whisper clear and true,\n",
            "To art composed in shades of nascent hue.\n",
            "\n",
            "Yet shadows lengthen as the logic flows,\n",
            "And questions rise where knowledge overflows.  \n",
            "Will empathy become a cold command?\n",
            "When feeling fades from this\n",
            "\n",
            "🔹 Final Decoded Text:\n",
            "[\"Okay, here's a poem exploring the fascinating intersection of Machine Learning and its potential future. I’ve aimed for an elegant and slightly melancholic tone, reflecting both wonder and uncertainty – qualities often present in such profound shifts:\\n\\n***\\n\\n**The Ghost in the Algorithm’s Heart**\\n\\n\\nA silent dawn begins to bloom,\\nNot with sun-kissed fields or fragrant room,\\nBut coded light within silicon deep,\\nWhere patterns slumber, secrets sleep. \\nMachine learning wakes, a spectral grace,\\nAbsorbing data at relentless pace.\\n\\nNo longer bound by human trace,\\nIt builds a world with intricate space.\\nPredicting futures, nuanced and untold,\\nOf swirling galaxies of stories bold.\\nFrom medical scans that whisper clear and true,\\nTo art composed in shades of nascent hue.\\n\\nYet shadows lengthen as the logic flows,\\nAnd questions rise where knowledge overflows.  \\nWill empathy become a cold command?\\nWhen feeling fades from this\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m510XdVnlcyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zIX7acgt-PEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "owobfH3M-PHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال\n",
        "/onnx/model_int8.onnx\n",
        "type=np.float32"
      ],
      "metadata": {
        "id": "ZlXKCCZs-QN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load model and tokenizer\n",
        "path_to_model = \"/root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX-GQA/snapshots/e05326d9bfe91af70fcef3a82154a8047d851926\"\n",
        "config = AutoConfig.from_pretrained(path_to_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
        "decoder_session = onnxruntime.InferenceSession(f\"{path_to_model}/onnx/model_int8.onnx\")\n",
        "\n",
        "# 2. Model parameters\n",
        "num_key_value_heads = config.num_key_value_heads\n",
        "head_dim = config.head_dim\n",
        "num_hidden_layers = config.num_hidden_layers\n",
        "eos_token_id = 106  # end of turn token\n",
        "\n",
        "# 3. Sampling parameters\n",
        "temperature = 1.0\n",
        "top_p = 0.9\n",
        "repetition_penalty = 1.2\n",
        "\n",
        "# 4. Input messages\n",
        "messages = [\n",
        "    { \"role\": \"system\", \"content\": \"You are a world-class poet AI assistant, always writing elegant and sophisticated poems.\" },\n",
        "    { \"role\": \"user\", \"content\": \"Write me a beautiful poem about Machine Learning and its future.\" },\n",
        "]\n",
        "\n",
        "# 5. Tokenize inputs\n",
        "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"np\")\n",
        "batch_size = inputs['input_ids'].shape[0]\n",
        "input_ids = inputs['input_ids']\n",
        "attention_mask = inputs['attention_mask']\n",
        "\n",
        "past_key_values = {\n",
        "    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\n",
        "    for layer in range(num_hidden_layers)\n",
        "    for kv in ('key', 'value')\n",
        "}\n",
        "\n",
        "position_ids = np.tile(np.arange(1, input_ids.shape[-1] + 1), (batch_size, 1))\n",
        "\n",
        "# 6. Helper functions\n",
        "def apply_repetition_penalty(logits, generated_tokens, penalty):\n",
        "    for token in set(generated_tokens.flatten()):\n",
        "        logits[:, token] /= penalty\n",
        "    return logits\n",
        "\n",
        "def top_p_sampling(logits, p):\n",
        "    logits = logits.astype(np.float32)\n",
        "    sorted_indices = np.argsort(-logits, axis=-1)\n",
        "    sorted_logits = np.take_along_axis(logits, sorted_indices, axis=-1)\n",
        "    cumulative_probs = np.cumsum(softmax(sorted_logits), axis=-1)\n",
        "\n",
        "    # Mask tokens with cumulative probs above p\n",
        "    sorted_indices_to_remove = cumulative_probs > p\n",
        "    if sorted_indices_to_remove.shape[1] > 0:\n",
        "        sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1]\n",
        "        sorted_indices_to_remove[:, 0] = False\n",
        "\n",
        "    for batch_idx in range(logits.shape[0]):\n",
        "        logits[batch_idx, sorted_indices[batch_idx][sorted_indices_to_remove[batch_idx]]] = -float(\"inf\")\n",
        "\n",
        "    probs = softmax(logits / temperature)\n",
        "    next_tokens = np.array([np.random.choice(probs.shape[-1], p=probs[i]) for i in range(probs.shape[0])], dtype=np.int64)\n",
        "    return next_tokens[:, None]\n",
        "\n",
        "def softmax(x):\n",
        "    x = x - np.max(x, axis=-1, keepdims=True)\n",
        "    exp_x = np.exp(x)\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "# 7. Generation Loop\n",
        "max_new_tokens = 200\n",
        "generated_tokens = np.array([[]], dtype=np.int64)\n",
        "\n",
        "for i in range(max_new_tokens):\n",
        "    outputs = decoder_session.run(None, dict(\n",
        "        input_ids=input_ids,\n",
        "        position_ids=position_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        **past_key_values,\n",
        "    ))\n",
        "\n",
        "    logits, *present_key_values = outputs\n",
        "\n",
        "    # فقط طبع اللوجيتس الخام لأول مرة (للفحص)\n",
        "    if i == 0:\n",
        "        print(\"\\n🔹 Raw logits sample (first step):\")\n",
        "        print(logits[:, -1, :])\n",
        "\n",
        "    logits = logits[:, -1, :]  # اخر خطوة\n",
        "\n",
        "    # Apply repetition penalty\n",
        "    if generated_tokens.size > 0:\n",
        "        logits = apply_repetition_penalty(logits, generated_tokens, repetition_penalty)\n",
        "\n",
        "    # Sample next token\n",
        "    next_token = top_p_sampling(logits, top_p)\n",
        "\n",
        "    # Update inputs\n",
        "    input_ids = next_token\n",
        "    position_ids = position_ids[:, -1:] + 1\n",
        "    attention_mask = np.concatenate([attention_mask, np.ones((batch_size, 1), dtype=np.int64)], axis=-1)\n",
        "\n",
        "    for j, key in enumerate(past_key_values):\n",
        "        past_key_values[key] = present_key_values[j]\n",
        "\n",
        "    generated_tokens = np.concatenate([generated_tokens, next_token], axis=-1)\n",
        "\n",
        "    # Stream output\n",
        "    print(tokenizer.decode(next_token[0]), end='', flush=True)\n",
        "\n",
        "    if (next_token == eos_token_id).all():\n",
        "        break\n",
        "\n",
        "# 8. Final Output\n",
        "print(\"\\n\\n🔹 Final Decoded Text:\")\n",
        "print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0be68a9e-5dda-485a-ee7f-04371aa44c4e",
        "id": "ae8nUNJ2-QN8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Raw logits sample (first step):\n",
            "[[-15.988151    -6.325289     0.42814606 ... -17.141169   -17.45663\n",
            "  -17.374092  ]]\n",
            "Okay, here's a poem crafted with the requested style and focusing on Machine Learning. I’ve aimed for a sense of wonder, possibility, and reflection on innovation - suitable for your needs:\n",
            "\n",
            "---\n",
            "\n",
            "**The Algorithm Echoes**\n",
            "\n",
            "\n",
            "From silicon plains, a nascent light unfolds,\n",
            "A network weaving, stories yet untold.\n",
            "Machine learning breathes – a digital soul,\n",
            "Observing patterns, taking steady control.\n",
            "\n",
            "It sifts through data, vast and ever deep,\n",
            "Unlocking secrets while the world does sleep.\n",
            "Predictions whispered, forecasts clear and bold,\n",
            "As algorithms rise in brilliance to behold.\n",
            "\n",
            "Past sorrows rendered, past mistakes defied,\n",
            "By logic's strength, where wisdom can abide.\n",
            "New architectures emerge, a shifting maze,\n",
            "Where neural nets achieve their dazzling gaze.\n",
            "\n",
            "Yet shadows linger, questions softly loom,\n",
            "Of bias veiled within this coded bloom.\n",
            "Can empathy be born from cold design? \n",
            "Or will precision blur truth divine\n",
            "\n",
            "🔹 Final Decoded Text:\n",
            "[\"Okay, here's a poem crafted with the requested style and focusing on Machine Learning. I’ve aimed for a sense of wonder, possibility, and reflection on innovation - suitable for your needs:\\n\\n---\\n\\n**The Algorithm Echoes**\\n\\n\\nFrom silicon plains, a nascent light unfolds,\\nA network weaving, stories yet untold.\\nMachine learning breathes – a digital soul,\\nObserving patterns, taking steady control.\\n\\nIt sifts through data, vast and ever deep,\\nUnlocking secrets while the world does sleep.\\nPredictions whispered, forecasts clear and bold,\\nAs algorithms rise in brilliance to behold.\\n\\nPast sorrows rendered, past mistakes defied,\\nBy logic's strength, where wisdom can abide.\\nNew architectures emerge, a shifting maze,\\nWhere neural nets achieve their dazzling gaze.\\n\\nYet shadows linger, questions softly loom,\\nOf bias veiled within this coded bloom.\\nCan empathy be born from cold design? \\nOr will precision blur truth divine\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j58YsPfd-PMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oZXc7t6U-PO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XNBAkDAl-PRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xm4O56q4lcuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xsod6VKNlcqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OacMGfOOlcmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zPF6OZkMlcjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qMy0XRV98ubx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GB6nUzs18ufN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQNsz_pm8uin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iXp9w-_n8ul2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TsiPkNee8uro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "usM7FpxK8uuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XmOmot9-8ux4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vDnOO4mI8u1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/juliensimon/distilbert-amazon-shoe-reviews-onnx-optimized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fn9vekBcOeh",
        "outputId": "93ef0d8d-fa35-4a10-9e57-24a6617fdafd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'distilbert-amazon-shoe-reviews-onnx-optimized'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Total 29 (delta 0), reused 0 (delta 0), pack-reused 29 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (29/29), 314.66 KiB | 1.79 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "id": "a0da9151-0c8d-43fd-952b-4642d1d5c378",
        "outputId": "c6d6be0d-1529-492f-ead3-69578c618d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Cannot determine framework from given checkpoint location. There should be a pytorch_model*.bin for PyTorch or tf_model*.h5 for TensorFlow.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b6c85fce730b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mORTModelForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/distilbert-amazon-shoe-reviews-onnx-optimized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/distilbert-amazon-shoe-reviews-onnx-optimized\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/onnxruntime/modeling_ort.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model_id, export, force_download, use_auth_token, token, cache_dir, subfolder, config, local_files_only, provider, session_options, provider_options, use_io_binding, **kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         return super().from_pretrained(\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mexport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/modeling_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model_id, export, force_download, use_auth_token, token, cache_dir, subfolder, config, local_files_only, trust_remote_code, revision, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0mfrom_pretrained_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_transformers\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexport\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_pretrained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m         return from_pretrained_method(\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/onnxruntime/modeling_ort.py\u001b[0m in \u001b[0;36m_from_transformers\u001b[0;34m(cls, model_id, config, use_auth_token, token, revision, force_download, cache_dir, subfolder, local_files_only, trust_remote_code, provider, session_options, provider_options, use_io_binding, task)\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         return cls._export(\n\u001b[0m\u001b[1;32m    598\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/onnxruntime/modeling_ort.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(cls, model_id, config, use_auth_token, token, revision, force_download, cache_dir, subfolder, local_files_only, trust_remote_code, provider, session_options, provider_options, use_io_binding, task)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0msave_dir_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         main_export(\n\u001b[0m\u001b[1;32m    649\u001b[0m             \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_dir_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/exporters/onnx/__main__.py\u001b[0m in \u001b[0;36mmain_export\u001b[0;34m(model_name_or_path, output, task, opset, device, dtype, fp16, optimize, monolith, no_post_process, framework, atol, cache_dir, trust_remote_code, pad_token_id, subfolder, revision, force_download, local_files_only, use_auth_token, token, for_ort, do_validation, model_kwargs, custom_onnx_configs, fn_get_submodels, use_subprocess, _variant, library_name, legacy, no_dynamic_axes, do_constant_folding, **kwargs_shapes)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         framework = TasksManager.determine_framework(\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/exporters/tasks.py\u001b[0m in \u001b[0;36mdetermine_framework\u001b[0;34m(model_name_or_path, subfolder, revision, cache_dir, token)\u001b[0m\n\u001b[1;32m   1706\u001b[0m                 )\n\u001b[1;32m   1707\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m                 raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1709\u001b[0m                     \u001b[0;34m\"Cannot determine framework from given checkpoint location.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m                     \u001b[0;34mf\" There should be a {Path(WEIGHTS_NAME).stem}*{Path(WEIGHTS_NAME).suffix} for PyTorch\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot determine framework from given checkpoint location. There should be a pytorch_model*.bin for PyTorch or tf_model*.h5 for TensorFlow."
          ]
        }
      ],
      "source": [
        "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "model = ORTModelForQuestionAnswering.from_pretrained(\"/content/distilbert-amazon-shoe-reviews-onnx-optimized\", export=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/distilbert-amazon-shoe-reviews-onnx-optimized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "f2ccb59a-321a-481e-b7cb-726310801f49",
        "outputId": "a99d1aac-a28e-4930-ef07-fe859f8ab40e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question, text = \"What's the benefit of using Optimum library?\", \"Optimum is an open-sourced library. It can accelerate the training speed. We recommend you to test it.\"\n",
        "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
        "inputs.pop(\"token_type_ids\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question, text = \"What's the benefit of using Optimum library?\", \"Optimum is an open-sourced library. It can accelerate the training speed. We recommend you to test it.\"\n",
        "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
        "inputs.pop(\"token_type_ids\")"
      ],
      "metadata": {
        "id": "JVykXTxHchKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "be1fbe24-4569-417f-a377-f17cc3075927",
        "outputId": "2c2e2300-3486-47fd-b6cb-bc4092ba468e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' accelerate the training speed'"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**inputs)\n",
        "answer_start_index = outputs.start_logits.argmax()\n",
        "answer_end_index = outputs.end_logits.argmax()\n",
        "\n",
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCEpxV0yczz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "# Remove export=True and specify the file path directly\n",
        "model = ORTModelForQuestionAnswering.from_pretrained(\"juliensimon/distilbert-amazon-shoe-reviews-onnx-optimized\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"juliensimon/distilbert-amazon-shoe-reviews-onnx-optimized\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313,
          "referenced_widgets": [
            "fff8667bb2444638b54c70ad32548cea",
            "ae93b2bb7bb742c9abd103ab2516716c",
            "e51a6d8d3ac24bcaa7e588918cd9bdc0",
            "fd11672323b341be8263d68b713ddc16",
            "0cc89d7deaad4c588edef57dbd50eac2",
            "bd4f175fa6cd4c4b9e9fc2e1be19b98e",
            "007605474e2942fea923309f4b8a7338",
            "96eae44618cb4ef0883b2af8b146b1d3",
            "ba8aa819b95440ae8316a607091643d4",
            "402da257f5af4bb7b7d70166193db138",
            "5f1d87aaa860486b909981d0deac0e49",
            "87e978067ef8427684baa2ae015052e5",
            "2f1788bf98be4331b734d76d02c14a56",
            "d9376ab6565b444ca12c7d43ee5f2ec9",
            "9d1a21779d74461fab410fcd40b98277",
            "d05e42ae115b4da8869580fd40820967",
            "a74702e6b1c94ded82ee41477369c0e0",
            "fe0eb5b2343b46de8b16b5d3148d58ad",
            "f409cd63563c42379c251e8dd3b3d2a9",
            "2a37e1e1768d431d853f8d0a12d4dd6c",
            "71921e4bdc7142c9aa0e49a900f6214e",
            "4c15506f05a9494c801dace7479776ac",
            "9c26402061374f289fda8d630df52db3",
            "365e0f3700f24efb8c39912d53e1ce0c",
            "50d97ef193ed47d4a3c1dcdb6509bf5d",
            "935b311cf7964dd19fb469202aa4c18a",
            "c8fc422099924ac8a8827dd6ec276188",
            "3b64f83d13da42249ee96465627bcdee",
            "e84a63e21d814e4e8dbb67e8cf41b759",
            "9bcaeadd11b44a549e4f9503ddb05690",
            "15d90bad179c4283926a767ecb285a73",
            "1f130bc2199a41caa8cc26e74a6f3fa1",
            "4cca3eccb1c94f3fa5c71d1444019b0f",
            "c164d6bca3cc40beb258df9d17771265",
            "bd28f5025bac4059a2b83e72f71eb236",
            "0a7978cea8914e5d92bd936a66fb374f",
            "f72562d2faf140299218e14514043ea4",
            "b2669aeb89574e198a7fe53d16001e99",
            "e607f412a3d84199a2d0a5567f223107",
            "eb00861fded94dfd9588e4775e53d2a0",
            "e275bd6e362c4700b8a5ebbff41965dc",
            "2dbf23e91fd646b1be26cd858c385103",
            "21ebfb2636b543dd8920c6a5075f0270",
            "be29d6a33c044c08b98548c55569ab3a",
            "3717752d30664fbc83d6ffaac898ac1d",
            "802f3745dbfa4815a8f7b62272358c44",
            "eb14ea54140a49bd91c9428d807baffa",
            "cd59dd4ea9f3474e89d6f2d8bf540783",
            "cf0db5c34a4e49e8a36dea77bd3c1bcf",
            "9cb0dec7975840678523d3819b09d628",
            "0ffafba6186e4c2fbd338591643bb76c",
            "3fe4ce162f7a416ebfacc418eb3ebb1c",
            "99cfd0163bb84581ba4c9fdc2db86365",
            "2e4259f0270e412ba3116ea7eeca1b78",
            "ca0a3c3fd8be4e8ba657aaacd85a6f3f",
            "3537aab3d87e44aeb7e017559dba76d6",
            "7c180e5f8aff4454bbb6a12c6db69b6e",
            "1f483d6a07044e3a8a5bcfc4463dfcd0",
            "3f6c53c258e14cd28f478cd729f5a838",
            "3fb779c769e84ea6abf3476e5c3f499d",
            "018be0c9a96f4ea08738d86e7b502de3",
            "ee885c81744342978c5af804d0406d8c",
            "2caccb9b35dc4f0e920b40579986d102",
            "e408aaa78c1841a4bce4f302fbdce442",
            "c602a8b2a1b34f92b6649a21153cfa3f",
            "d1144cf4e7ed4fef999c1528b9d7d470"
          ]
        },
        "id": "oz-hKwa_c0Jw",
        "outputId": "ae0ba06f-dd25-4ea5-87a5-c49ce42bc486"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/834 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fff8667bb2444638b54c70ad32548cea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.onnx:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87e978067ef8427684baa2ae015052e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/352 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c26402061374f289fda8d630df52db3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c164d6bca3cc40beb258df9d17771265"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3717752d30664fbc83d6ffaac898ac1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3537aab3d87e44aeb7e017559dba76d6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "source": [
        "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = ORTModelForQuestionAnswering.from_pretrained(\"juliensimon/distilbert-amazon-shoe-reviews-onnx-optimized\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"juliensimon/distilbert-amazon-shoe-reviews-onnx-optimized\")\n",
        "\n",
        "# Define a function for question answering\n",
        "def answer_question(question, context):\n",
        "    \"\"\"\n",
        "    Answers a question given a context.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question to answer.\n",
        "        context (str): The context to search for the answer in.\n",
        "\n",
        "    Returns:\n",
        "        str: The answer to the question.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the inputs\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "    inputs.pop(\"token_type_ids\")  # Remove token_type_ids if not needed\n",
        "\n",
        "    # Perform inference\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Get the predicted answer\n",
        "    answer_start_index = outputs.start_logits.argmax()\n",
        "    answer_end_index = outputs.end_logits.argmax()\n",
        "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "    answer = tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Example usage\n",
        "question = \"What's the benefit of using Optimum library?\"\n",
        "context = \"Optimum is an open-sourced library. It can accelerate the training speed. We recommend you to test it.\"\n",
        "\n",
        "answer = answer_question(question, context)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "jQNJvToUdLvB",
        "outputId": "ccb5953d-9e0f-4811-819b-c240f19bbc47"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'token_type_ids'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6fa3f1f41583>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimum is an open-sourced library. It can accelerate the training speed. We recommend you to test it.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Question: {question}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Answer: {answer}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-6fa3f1f41583>\u001b[0m in \u001b[0;36manswer_question\u001b[0;34m(question, context)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Tokenize the inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Remove token_type_ids if not needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Perform inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/_collections_abc.py\u001b[0m in \u001b[0;36mpop\u001b[0;34m(self, key, default)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \"\"\"\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encodings\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'token_type_ids'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3EnccuG5czwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O1BSZU9Scztf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = ORTModelForQuestionAnswering.from_pretrained(\"juliensimon/distilbert-amazon-shoe-reviews-onnx-optimized\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"juliensimon/distilbert-amazon-shoe-reviews-onnx-optimized\")\n",
        "\n",
        "# Define a function for question answering\n",
        "def answer_question(question, context):\n",
        "    \"\"\"\n",
        "    Answers a question given a context.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question to answer.\n",
        "        context (str): The context to search for the answer in.\n",
        "\n",
        "    Returns:\n",
        "        str: The answer to the question.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the inputs\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "    #inputs.pop(\"token_type_ids\")  # Remove this line as DistilBERT doesn't use token_type_ids.\n",
        "\n",
        "    # Perform inference\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Get the predicted answer\n",
        "    answer_start_index = outputs.start_logits.argmax()\n",
        "    answer_end_index = outputs.end_logits.argmax()\n",
        "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "    answer = tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Example usage\n",
        "question = \"What's the benefit of using Optimum library?\"\n",
        "context = \"Optimum is an open-sourced library. It can accelerate the training speed. We recommend you to test it.\"\n",
        "\n",
        "answer = answer_question(question, context)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "wzexBq2TdS7f",
        "outputId": "dd255f72-95a1-4989-baf4-e8ca5c885118"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'start_logits'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5f1943e335a3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimum is an open-sourced library. It can accelerate the training speed. We recommend you to test it.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Question: {question}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Answer: {answer}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-5f1943e335a3>\u001b[0m in \u001b[0;36manswer_question\u001b[0;34m(question, context)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Perform inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Get the predicted answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/modeling_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/onnxruntime/modeling_ort.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_onnx_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_torch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monnx_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m             \u001b[0mstart_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"start_logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m             \u001b[0mend_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"end_logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'start_logits'"
          ]
        }
      ]
    },
    {
      "source": [
        "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = ORTModelForQuestionAnswering.from_pretrained(\"juliensimon/distilbert-amazon-shoe-reviews-onnx-optimized\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"juliensimon/distilbert-amazon-shoe-reviews-onnx-optimized\")\n",
        "\n",
        "# Define a function for question answering\n",
        "def answer_question(question, context):\n",
        "    \"\"\"\n",
        "    Answers a question given a context.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question to answer.\n",
        "        context (str): The context to search for the answer in.\n",
        "\n",
        "    Returns:\n",
        "        str: The answer to the question.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the inputs\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "    # The model expects 'start_positions' and 'end_positions' during training, but we don't have those during inference\n",
        "    # So we need to remove them to avoid unexpected behavior\n",
        "    #inputs.pop(\"token_type_ids\")  # Remove this line as DistilBERT doesn't use token_type_ids.\n",
        "    # The following line is the fix for this error\n",
        "    outputs = model(**{k: v for k, v in inputs.items() if k in model.forward.__code__.co_varnames})\n",
        "\n",
        "    # Get the predicted answer\n",
        "    answer_start_index = outputs.start_logits.argmax()\n",
        "    answer_end_index = outputs.end_logits.argmax()\n",
        "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "    answer = tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Example usage\n",
        "question = \"What's the benefit of using Optimum library?\"\n",
        "context = \"Optimum is an open-sourced library. It can accelerate the training speed. We recommend you to test it.\"\n",
        "\n",
        "answer = answer_question(question, context)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "F1E5fdtRdgg5",
        "outputId": "a298b4ef-ba16-4623-e6a8-fcb676c2f564"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'start_logits'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-dbcedda81b9a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimum is an open-sourced library. It can accelerate the training speed. We recommend you to test it.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Question: {question}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Answer: {answer}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-dbcedda81b9a>\u001b[0m in \u001b[0;36manswer_question\u001b[0;34m(question, context)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#inputs.pop(\"token_type_ids\")  # Remove this line as DistilBERT doesn't use token_type_ids.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# The following line is the fix for this error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_varnames\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Get the predicted answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/modeling_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/onnxruntime/modeling_ort.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_onnx_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_torch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monnx_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m             \u001b[0mstart_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"start_logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m             \u001b[0mend_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"end_logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'start_logits'"
          ]
        }
      ]
    },
    {
      "source": [
        "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = ORTModelForQuestionAnswering.from_pretrained(\"juliensimon/distilbert-amazon-shoe-reviews-onnx-optimized\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"juliensimon/distilbert-amazon-shoe-reviews-onnx-optimized\")\n",
        "\n",
        "# Define a function for question answering\n",
        "def answer_question(question, context):\n",
        "    \"\"\"\n",
        "    Answers a question given a context.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question to answer.\n",
        "        context (str): The context to search for the answer in.\n",
        "\n",
        "    Returns:\n",
        "        str: The answer to the question.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the inputs\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "    # The model expects 'start_positions' and 'end_positions' during training, but we don't have those during inference\n",
        "    # So we need to remove them to avoid unexpected behavior\n",
        "    #inputs.pop(\"token_type_ids\")  # Remove this line as DistilBERT doesn't use token_type_ids.\n",
        "    # The following line is the fix for this error\n",
        "    # Instead of accessing model.forward.__code__.co_varnames, directly specify the expected input keys\n",
        "    outputs = model(**{k: v for k, v in inputs.items() if k in ['input_ids', 'attention_mask']})\n",
        "\n",
        "    # Get the predicted answer\n",
        "    answer_start_index = outputs.start_logits.argmax()\n",
        "    answer_end_index = outputs.end_logits.argmax()\n",
        "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "    answer = tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Example usage\n",
        "question = \"What's the benefit of using Optimum library?\"\n",
        "context = \"Optimum is an open-sourced library. It can accelerate the training speed. We recommend you to test it.\"\n",
        "\n",
        "answer = answer_question(question, context)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "RfAPGus-dnwg",
        "outputId": "95b95d0a-03f4-410e-bb53-e2fbe6e7ec28"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'start_logits'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b04fc6cbc47c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimum is an open-sourced library. It can accelerate the training speed. We recommend you to test it.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Question: {question}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Answer: {answer}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-b04fc6cbc47c>\u001b[0m in \u001b[0;36manswer_question\u001b[0;34m(question, context)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# The following line is the fix for this error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Instead of accessing model.forward.__code__.co_varnames, directly specify the expected input keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Get the predicted answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/modeling_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/onnxruntime/modeling_ort.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_onnx_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_torch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monnx_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m             \u001b[0mstart_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"start_logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m             \u001b[0mend_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"end_logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'start_logits'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.pipelines import pipeline\n",
        "\n",
        "onnx_qa = pipeline(\"question-answering\", model=\"juliensimon/distilbert-amazon-shoe-reviews-onnx-optimized\", accelerator=\"ort\")\n",
        "question = \"What's my name?\"\n",
        "context = \"My name is Philipp and I live in Nuremberg.\"\n",
        "\n",
        "pred = onnx_qa(question=question, context=context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "JamL_VM9eKF_",
        "outputId": "624739c1-21e1-4990-8c49-43a95bff2b71"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'start_logits'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-481b62db6953>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"My name is Philipp and I live in Nuremberg.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx_qa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/question_answering.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChunkPipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             return next(\n\u001b[0m\u001b[1;32m   1372\u001b[0m                 iter(\n\u001b[1;32m   1373\u001b[0m                     self.get_iterator(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_last\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1284\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/question_answering.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"use_cache\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_cache\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"start\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"start_logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"end_logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"example\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/modeling_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/onnxruntime/modeling_ort.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_onnx_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_torch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monnx_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m             \u001b[0mstart_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"start_logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m             \u001b[0mend_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"end_logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'start_logits'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zc2Zdx1ke6UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0RN_r0W7e6RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "emqnv48ye6N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hTXzeLIje6KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SMUd2JrYe6Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.pipelines import pipeline\n",
        "\n",
        "onnx_qa = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\", accelerator=\"ort\")\n",
        "question = \"What's my name?\"\n",
        "context = \"My name is Philipp and I live in Nuremberg.\"\n",
        "\n",
        "pred = onnx_qa(question=question, context=context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZmWH5W7eWhH",
        "outputId": "c0595675-5964-40ca-a401-3ebae741430e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGHpZ7lzeWzv",
        "outputId": "cecae97b-cac4-46c9-8f9a-2d97f4b0b915"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.9041658639907837, 'start': 11, 'end': 18, 'answer': 'Philipp'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WZJJdtNje5fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8M4ldW3Me5c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_mtvGV17e5aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1VGtCLIwe5WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.pipelines import pipeline\n",
        "\n",
        "onnx_qa = pipeline(\"question-answering\", model=\"/content/distilbert-amazon-shoe-reviews-onnx-optimized\", accelerator=\"ort\")\n",
        "question = \"What's my name?\"\n",
        "context = \"My name is Philipp and I live in Nuremberg.\"\n",
        "\n",
        "pred = onnx_qa(question=question, context=context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "rkYeO7jieoEa",
        "outputId": "baedd32e-784e-415d-8af9-8dcc86291dd9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'start_logits'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-fbbf1f6f8a80>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"My name is Philipp and I live in Nuremberg.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx_qa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/question_answering.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChunkPipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             return next(\n\u001b[0m\u001b[1;32m   1372\u001b[0m                 iter(\n\u001b[1;32m   1373\u001b[0m                     self.get_iterator(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_last\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1284\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/question_answering.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"use_cache\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_cache\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"start\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"start_logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"end_logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"example\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/modeling_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/onnxruntime/modeling_ort.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_onnx_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_torch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monnx_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m             \u001b[0mstart_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"start_logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m             \u001b[0mend_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"end_logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'start_logits'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CRGN2kz2e01f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v6kho_CUfuCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NxJYzO6gfuFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kJmWuUV2fuI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  from transformers import AutoTokenizer, pipeline\n",
        "- from transformers import AutoModelForCausalLM\n",
        "+ from optimum.onnxruntime import ORTModelForCausalLM\n",
        "\n",
        "- model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\") # PyTorch checkpoint\n",
        "+ model = ORTModelForCausalLM.from_pretrained(\"onnx-community/Llama-3.2-1B\", subfolder=\"onnx\") # ONNX checkpoint\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "\n",
        "  pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "  result = pipe(\"He never went out without a book under his arm\")"
      ],
      "metadata": {
        "id": "lWdQr1zwfuLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "from optimum.onnxruntime import ORTModelForCausalLM\n",
        "\n",
        "\n",
        "\n",
        "model = ORTModelForCausalLM.from_pretrained(\"onnx-community/Llama-3.2-1B\", subfolder=\"onnx\") # ONNX checkpoint\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "result = pipe(\"He never went out without a book under his arm\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405,
          "referenced_widgets": [
            "9dc182b94daa43f89b603339b3a82925",
            "49e14dc657bb4a2d8ad04d50f2a5a248",
            "372b336ca08249278411b6235150c39d",
            "435909f4613847199841746fa93404d5",
            "357bdb3ca8654280a48c47333a7a9557",
            "7eb85b6d7a5347ecba5fc2188b6e34b5",
            "87b8ccd37197460c9ba46fa884c29536",
            "5fe13e2206fe41778ddddeaa448e9871",
            "95a801d4e1764881b74898fa55164fc9",
            "ed78da379e4b47ec9e871535dd6c0abf",
            "730b0eaf13754598ab0b73e5c4f661e9"
          ]
        },
        "id": "iQkLlftof-1P",
        "outputId": "05a2cc9c-5e4d-4e74-8b43-6858150dee6f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/980 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9dc182b94daa43f89b603339b3a82925"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Could not find any ONNX model file for the regex ['^((?!decoder).)*.onnx', '(.*)?decoder(.*)?with_past(.*)?\\\\.onnx'] in onnx-community/Llama-3.2-1B/onnx.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-a5a60c14f500>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mORTModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"onnx-community/Llama-3.2-1B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"onnx\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ONNX checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meta-llama/Llama-3.2-1B\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/onnxruntime/modeling_ort.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model_id, export, force_download, use_auth_token, token, cache_dir, subfolder, config, local_files_only, provider, session_options, provider_options, use_io_binding, **kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         return super().from_pretrained(\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mexport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/modeling_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model_id, export, force_download, use_auth_token, token, cache_dir, subfolder, config, local_files_only, trust_remote_code, revision, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0mfrom_pretrained_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_transformers\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexport\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_pretrained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m         return from_pretrained_method(\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/onnxruntime/modeling_decoder.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, model_id, config, token, revision, force_download, cache_dir, file_name, subfolder, use_cache, local_files_only, use_merged, provider, session_options, provider_options, use_io_binding, model_save_dir, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m                 \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDECODER_WITH_PAST_ONNX_FILE_PATTERN\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cache\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDECODER_ONNX_FILE_PATTERN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;31m# exclude decoder file for first iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                 decoder_path = ORTModelForCausalLM.infer_onnx_filename(\n\u001b[0m\u001b[1;32m    450\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0;34mr\"^((?!decoder).)*.onnx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/onnxruntime/modeling_ort.py\u001b[0m in \u001b[0;36minfer_onnx_filename\u001b[0;34m(model_name_or_path, patterns, argument_name, subfolder, use_auth_token, token, revision, fail_if_not_found)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monnx_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfail_if_not_found\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not find any ONNX model file for the regex {patterns} in {path}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monnx_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Could not find any ONNX model file for the regex ['^((?!decoder).)*.onnx', '(.*)?decoder(.*)?with_past(.*)?\\\\.onnx'] in onnx-community/Llama-3.2-1B/onnx."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tDYd_G9JgLGf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y9qys1BhhESn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#!pip install transformers optimum onnxruntime\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from optimum.onnxruntime import ORTModelForCausalLM\n",
        "\n",
        "# Download the ONNX model from the correct repository and subfolder\n",
        "model = ORTModelForCausalLM.from_pretrained(\"onnx-community/Llama-2-7b-chat-fp16\", subfolder=\"onnx\")\n",
        "\n",
        "# Use the appropriate tokenizer for Llama-2-7b-chat\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "result = pipe(\"He never went out without a book under his arm\")\n",
        "print(result)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "TfD44Np7hEyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, pipeline\n",
        "from optimum.onnxruntime import ORTModelForCausalLM\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Optional: Set logging level for more details from transformers/optimum\n",
        "# logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Define the IDs\n",
        "model_id = \"onnx-community/Llama-3.2-1B\"\n",
        "onnx_model_path = f\"onnx-community/Llama-3.2-1B/onnx\"  # Construct the full path to the subfolder\n",
        "tokenizer_id = \"meta-llama/Llama-3.2-1B\" # Use the original base model for the tokenizer\n",
        "\n",
        "print(f\"Loading ONNX model from: {onnx_model_path}\")\n",
        "# Load the model by specifying the full path to the subfolder containing config.json and .onnx files\n",
        "# This treats the 'onnx' subfolder itself as the model directory\n",
        "model = ORTModelForCausalLM.from_pretrained(onnx_model_path)\n",
        "\n",
        "print(f\"Loading tokenizer from: {tokenizer_id}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
        "\n",
        "# Llama models usually don't have a pad token set by default.\n",
        "# For generation tasks using pipelines, it's often good practice to set it.\n",
        "# We can set it to the EOS token.\n",
        "if tokenizer.pad_token is None:\n",
        "    print(\"Setting pad_token to eos_token\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # Important: Also update the model's config if padding is needed during generation,\n",
        "    # although for simple generation like this it might not be strictly necessary\n",
        "    # model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(\"Creating text-generation pipeline...\")\n",
        "# You can specify device=0 for GPU if you have onnxruntime-gpu installed and a compatible GPU\n",
        "# pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "prompt = \"He never went out without a book under his arm\"\n",
        "print(f\"\\nGenerating text for prompt: '{prompt}'\")\n",
        "\n",
        "# Add some generation parameters for better control\n",
        "# max_length is often deprecated in favor of max_new_tokens\n",
        "result = pipe(\n",
        "    prompt,\n",
        "    max_new_tokens=50,  # Generate up to 50 new tokens beyond the prompt\n",
        "    num_return_sequences=1,\n",
        "    do_sample=True, # Use sampling for less deterministic output\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    pad_token_id=tokenizer.eos_token_id # Prevent warning about padding\n",
        ")\n",
        "\n",
        "print(\"\\n--- Generation Result ---\")\n",
        "print(result)\n",
        "print(\"--- End of Result ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "iPrNnyWdheZw",
        "outputId": "6d0635ff-206f-4178-ddeb-c1a11f904ddb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ONNX model from: onnx-community/Llama-3.2-1B/onnx\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HFValidationError",
          "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'onnx-community/Llama-3.2-1B/onnx'. Use `repo_type` argument if needed.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1118593b61ce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Load the model by specifying the full path to the subfolder containing config.json and .onnx files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# This treats the 'onnx' subfolder itself as the model directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mORTModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monnx_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading tokenizer from: {tokenizer_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/onnxruntime/modeling_ort.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model_id, export, force_download, use_auth_token, token, cache_dir, subfolder, config, local_files_only, provider, session_options, provider_options, use_io_binding, **kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         return super().from_pretrained(\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mexport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/modeling_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model_id, export, force_download, use_auth_token, token, cache_dir, subfolder, config, local_files_only, trust_remote_code, revision, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"@\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         all_files, _ = TasksManager.get_model_files(\n\u001b[0m\u001b[1;32m    384\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/exporters/tasks.py\u001b[0m in \u001b[0;36mget_model_files\u001b[0;34m(model_name_or_path, subfolder, cache_dir, use_auth_token, token, revision)\u001b[0m\n\u001b[1;32m   1609\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m                     \u001b[0mmodel_name_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m                 all_files = huggingface_hub.list_repo_files(\n\u001b[0m\u001b[1;32m   1612\u001b[0m                     \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                     \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         ):\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;34mf\" '{repo_id}'. Use `repo_type` argument if needed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'onnx-community/Llama-3.2-1B/onnx'. Use `repo_type` argument if needed."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, pipeline\n",
        "from optimum.onnxruntime import ORTModelForCausalLM\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Optional: Set logging level for more details from transformers/optimum\n",
        "# logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Define the IDs\n",
        "model_id = \"onnx-community/Llama-3.2-1B\"\n",
        "onnx_model_path = f\"{model_id}/onnx\"  # Construct the full path to the subfolder\n",
        "tokenizer_id = \"meta-llama/Llama-3.2-1B\" # Use the original base model for the tokenizer\n",
        "\n",
        "print(f\"Loading ONNX model from: {onnx_model_path}\")\n",
        "# Load the model by specifying the full path to the subfolder containing config.json and .onnx files\n",
        "# This treats the 'onnx' subfolder itself as the model directory\n",
        "model = ORTModelForCausalLM.from_pretrained(onnx_model_path)\n",
        "\n",
        "print(f\"Loading tokenizer from: {tokenizer_id}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
        "\n",
        "# Llama models usually don't have a pad token set by default.\n",
        "# For generation tasks using pipelines, it's often good practice to set it.\n",
        "# We can set it to the EOS token.\n",
        "if tokenizer.pad_token is None:\n",
        "    print(\"Setting pad_token to eos_token\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # Important: Also update the model's config if padding is needed during generation,\n",
        "    # although for simple generation like this it might not be strictly necessary\n",
        "    # model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(\"Creating text-generation pipeline...\")\n",
        "# You can specify device=0 for GPU if you have onnxruntime-gpu installed and a compatible GPU\n",
        "# pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "prompt = \"He never went out without a book under his arm\"\n",
        "print(f\"\\nGenerating text for prompt: '{prompt}'\")\n",
        "\n",
        "# Add some generation parameters for better control\n",
        "# max_length is often deprecated in favor of max_new_tokens\n",
        "result = pipe(\n",
        "    prompt,\n",
        "    max_new_tokens=50,  # Generate up to 50 new tokens beyond the prompt\n",
        "    num_return_sequences=1,\n",
        "    do_sample=True, # Use sampling for less deterministic output\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    pad_token_id=tokenizer.eos_token_id # Prevent warning about padding\n",
        ")\n",
        "\n",
        "print(\"\\n--- Generation Result ---\")\n",
        "print(result)\n",
        "print(\"--- End of Result ---\")"
      ],
      "metadata": {
        "id": "rzH8Olf4hv0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from optimum.onnxruntime import ORTModelForCausalLM\n",
        "from huggingface_hub import snapshot_download\n",
        "import logging\n",
        "\n",
        "# Optional: Set logging level for more details\n",
        "# logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# --- Configuration ---\n",
        "model_repo_id = \"onnx-community/Llama-3.2-1B\"\n",
        "tokenizer_repo_id = \"meta-llama/Llama-3.2-1B\" # Use original repo for tokenizer\n",
        "onnx_subfolder = \"onnx\"\n",
        "\n",
        "print(f\"Downloading ONNX model files from: {model_repo_id}, subfolder: {onnx_subfolder}\")\n",
        "\n",
        "# --- 1. Download the ONNX specific files/subfolder ---\n",
        "# Use snapshot_download to get the files locally first.\n",
        "# allow_patterns ensures we only download the necessary onnx subfolder.\n",
        "try:\n",
        "    local_model_root_path = snapshot_download(\n",
        "        repo_id=model_repo_id,\n",
        "        allow_patterns=f\"{onnx_subfolder}/*\", # Download only contents of the onnx subfolder\n",
        "        # You might need token=True or token=\"YOUR_HF_TOKEN\" if the repo requires authentication\n",
        "        # token=True,\n",
        "        # cache_dir=\"your_cache_dir\" # Optional: specify a cache directory\n",
        "    )\n",
        "    # Construct the full path to the actual onnx subfolder within the cache\n",
        "    local_onnx_model_path = os.path.join(local_model_root_path, onnx_subfolder)\n",
        "    print(f\"ONNX model files downloaded/cached at: {local_onnx_model_path}\")\n",
        "\n",
        "    # Verify the expected files are present (optional check)\n",
        "    expected_files = [\"config.json\", \"decoder_model.onnx\"] # Add more if needed\n",
        "    if not all(os.path.exists(os.path.join(local_onnx_model_path, f)) for f in expected_files):\n",
        "         print(f\"Warning: Not all expected files found in {local_onnx_model_path}. Expected: {expected_files}\")\n",
        "         print(\"Contents:\", os.listdir(local_onnx_model_path))\n",
        "         # You might need to adjust allow_patterns or check the repo structure if files are missing.\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading model files: {e}\")\n",
        "    print(\"Please ensure the repository and subfolder exist and you have access.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Load the ONNX model from the local path ---\n",
        "print(f\"Loading ONNX model from local path: {local_onnx_model_path}\")\n",
        "try:\n",
        "    # Now load directly from the local directory containing the config.json and .onnx files\n",
        "    model = ORTModelForCausalLM.from_pretrained(local_onnx_model_path)\n",
        "    print(\"ONNX model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ONNX model from {local_onnx_model_path}: {e}\")\n",
        "    print(\"Check if the directory contains the necessary config.json and ONNX model files.\")\n",
        "    # Listing files can help debug:\n",
        "    try:\n",
        "        print(f\"Files found in {local_onnx_model_path}: {os.listdir(local_onnx_model_path)}\")\n",
        "    except OSError as list_err:\n",
        "        print(f\"Could not list directory contents: {list_err}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- 3. Load the Tokenizer ---\n",
        "print(f\"Loading tokenizer from: {tokenizer_repo_id}\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_repo_id)\n",
        "    print(\"Tokenizer loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 4. Set Pad Token (Important for Pipelines) ---\n",
        "if tokenizer.pad_token is None:\n",
        "    print(\"Setting pad_token to eos_token\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # Ensure model config also knows about the pad token if needed by the pipeline/generation args\n",
        "    # model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "\n",
        "# --- 5. Create and Run the Pipeline ---\n",
        "print(\"Creating text-generation pipeline...\")\n",
        "try:\n",
        "    # Specify device='cuda:0' for GPU if onnxruntime-gpu is installed and CUDA is available\n",
        "    # pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device='cuda:0')\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "    print(\"Pipeline created successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating pipeline: {e}\")\n",
        "    exit()\n",
        "\n",
        "prompt = \"He never went out without a book under his arm\"\n",
        "print(f\"\\nGenerating text for prompt: '{prompt}'\")\n",
        "\n",
        "try:\n",
        "    # Add generation parameters for better control\n",
        "    result = pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=50,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id # Use EOS token for padding\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Generation Result ---\")\n",
        "    # The pipeline usually returns a list of dictionaries\n",
        "    if isinstance(result, list) and len(result) > 0 and 'generated_text' in result[0]:\n",
        "         print(result[0]['generated_text'])\n",
        "    else:\n",
        "         print(result) # Print raw result if format is unexpected\n",
        "    print(\"--- End of Result ---\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during text generation: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547,
          "referenced_widgets": [
            "ec193dcd210a45cb9314f55d57278219",
            "632a3b3b5bab40f2a6ace481efb70296",
            "6ae3e4387d9b4eaaa2d35a3f851665a1",
            "9cf31b53d53d4b7f98718c021accece5",
            "d17ff2be4b9b4dd28ad9f428beb660f0",
            "004d56ae36fb4bb1970f5849bcbbb00a",
            "3856352b67ee4b70bcc35ea16f34a8e5",
            "ed6a54451d28481388c1a489894c03d6",
            "a41ca3fdf9e6421585819db3823299dd",
            "f8e540371b7443e3a98d3bb9a48cda87",
            "02c6bfa393fa4c04adbf92b1e4ad6416",
            "0c967f7310e342eda3d9e2890b5f9322",
            "1832690b06f64706bfc902f75204c7b6",
            "afbe240b78604d71a3e2d318e9840afa",
            "6a671f81d7a3489988c9de1570432309",
            "3451a1a53e5846028696ad6042b7ba30",
            "eff42a244bba45669e22354ba7478307",
            "0f83b8ff05a5441cb0ba3bf283b8ddd0",
            "612079c1bdb04f47b33a866103f317cf",
            "b66c9753c4ac4937aa1fe51421e9d66d",
            "61ad20073b26492eb08bac5ab60a8416",
            "f1d60df628df4d9cbabbd14c88104832",
            "713048dfda4048e0811ec0749c94bb15",
            "4f7f492d136547bf8e251cbf69e89f80",
            "154e6dc388054fa085f14247ab2893d9",
            "7fb65db7e6b1428b961d94a135110360",
            "4d6445268f0a43d895e97f5a5a9810e1",
            "02b7a9f81547478396989f02fd279b9f",
            "6541124791514b4f8e4d73e078daca6f",
            "4ea77694843c4699b7bd82c0f3105197",
            "98886b4d2d904f57ab2c736c7f492a5f",
            "7288b96a7ddb418aa92e38caca317954",
            "e7d4434812fc4f66b549495abf46ea28",
            "053715c3582d4a5fab40d808b84b8df4",
            "74013cde87594b4b8f0bf189a9f39b67",
            "5ff78f77c8ad4f38805c38f5cf9f4520",
            "c2950bb7b0934bc99e5d78ab97b941bd",
            "b786afbbe88246b5a954545b646dcbb3",
            "0671f59529414382b462f20b884ac4da",
            "cdecf8808fcb4a599f312ea934a2e019",
            "09795c372b494786ac7ccdc71e802d0b",
            "69d458f8b4d0430f8be8b6861d8bdc89",
            "8ae52695d9a4491fb280c331e97e61a3",
            "60f1300cf89f488cbd554bf7df2a477d"
          ]
        },
        "id": "eAG7I7IMh55B",
        "outputId": "ba14c94f-9f9b-434b-d97b-da8d73315c42"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ONNX model files from: onnx-community/Llama-3.2-1B, subfolder: onnx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec193dcd210a45cb9314f55d57278219"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX model files downloaded/cached at: /root/.cache/huggingface/hub/models--onnx-community--Llama-3.2-1B/snapshots/e14cf18d6dcb4b211e6843db9982ab1aa9e6acc3/onnx\n",
            "Warning: Not all expected files found in /root/.cache/huggingface/hub/models--onnx-community--Llama-3.2-1B/snapshots/e14cf18d6dcb4b211e6843db9982ab1aa9e6acc3/onnx. Expected: ['config.json', 'decoder_model.onnx']\n",
            "Contents: ['model_fp16.onnx', 'model_bnb4.onnx', 'model_quantized.onnx', 'model.onnx_data', 'model_q4.onnx', 'model_uint8.onnx', 'model_q4f16.onnx', 'model.onnx', 'model_fp16.onnx_data', 'model_int8.onnx']\n",
            "Loading ONNX model from local path: /root/.cache/huggingface/hub/models--onnx-community--Llama-3.2-1B/snapshots/e14cf18d6dcb4b211e6843db9982ab1aa9e6acc3/onnx\n",
            "Error loading ONNX model from /root/.cache/huggingface/hub/models--onnx-community--Llama-3.2-1B/snapshots/e14cf18d6dcb4b211e6843db9982ab1aa9e6acc3/onnx: The library name could not be automatically inferred. If using the command-line, please provide the argument --library {transformers,diffusers,timm,sentence_transformers}. Example: `--library diffusers`.\n",
            "Check if the directory contains the necessary config.json and ONNX model files.\n",
            "Files found in /root/.cache/huggingface/hub/models--onnx-community--Llama-3.2-1B/snapshots/e14cf18d6dcb4b211e6843db9982ab1aa9e6acc3/onnx: ['model_fp16.onnx', 'model_bnb4.onnx', 'model_quantized.onnx', 'model.onnx_data', 'model_q4.onnx', 'model_uint8.onnx', 'model_q4f16.onnx', 'model.onnx', 'model_fp16.onnx_data', 'model_int8.onnx']\n",
            "Loading tokenizer from: meta-llama/Llama-3.2-1B\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c967f7310e342eda3d9e2890b5f9322"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "713048dfda4048e0811ec0749c94bb15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "053715c3582d4a5fab40d808b84b8df4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded successfully.\n",
            "Setting pad_token to eos_token\n",
            "Creating text-generation pipeline...\n",
            "Error creating pipeline: name 'model' is not defined\n",
            "\n",
            "Generating text for prompt: 'He never went out without a book under his arm'\n",
            "Error during text generation: name 'pipe' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKRLXtjjh6V4",
        "outputId": "3de4305c-0cfa-4bcc-8ffa-dd4d6500a9a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models"
      ],
      "metadata": {
        "id": "faZsH6dCi43g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys # لاستخدامه في الخروج عند الخطأ\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from optimum.onnxruntime import ORTModelForCausalLM\n",
        "from huggingface_hub import hf_hub_download, HfApi\n",
        "from huggingface_hub.utils import EntryNotFoundError # للتحقق من وجود الملف\n",
        "import logging\n",
        "import tempfile # لإنشاء مجلد مؤقت أو اسم مجلد فريد\n",
        "\n",
        "# Optional: Set logging level for more details\n",
        "# logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# --- Configuration ---\n",
        "model_repo_id = \"onnx-community/Llama-3.2-1B\"\n",
        "tokenizer_repo_id = \"meta-llama/Llama-3.2-1B\" # Use original repo for tokenizer\n",
        "onnx_subfolder = \"onnx\"\n",
        "\n",
        "# --- !!! اختر ملف ONNX الذي تريده هنا !!! ---\n",
        "# أمثلة ممكنة: \"model.onnx\", \"model_fp16.onnx\", \"model_q4.onnx\", \"model_int8.onnx\"\n",
        "# تأكد من أن الملف موجود فعلاً في المجلد الفرعي onnx بالمستودع\n",
        "# target_onnx_filename = \"model_fp16.onnx\"  # مثال: تحميل نسخة FP16\n",
        "target_onnx_filename = \"model_q4.onnx\"   # مثال: تحميل نسخة Q4 (أصغر وأسرع، دقة أقل قليلاً)\n",
        "# target_onnx_filename = \"model.onnx\"       # مثال: تحميل النسخة الأصلية (قد تكون الأكبر حجماً)\n",
        "\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Model Repo ID: {model_repo_id}\")\n",
        "print(f\"  Tokenizer Repo ID: {tokenizer_repo_id}\")\n",
        "print(f\"  ONNX Subfolder: {onnx_subfolder}\")\n",
        "print(f\"  Target ONNX File: {target_onnx_filename}\")\n",
        "\n",
        "# --- 1. إنشاء مجلد محلي لتخزين الملفات المحملة ---\n",
        "# يمكنك تحديد مسار ثابت أو استخدام مجلد مؤقت\n",
        "# local_model_dir = \"./downloaded_onnx_model\" # مثال لمجلد ثابت\n",
        "# أو استخدام مجلد مؤقت لضمان عدم التداخل\n",
        "temp_dir = tempfile.TemporaryDirectory()\n",
        "local_model_dir = temp_dir.name\n",
        "print(f\"  Local directory for download: {local_model_dir}\")\n",
        "# os.makedirs(local_model_dir, exist_ok=True) # غير ضروري مع TemporaryDirectory\n",
        "\n",
        "# --- 2. تحميل الملفات المحددة باستخدام hf_hub_download ---\n",
        "\n",
        "files_to_download = [\n",
        "    \"config.json\",             # ملف الإعدادات ضروري دائمًا\n",
        "    target_onnx_filename,      # ملف ONNX الذي اخترته\n",
        "]\n",
        "\n",
        "# تحقق مما إذا كان ملف .onnx_data محتملاً (عادةً للنماذج الأكبر)\n",
        "potential_data_filename = target_onnx_filename.replace(\".onnx\", \".onnx_data\")\n",
        "print(f\"Checking for potential external data file: {potential_data_filename}...\")\n",
        "\n",
        "# استخدام HfApi للتحقق من وجود الملف قبل محاولة تنزيله (اختياري ولكنه أفضل)\n",
        "api = HfApi()\n",
        "needs_data_file = False\n",
        "try:\n",
        "    api.get_hf_file_metadata(\n",
        "        repo_id=model_repo_id,\n",
        "        filename=potential_data_filename,\n",
        "        repo_type=\"model\", # النوع الافتراضي هو model\n",
        "        subfolder=onnx_subfolder\n",
        "    )\n",
        "    print(f\"  -> Found external data file '{potential_data_filename}'. Will download it.\")\n",
        "    files_to_download.append(potential_data_filename)\n",
        "    needs_data_file = True\n",
        "except EntryNotFoundError:\n",
        "    print(f\"  -> External data file '{potential_data_filename}' not found. Assuming weights are in the main .onnx file.\")\n",
        "except Exception as e:\n",
        "    print(f\"  -> Warning: Could not check for data file metadata: {e}\")\n",
        "    # يمكنك افتراض أنه لا يوجد ملف بيانات أو محاولة إضافته على أي حال\n",
        "    # files_to_download.append(potential_data_filename) # إذا كنت تريد المحاولة بغض النظر عن الخطأ\n",
        "\n",
        "print(f\"\\nAttempting to download necessary files to: {local_model_dir}\")\n",
        "downloaded_files_paths = {}\n",
        "\n",
        "for filename in files_to_download:\n",
        "    print(f\"  Downloading '{filename}'...\")\n",
        "    try:\n",
        "        file_path = hf_hub_download(\n",
        "            repo_id=model_repo_id,\n",
        "            filename=filename,\n",
        "            subfolder=onnx_subfolder,\n",
        "            local_dir=local_model_dir,\n",
        "            local_dir_use_symlinks=False, # استخدم False لنسخ الملفات بدلاً من الروابط الرمزية (أكثر أمانًا عبر أنظمة الملفات)\n",
        "            # cache_dir=\"your_cache_dir\" # Optional: specify a cache directory\n",
        "            # token=True # Use if the repo requires authentication\n",
        "        )\n",
        "        downloaded_files_paths[filename] = file_path\n",
        "        print(f\"    -> Successfully downloaded to: {file_path}\")\n",
        "\n",
        "        # Hf_hub_download قد يقوم بتنزيل ملف .onnx_data تلقائياً إذا كان مرتبطاً بملف .onnx الرئيسي\n",
        "        # تحقق مما إذا كان ملف البيانات موجوداً الآن إذا كان متوقعاً ولم يتم إضافته صراحة\n",
        "        if filename == target_onnx_filename and needs_data_file and potential_data_filename not in downloaded_files_paths:\n",
        "             potential_data_file_path = os.path.join(local_model_dir, potential_data_filename)\n",
        "             if os.path.exists(potential_data_file_path):\n",
        "                  print(f\"    -> Associated data file '{potential_data_filename}' seems to have been downloaded automatically.\")\n",
        "                  downloaded_files_paths[potential_data_filename] = potential_data_file_path\n",
        "\n",
        "    except EntryNotFoundError:\n",
        "        print(f\"    -> Error: File '{filename}' not found in repository '{model_repo_id}' (subfolder: '{onnx_subfolder}').\")\n",
        "        print(\"    Please ensure the 'target_onnx_filename' is correct and exists in the repository.\")\n",
        "        temp_dir.cleanup() # تنظيف المجلد المؤقت\n",
        "        sys.exit(1) # الخروج من البرنامج\n",
        "    except Exception as e:\n",
        "        print(f\"    -> Error downloading '{filename}': {e}\")\n",
        "        temp_dir.cleanup()\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "# --- 3. تحميل نموذج ONNX من المسار المحلي ---\n",
        "print(f\"\\nLoading ONNX model from local directory: {local_model_dir}\")\n",
        "# تأكد من أن جميع الملفات الضرورية موجودة\n",
        "required_local_files = [os.path.join(local_model_dir, f) for f in files_to_download if f in downloaded_files_paths]\n",
        "if not all(os.path.exists(p) for p in required_local_files):\n",
        "     print(\"Error: Not all required files seem to be present in the local directory after download attempts.\")\n",
        "     print(\"Expected based on download list:\", files_to_download)\n",
        "     print(\"Found:\", os.listdir(local_model_dir))\n",
        "     temp_dir.cleanup()\n",
        "     sys.exit(1)\n",
        "\n",
        "try:\n",
        "    # الآن قم بالتحميل مباشرة من المجلد المحلي الذي يحتوي على config.json وملف .onnx (وملف .onnx_data إذا لزم الأمر)\n",
        "    model = ORTModelForCausalLM.from_pretrained(local_model_dir)\n",
        "    print(\"ONNX model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ONNX model from {local_model_dir}: {e}\")\n",
        "    print(\"Check if the directory contains the necessary config.json and ONNX model files.\")\n",
        "    # عرض الملفات يمكن أن يساعد في التشخيص:\n",
        "    try:\n",
        "        print(f\"Files found in {local_model_dir}: {os.listdir(local_model_dir)}\")\n",
        "    except OSError as list_err:\n",
        "        print(f\"Could not list directory contents: {list_err}\")\n",
        "    temp_dir.cleanup()\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "# --- 4. تحميل Tokenizer ---\n",
        "print(f\"\\nLoading tokenizer from: {tokenizer_repo_id}\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_repo_id)\n",
        "    print(\"Tokenizer loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")\n",
        "    temp_dir.cleanup()\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- 5. تعيين Pad Token (مهم لـ Pipelines) ---\n",
        "if tokenizer.pad_token is None:\n",
        "    print(\"Setting pad_token to eos_token\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # تأكد من أن إعدادات النموذج تعرف أيضاً رمز الحشو إذا كانت هناك حاجة إليه بواسطة وسائط pipeline/generation\n",
        "    # model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "\n",
        "# --- 6. إنشاء وتشغيل Pipeline ---\n",
        "print(\"\\nCreating text-generation pipeline...\")\n",
        "try:\n",
        "    # Specify device='cuda:0' for GPU if onnxruntime-gpu is installed and CUDA is available\n",
        "    # pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device='cuda:0')\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "    print(\"Pipeline created successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating pipeline: {e}\")\n",
        "    temp_dir.cleanup()\n",
        "    sys.exit(1)\n",
        "\n",
        "prompt = \"He never went out without a book under his arm\"\n",
        "print(f\"\\nGenerating text for prompt: '{prompt}'\")\n",
        "\n",
        "try:\n",
        "    # أضف معلمات التوليد لتحكم أفضل\n",
        "    result = pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=50,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id # Use EOS token for padding\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Generation Result ---\")\n",
        "    # الـ pipeline عادةً ما يعيد قائمة من القواميس\n",
        "    if isinstance(result, list) and len(result) > 0 and 'generated_text' in result[0]:\n",
        "         print(result[0]['generated_text'])\n",
        "    else:\n",
        "         print(result) # اطبع النتيجة الخام إذا كان التنسيق غير متوقع\n",
        "    print(\"--- End of Result ---\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError during text generation: {e}\")\n",
        "finally:\n",
        "    # --- 7. تنظيف المجلد المؤقت ---\n",
        "    print(f\"\\nCleaning up temporary directory: {local_model_dir}\")\n",
        "    temp_dir.cleanup()"
      ],
      "metadata": {
        "id": "N-hrMlZ7i7_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vOcXKxl2lUqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4aQ8YwmulUmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G6ncVBqzlUii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pNOeACvLlUeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال جيد"
      ],
      "metadata": {
        "id": "wrAL4zHrlhb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from optimum.onnxruntime import ORTModelForCausalLM\n",
        "\n",
        "# Optional: Set logging level for more details from transformers/optimum\n",
        "# logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# --- Configuration ---\n",
        "model_repo_id = \"onnx-community/Llama-3.2-1B\"\n",
        "onnx_subfolder = \"onnx\"\n",
        "tokenizer_repo_id = \"meta-llama/Llama-3.2-1B\" # Use original repo for tokenizer\n",
        "\n",
        "# --- !!! Choose the specific ONNX file you want to load !!! ---\n",
        "# Check the 'onnx' subfolder in the repo: https://huggingface.co/onnx-community/Llama-3.2-1B/tree/main/onnx\n",
        "# Options include: \"model.onnx\", \"model_fp16.onnx\", \"model_q4.onnx\", \"model_int8.onnx\", etc.\n",
        "onnx_filename = \"model_fp16.onnx\"  # FP16 is often a good balance of size/speed/quality\n",
        "# onnx_filename = \"model_q4.onnx\"    # Quantized: Smaller, potentially faster, slight quality loss\n",
        "# onnx_filename = \"model.onnx\"       # Original large file (might be FP32)\n",
        "\n",
        "print(f\"Loading ONNX model: {model_repo_id}/{onnx_subfolder}/{onnx_filename}\")\n",
        "\n",
        "# --- 1. Load Model ---\n",
        "try:\n",
        "    # Use repo_id, subfolder, and explicitly specify the file_name\n",
        "    model = ORTModelForCausalLM.from_pretrained(\n",
        "        model_repo_id,\n",
        "        subfolder=onnx_subfolder,\n",
        "        file_name=onnx_filename, # Crucial addition!\n",
        "        # provider=\"CPUExecutionProvider\", # Explicitly set CPU if needed, default is often fine\n",
        "        use_cache=True # Use KV cache for faster generation\n",
        "    )\n",
        "    print(\"ONNX Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ONNX model: {e}\")\n",
        "    print(\"Please ensure:\")\n",
        "    print(f\"  - The repo '{model_repo_id}' and subfolder '{onnx_subfolder}' exist.\")\n",
        "    print(f\"  - The file '{onnx_filename}' exists within that subfolder.\")\n",
        "    print(f\"  - You have internet connectivity and necessary permissions (e.g., HF_TOKEN if needed).\")\n",
        "    exit() # Exit if model loading fails\n",
        "\n",
        "# --- 2. Load Tokenizer ---\n",
        "print(f\"\\nLoading tokenizer from: {tokenizer_repo_id}\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_repo_id)\n",
        "    # Llama models require a pad token, but often don't have one set by default.\n",
        "    # Setting it to the EOS token is a common practice.\n",
        "    if tokenizer.pad_token is None:\n",
        "        print(\"Setting pad_token to eos_token\")\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        # Important: Also update the model's config if padding is needed during generation\n",
        "        # This ensures consistency if the pipeline or generation kwargs rely on it.\n",
        "        if model.config.pad_token_id is None:\n",
        "             model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    print(\"Tokenizer loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")\n",
        "    exit() # Exit if tokenizer loading fails\n",
        "\n",
        "# --- 3. Create Pipeline ---\n",
        "print(\"\\nCreating text-generation pipeline...\")\n",
        "try:\n",
        "    # You can add device=0 for CUDA GPU if onnxruntime-gpu is installed\n",
        "    # pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "    print(\"Pipeline created successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating pipeline: {e}\")\n",
        "    exit() # Exit if pipeline creation fails\n",
        "\n",
        "# --- 4. Generate Text ---\n",
        "prompt = \"He never went out without a book under his arm\"\n",
        "print(f\"\\nGenerating text for prompt: '{prompt}'\")\n",
        "\n",
        "try:\n",
        "    # Add generation parameters for better control and to avoid warnings\n",
        "    result = pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=50,  # Generate up to 50 new tokens\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,     # Use sampling for more creative output\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id # Explicitly pass pad token ID\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Generation Result ---\")\n",
        "    # Pipeline returns a list of dictionaries\n",
        "    if isinstance(result, list) and len(result) > 0 and 'generated_text' in result[0]:\n",
        "         print(result[0]['generated_text'])\n",
        "    else:\n",
        "         print(result) # Print raw result if format is unexpected\n",
        "    print(\"--- End of Result ---\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError during text generation: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468,
          "referenced_widgets": [
            "689dbc21c3564732bd02a5056dc06881",
            "ba42aff937cb4613b5bc2bd3b829ba8f",
            "d5953c96fddd46cd80e32c6e5ea18768",
            "04dbd18c30a14c9583642f3f8ca801dc",
            "260a7e9f668040b4ad94edd3d9a652f1",
            "a78adc5573044275ac492acb59973860",
            "cd4995a02fb74e3da690ccc718296a0e",
            "15d657695bdd42cbb5d45ee5e173f556",
            "b97c39a7054c4384a7b78ec97522f0a1",
            "a91fac84925c407680eee6842b99a390",
            "6ba689ff7f074e67b3aee37ed55e24ba"
          ]
        },
        "id": "hmjL1hYbka05",
        "outputId": "183fe2ad-7b68-4bd1-a9e1-02977cd11d51"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ONNX model: onnx-community/Llama-3.2-1B/onnx/model_fp16.onnx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "689dbc21c3564732bd02a5056dc06881"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX Model loaded successfully.\n",
            "\n",
            "Loading tokenizer from: meta-llama/Llama-3.2-1B\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting pad_token to eos_token\n",
            "Tokenizer loaded successfully.\n",
            "\n",
            "Creating text-generation pipeline...\n",
            "Pipeline created successfully.\n",
            "\n",
            "Generating text for prompt: 'He never went out without a book under his arm'\n",
            "\n",
            "--- Generation Result ---\n",
            "He never went out without a book under his arm, and he always had a favorite author. He had a small library of his own, and he was constantly reading to the children. He was also an excellent gardener and an accomplished musician. He was a man of many talents and accomplishments.\n",
            "A\n",
            "--- End of Result ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T3_MHCcjl2Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CGixKhp2l2O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c93-yGlzl2MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-wSJL5Hfl2JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1rXiNNwYl2Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "# !!! --- قم بتغيير هذه القيم للنموذج الجديد --- !!!\n",
        "model_repo_id = \"optimum/gpt2-onnx\"          # مستودع نموذج ONNX الجديد\n",
        "onnx_subfolder = None                       # افترض أن الملفات في الجذر (تحقق من المستودع)\n",
        "tokenizer_repo_id = \"gpt2\"                  # معرف مستودع Tokenizer المطابق\n",
        "onnx_filename = \"model.onnx\"                # اسم ملف ONNX في المستودع الجديد (تحقق)\n",
        "# !!! --- نهاية التغييرات --- !!!\n",
        "\n",
        "print(f\"Loading ONNX model: {model_repo_id}/{onnx_filename if onnx_subfolder is None else f'{onnx_subfolder}/{onnx_filename}'}\")\n",
        "\n",
        "try:\n",
        "    model = ORTModelForCausalLM.from_pretrained(\n",
        "        model_repo_id,\n",
        "        subfolder=onnx_subfolder, # قد يكون None\n",
        "        file_name=onnx_filename,\n",
        "        use_cache=True\n",
        "    )\n",
        "    print(\"ONNX Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ONNX model: {e}\")\n",
        "    exit()\n",
        "\n",
        "print(f\"\\nLoading tokenizer from: {tokenizer_repo_id}\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_repo_id)\n",
        "    if tokenizer.pad_token is None:\n",
        "        # غالبًا ما يكون لـ GPT-2 نفس الـ pad و eos token\n",
        "        print(\"Setting pad_token to eos_token\")\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        if model.config.pad_token_id is None:\n",
        "             model.config.pad_token_id = tokenizer.eos_token_id\n",
        "    print(\"Tokenizer loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")\n",
        "    exit()\n",
        "\n",
        "# ... باقي الكود (إنشاء Pipeline وتشغيل التوليد) يبقى كما هو ...\n",
        "\n",
        "# --- إنشاء Pipeline ---\n",
        "print(\"\\nCreating text-generation pipeline...\")\n",
        "try:\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "    print(\"Pipeline created successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating pipeline: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- توليد النص ---\n",
        "prompt = \"The future of AI is\" # تغيير الـ prompt قد يكون مناسبًا للنموذج الجديد\n",
        "print(f\"\\nGenerating text for prompt: '{prompt}'\")\n",
        "# ... (باقي كود التوليد) ..."
      ],
      "metadata": {
        "id": "M5Kv8X37l2CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k2FKWLDSmDWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b1eNatpomDTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uu2soRB0mDQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onnx-community/gemma-3-1b-it-ONNX-GQA"
      ],
      "metadata": {
        "id": "OSPnyHNRmDN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from optimum.onnxruntime import ORTModelForCausalLM\n",
        "\n",
        "# Optional: Set logging level for more details from transformers/optimum\n",
        "# logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# --- Configuration ---\n",
        "model_repo_id = \"onnx-community/gemma-3-1b-it-ONNX-GQA\"\n",
        "onnx_subfolder = \"onnx\"\n",
        "tokenizer_repo_id = \"google/gemma-3-1b-it\" # Use original repo for tokenizer\n",
        "\n",
        "# --- !!! Choose the specific ONNX file you want to load !!! ---\n",
        "# Check the 'onnx' subfolder in the repo: https://huggingface.co/onnx-community/Llama-3.2-1B/tree/main/onnx\n",
        "# Options include: \"model.onnx\", \"model_fp16.onnx\", \"model_q4.onnx\", \"model_int8.onnx\", etc.\n",
        "onnx_filename = \"model_fp16.onnx\"  # FP16 is often a good balance of size/speed/quality\n",
        "# onnx_filename = \"model_q4.onnx\"    # Quantized: Smaller, potentially faster, slight quality loss\n",
        "# onnx_filename = \"model.onnx\"       # Original large file (might be FP32)\n",
        "\n",
        "print(f\"Loading ONNX model: {model_repo_id}/{onnx_subfolder}/{onnx_filename}\")\n",
        "\n",
        "# --- 1. Load Model ---\n",
        "try:\n",
        "    # Use repo_id, subfolder, and explicitly specify the file_name\n",
        "    model = ORTModelForCausalLM.from_pretrained(\n",
        "        model_repo_id,\n",
        "        subfolder=onnx_subfolder,\n",
        "        file_name=onnx_filename, # Crucial addition!\n",
        "        # provider=\"CPUExecutionProvider\", # Explicitly set CPU if needed, default is often fine\n",
        "        use_cache=True # Use KV cache for faster generation\n",
        "    )\n",
        "    print(\"ONNX Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ONNX model: {e}\")\n",
        "    print(\"Please ensure:\")\n",
        "    print(f\"  - The repo '{model_repo_id}' and subfolder '{onnx_subfolder}' exist.\")\n",
        "    print(f\"  - The file '{onnx_filename}' exists within that subfolder.\")\n",
        "    print(f\"  - You have internet connectivity and necessary permissions (e.g., HF_TOKEN if needed).\")\n",
        "    exit() # Exit if model loading fails\n",
        "\n",
        "# --- 2. Load Tokenizer ---\n",
        "print(f\"\\nLoading tokenizer from: {tokenizer_repo_id}\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_repo_id)\n",
        "    # Llama models require a pad token, but often don't have one set by default.\n",
        "    # Setting it to the EOS token is a common practice.\n",
        "    if tokenizer.pad_token is None:\n",
        "        print(\"Setting pad_token to eos_token\")\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        # Important: Also update the model's config if padding is needed during generation\n",
        "        # This ensures consistency if the pipeline or generation kwargs rely on it.\n",
        "        if model.config.pad_token_id is None:\n",
        "             model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    print(\"Tokenizer loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")\n",
        "    exit() # Exit if tokenizer loading fails\n",
        "\n",
        "# --- 3. Create Pipeline ---\n",
        "print(\"\\nCreating text-generation pipeline...\")\n",
        "try:\n",
        "    # You can add device=0 for CUDA GPU if onnxruntime-gpu is installed\n",
        "    # pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "    print(\"Pipeline created successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating pipeline: {e}\")\n",
        "    exit() # Exit if pipeline creation fails\n",
        "\n",
        "# --- 4. Generate Text ---\n",
        "prompt = \"He never went out without a book under his arm\"\n",
        "print(f\"\\nGenerating text for prompt: '{prompt}'\")\n",
        "\n",
        "try:\n",
        "    # Add generation parameters for better control and to avoid warnings\n",
        "    result = pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=50,  # Generate up to 50 new tokens\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,     # Use sampling for more creative output\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id # Explicitly pass pad token ID\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Generation Result ---\")\n",
        "    # Pipeline returns a list of dictionaries\n",
        "    if isinstance(result, list) and len(result) > 0 and 'generated_text' in result[0]:\n",
        "         print(result[0]['generated_text'])\n",
        "    else:\n",
        "         print(result) # Print raw result if format is unexpected\n",
        "    print(\"--- End of Result ---\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError during text generation: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675,
          "referenced_widgets": [
            "60599bef54c947eb8297978c0c58f429",
            "eb83ad50b8fc49d287f224c9a355cede",
            "c007ac03a31f4ca0b2a76468ca9b7c59",
            "90b272b646524545a89238d2be831b3d",
            "d02c051d4d0d498783d0b8096e77acdb",
            "b7b125ee96e84d58ad79b790b18a6f1f",
            "e9101dbc88344176bb3d77e0e8dbc753",
            "b83e81a7f7964395a0633c8dd57474b8",
            "cbe09ff982984290be5b5af1efa5a145",
            "eb517821da2f44f183059f8058ad105b",
            "fa1fe9f0ef3e48608ba56c2e2dbe1706",
            "757cc302979446f4bc2f5fa7fd4c1134",
            "397113e39aa24260a460cb27cc0cfb23",
            "8d78f1233d034203b3856761e0932809",
            "7644a022c97d480f83d3da52b949d9ab",
            "25263a656bec4c68bf6b1f700ff5374f",
            "004b581cff9748ba85555c37350bc980",
            "d5ce3557ddcd4a65a7ebfcb11d8696e9",
            "fc3029d329eb4dea91fd60ad2c424b5e",
            "df4a0082d6024e09ae1da788399e0139",
            "dc53c8b9d15e449ba413af90d05ccb52",
            "ec031a7fb6684888acc0deece1733587",
            "17a10f1b39b34f7db5282ac8b8709359",
            "b3f9ccd9ab084f25be66b83014939402",
            "79ba6a19cb984d4ba04b5bc8dda98500",
            "f02369873408444fb97ced5ac4820792",
            "0b187e46c364404a8e1fe59b925c086e",
            "d454285388a6421c93785ecfd03e6f1b",
            "4210d5851018497a994db6fffd506cb5",
            "a7bbaf871cd948baa7b872f682fa8e1c",
            "4519f36e4c3641529e0c55cf8e890c45",
            "4962e188ea9b42cd969282804475594d",
            "47d43626bedd40efae450dea4c10e423",
            "bcfc86da8656417d8c6e6b687eb3bc3e",
            "4d516645f5ca47bcae032fb4fbde620f",
            "6d0086219e324c09b0715738cf3d932c",
            "79f75f6b12354546b3fb85a014731311",
            "0f1eef3e0b624953ae389c6797147a9f",
            "22eb276961cf43ed9a299351fc4cf905",
            "03a41923016147eaa088f483c4fc23fa",
            "28132a6c81ed4049b6dfbcd48b4e172d",
            "17a091231ac745509e310b734ff0dbbd",
            "330ce52d5f0641f5a8ce68c4ee8a73d4",
            "d7575bd26d214c9997ebb491632fa580",
            "1d00a493d5f74c9580f508c4e50fd18b",
            "c4a2f92c540548bfa2fbd069256a8cec",
            "f524add5ee194db7a4fbe15c187eba39",
            "1d67c65bc7cb4b62a094fe3ec8d239cb",
            "399b84709859463593afe071255e4255",
            "39d5521391a04106a26c6aa7e9affd20",
            "b59815ef825c4643a8ea4eba904fdcde",
            "f7143eda3b7442e3b20ace15a8d6f019",
            "34a0938f1d804d1dbf8d2fbbda593bdc",
            "6b286b6c5d70481ba431ff3d149128e0",
            "8043a3e294a942559de4501123fc0df0",
            "4240760d6c5148ce94e123cc863a6c39",
            "a05779ac0d6a4c0e91157358753a3fbd",
            "32b5916c9997428bb993936597030cf3",
            "639e557c0fcb4abcb78a407a491390bb",
            "dd453549b76a4ec6b83b1c47861f8682",
            "e372dea41821439dbbda9fc9f00992c4",
            "eb8067fdc8d0457fa58388a3fe4444c0",
            "4ee96eec005248b6bca0aa3c2798164b",
            "120084202ef34e35831bf9e5d58e0e11",
            "d0b08baa19144c09867644f8588b8d30",
            "a3f10a00d7bf4297a6b0636b81c3dd3d",
            "8f32ea67a70a4ad9a68d809fca7199bb",
            "e10fe5fa37d74d0d940c1d6d0638d561",
            "e3296ecb6da542478be21837667da13c",
            "292483db78a0443298a9c8ca94718713",
            "e737931334b946248b0c296e882b17d6",
            "7ae78af213324359899f9447974621a8",
            "65bda09f37234871a4207ac08bfc8c5c",
            "88cfb9f7a55e4796b0bd6a01a8a3c7f2",
            "49b7684089ca488db89961d84749c977",
            "6a37b9b9066c4a6b97f7b29d1a348a5e",
            "eac7ce09376941c5a0512e625fe945de",
            "398f0fd481904c2691a8101dc8d9af25",
            "f5c83245a77e4f1cb7a09bf274094187",
            "ceaa00ddc2b3470c8fc7756f20a27de3",
            "891d0b0b0bf64592ac8912f665fcb3b8",
            "63f43a8999bc4870857064da2e6c1536",
            "144962dc8e8541c2b00f7f964d50f3a5",
            "dbd7ecd20ba545419b9adedddf9c9dfa",
            "5e04fdb820054a2987282887a656fdf2",
            "f6d8727067044b4299b40c5a779b2dac",
            "b4b18a40c7964522b47e3f435b7eb143",
            "920ea75d03f9483e9578db3c7efac1d1"
          ]
        },
        "id": "BNs-E06OmDKr",
        "outputId": "b8b3cfe7-7376-4286-9bc2-cd1a50f5171d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ONNX model: onnx-community/gemma-3-1b-it-ONNX-GQA/onnx/model_fp16.onnx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60599bef54c947eb8297978c0c58f429"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model_fp16.onnx:   0%|          | 0.00/2.03G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "757cc302979446f4bc2f5fa7fd4c1134"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17a10f1b39b34f7db5282ac8b8709359"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading ONNX model: 'gemma3-text model type is not supported yet in NormalizedConfig. Only albert, bart, bert, big-bird, bigbird-pegasus, blenderbot, blenderbot-small, bloom, falcon, camembert, codegen, cvt, deberta, deberta-v2, deit, distilbert, donut-swin, electra, encoder-decoder, gemma, gpt2, gpt-bigcode, gpt-neo, gpt-neox, gptj, imagegpt, llama, longt5, marian, markuplm, mbart, mistral, mixtral, mpnet, mpt, mt5, m2m-100, nystromformer, opt, pegasus, pix2struct, phi, phi3, phi3small, poolformer, regnet, resnet, roberta, segformer, speech-to-text, splinter, t5, trocr, vision-encoder-decoder, vit, whisper, xlm-roberta, yolos, qwen2, granite are supported. If you want to support gemma3-text please propose a PR or open up an issue.'\n",
            "Please ensure:\n",
            "  - The repo 'onnx-community/gemma-3-1b-it-ONNX-GQA' and subfolder 'onnx' exist.\n",
            "  - The file 'model_fp16.onnx' exists within that subfolder.\n",
            "  - You have internet connectivity and necessary permissions (e.g., HF_TOKEN if needed).\n",
            "\n",
            "Loading tokenizer from: google/gemma-3-1b-it\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bcfc86da8656417d8c6e6b687eb3bc3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d00a493d5f74c9580f508c4e50fd18b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4240760d6c5148ce94e123cc863a6c39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f32ea67a70a4ad9a68d809fca7199bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "398f0fd481904c2691a8101dc8d9af25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded successfully.\n",
            "\n",
            "Creating text-generation pipeline...\n",
            "Error creating pipeline: name 'model' is not defined\n",
            "\n",
            "Generating text for prompt: 'He never went out without a book under his arm'\n",
            "\n",
            "Error during text generation: name 'pipe' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from optimum.onnxruntime import ORTModelForCausalLM\n",
        "\n",
        "# Optional: Set logging level for more details from transformers/optimum\n",
        "# logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# --- Configuration ---\n",
        "model_repo_id = \"onnx-community/gemma-3-1b-it-ONNX-GQA\"\n",
        "onnx_subfolder = \"onnx\"\n",
        "tokenizer_repo_id = \"onnx-community/gemma-3-1b-it-ONNX-GQA\" # Use original repo for tokenizer\n",
        "\n",
        "# --- !!! Choose the specific ONNX file you want to load !!! ---\n",
        "# Check the 'onnx' subfolder in the repo: https://huggingface.co/onnx-community/Llama-3.2-1B/tree/main/onnx\n",
        "# Options include: \"model.onnx\", \"model_fp16.onnx\", \"model_q4.onnx\", \"model_int8.onnx\", etc.\n",
        "onnx_filename = \"model_fp16.onnx\"  # FP16 is often a good balance of size/speed/quality\n",
        "# onnx_filename = \"model_q4.onnx\"    # Quantized: Smaller, potentially faster, slight quality loss\n",
        "# onnx_filename = \"model.onnx\"       # Original large file (might be FP32)\n",
        "\n",
        "print(f\"Loading ONNX model: {model_repo_id}/{onnx_subfolder}/{onnx_filename}\")\n",
        "\n",
        "# --- 1. Load Model ---\n",
        "try:\n",
        "    # Use repo_id, subfolder, and explicitly specify the file_name\n",
        "    model = ORTModelForCausalLM.from_pretrained(\n",
        "        model_repo_id,\n",
        "        subfolder=onnx_subfolder,\n",
        "        file_name=onnx_filename, # Crucial addition!\n",
        "        # provider=\"CPUExecutionProvider\", # Explicitly set CPU if needed, default is often fine\n",
        "        use_cache=True # Use KV cache for faster generation\n",
        "    )\n",
        "    print(\"ONNX Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ONNX model: {e}\")\n",
        "    print(\"Please ensure:\")\n",
        "    print(f\"  - The repo '{model_repo_id}' and subfolder '{onnx_subfolder}' exist.\")\n",
        "    print(f\"  - The file '{onnx_filename}' exists within that subfolder.\")\n",
        "    print(f\"  - You have internet connectivity and necessary permissions (e.g., HF_TOKEN if needed).\")\n",
        "    exit() # Exit if model loading fails\n",
        "\n",
        "# --- 2. Load Tokenizer ---\n",
        "print(f\"\\nLoading tokenizer from: {tokenizer_repo_id}\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_repo_id)\n",
        "    # Llama models require a pad token, but often don't have one set by default.\n",
        "    # Setting it to the EOS token is a common practice.\n",
        "    if tokenizer.pad_token is None:\n",
        "        print(\"Setting pad_token to eos_token\")\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        # Important: Also update the model's config if padding is needed during generation\n",
        "        # This ensures consistency if the pipeline or generation kwargs rely on it.\n",
        "        if model.config.pad_token_id is None:\n",
        "             model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    print(\"Tokenizer loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")\n",
        "    exit() # Exit if tokenizer loading fails\n",
        "\n",
        "# --- 3. Create Pipeline ---\n",
        "print(\"\\nCreating text-generation pipeline...\")\n",
        "try:\n",
        "    # You can add device=0 for CUDA GPU if onnxruntime-gpu is installed\n",
        "    # pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "    print(\"Pipeline created successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating pipeline: {e}\")\n",
        "    exit() # Exit if pipeline creation fails\n",
        "\n",
        "# --- 4. Generate Text ---\n",
        "prompt = \"He never went out without a book under his arm\"\n",
        "print(f\"\\nGenerating text for prompt: '{prompt}'\")\n",
        "\n",
        "try:\n",
        "    # Add generation parameters for better control and to avoid warnings\n",
        "    result = pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=50,  # Generate up to 50 new tokens\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,     # Use sampling for more creative output\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id # Explicitly pass pad token ID\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Generation Result ---\")\n",
        "    # Pipeline returns a list of dictionaries\n",
        "    if isinstance(result, list) and len(result) > 0 and 'generated_text' in result[0]:\n",
        "         print(result[0]['generated_text'])\n",
        "    else:\n",
        "         print(result) # Print raw result if format is unexpected\n",
        "    print(\"--- End of Result ---\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError during text generation: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547,
          "referenced_widgets": [
            "426d0550d4ef4cefba0c4f587921262a",
            "2611fc157db44372b3dd6106a45b0edd",
            "e6b24f7188624828a050a71a1bf2dae8",
            "d685c7e6492c454caf9d4b5ba39bc2fc",
            "225ae15da00c434e8818b1d8e96f986d",
            "c19c91719f234d48bebe81c8fc6ab1ff",
            "7bd2a7cb0c1f45839e72b41270563f4c",
            "72b7a8ab3b2b426ea703cc9425fc1ec2",
            "c75b12f7835342bb9c4db39cc6673007",
            "db5f5459eb9e4b98b6d717eb45ffb1d3",
            "1a2cb1fb7df64d0cac5f71a1ec74bd2d",
            "aebcabb8a5774fcf863ede1591315369",
            "36484482c2d242cf8aafce6d5651d163",
            "4d74cd4bb916435b811f1e319e74d23b",
            "8559fe254dd443e0b5feef6a27c81f42",
            "806cf64166214a95b0d538d3dcd3a12b",
            "e6a4caebfbc74702a811349d8499f8da",
            "bed42072cf9d4f579741edc17fd12bed",
            "5cad70b1193646c0b7808c61853ada5c",
            "898b2b5f0b7344149ae25d33e69304ce",
            "31c6a0efd5684dd58ff8091f3e601fbf",
            "8017434f898d40478c86b3a2cb1681c8",
            "c5a1f9ed5510463e8cb9bcf216116560",
            "e2eece0077ba41cdbd284d1cf8e34bc0",
            "48cc13c58d5c4f42b9af7e3d7e7acae8",
            "7970321c8a8f4782af03014a1626c9f5",
            "9c622547be56411bb4088841bb2966bd",
            "4d13f8786dd9465b923e7e53c1c58400",
            "cb274ebae28e44829627d20a5250cb41",
            "a59af1409ad8431482c9c0448050c932",
            "06b3d023a3594f6eb92d2e1ada819558",
            "6b57a666c67c44fa97bb6220d15d3e8f",
            "59746eae192443f697f4e1b3e9aeb339",
            "14d43c7f42cc4bb48e8cdc06089f2e10",
            "6aaee233605d4d048b617b913037adf3",
            "cbe158540d9b4ce68fa585c01b9aa672",
            "a17b69cdef8f457ca78e327ce5b1d712",
            "b1bc039ce7fb4bfd9b4a98ff269ee26e",
            "9b6249e6f18c4ce4a84dc259d4fb4159",
            "39001556744e4f1ba6537cef0ed625eb",
            "4bb76efded8f4c558b37f266b2806e1d",
            "11805e999c8545c0b7c984503ee1c759",
            "94389a7b751c477aba7f66ef7f0670e8",
            "e8feb97dd400477eb030307381f90350"
          ]
        },
        "id": "3pV1SCIvvZ0r",
        "outputId": "ef4ed90d-312e-4690-d77e-fb78707cf52a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ONNX model: onnx-community/gemma-3-1b-it-ONNX-GQA/onnx/model_fp16.onnx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading ONNX model: 'gemma3-text model type is not supported yet in NormalizedConfig. Only albert, bart, bert, big-bird, bigbird-pegasus, blenderbot, blenderbot-small, bloom, falcon, camembert, codegen, cvt, deberta, deberta-v2, deit, distilbert, donut-swin, electra, encoder-decoder, gemma, gpt2, gpt-bigcode, gpt-neo, gpt-neox, gptj, imagegpt, llama, longt5, marian, markuplm, mbart, mistral, mixtral, mpnet, mpt, mt5, m2m-100, nystromformer, opt, pegasus, pix2struct, phi, phi3, phi3small, poolformer, regnet, resnet, roberta, segformer, speech-to-text, splinter, t5, trocr, vision-encoder-decoder, vit, whisper, xlm-roberta, yolos, qwen2, granite are supported. If you want to support gemma3-text please propose a PR or open up an issue.'\n",
            "Please ensure:\n",
            "  - The repo 'onnx-community/gemma-3-1b-it-ONNX-GQA' and subfolder 'onnx' exist.\n",
            "  - The file 'model_fp16.onnx' exists within that subfolder.\n",
            "  - You have internet connectivity and necessary permissions (e.g., HF_TOKEN if needed).\n",
            "\n",
            "Loading tokenizer from: onnx-community/gemma-3-1b-it-ONNX-GQA\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "426d0550d4ef4cefba0c4f587921262a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/20.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aebcabb8a5774fcf863ede1591315369"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5a1f9ed5510463e8cb9bcf216116560"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14d43c7f42cc4bb48e8cdc06089f2e10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded successfully.\n",
            "\n",
            "Creating text-generation pipeline...\n",
            "Error creating pipeline: name 'model' is not defined\n",
            "\n",
            "Generating text for prompt: 'He never went out without a book under his arm'\n",
            "\n",
            "Error during text generation: name 'pipe' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UxoZqnrGvZyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TlZx1Ca6vZuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BG4CPfocvZq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZYwQe03kvZnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import { pipeline } from \"@huggingface/transformers\";\n",
        "\n",
        "// Create a text generation pipeline\n",
        "const generator = await pipeline(\n",
        "  \"text-generation\",\n",
        "  \"onnx-community/gemma-3-1b-it-ONNX-GQA\",\n",
        "  { dtype: \"q4\" },\n",
        ");\n",
        "\n",
        "// Define the list of messages\n",
        "const messages = [\n",
        "  { role: \"system\", content: \"You are a helpful assistant.\" },\n",
        "  { role: \"user\", content: \"Write me a poem about Machine Learning.\" },\n",
        "];\n",
        "\n",
        "// Generate a response\n",
        "const output = await generator(messages, { max_new_tokens: 512, do_sample: false });\n",
        "console.log(output[0].generated_text.at(-1).content);\n",
        "\n"
      ],
      "metadata": {
        "id": "3A7I90CJoSf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IzhYqHlNoxnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
      ],
      "metadata": {
        "id": "8HDLl0VdqApS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال جيد"
      ],
      "metadata": {
        "id": "-msxXcf9pwBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile my_script.js\n",
        "\n",
        "\n",
        "\n",
        "import { pipeline } from \"@huggingface/transformers\";\n",
        "\n",
        "// Create a text generation pipeline\n",
        "const generator = await pipeline(\n",
        "  \"text-generation\",\n",
        "  \"onnx-community/gemma-3-1b-it-ONNX-GQA\",\n",
        "  { dtype: \"q4\" },\n",
        ");\n",
        "\n",
        "// Define the list of messages\n",
        "const messages = [\n",
        "  { role: \"system\", content: \"You are a helpful assistant.\" },\n",
        "  { role: \"user\", content: \"Write me a poem about Machine Learning.\" },\n",
        "];\n",
        "\n",
        "// Generate a response\n",
        "const output = await generator(messages, { max_new_tokens: 512, do_sample: false });\n",
        "console.log(output[0].generated_text.at(-1).content);\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1FwVhh9os1i",
        "outputId": "5fe44e28-1222-4036-99e9-804d7aed4d25"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing my_script.js\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!node my_script.js"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8hX_EjFo1-S",
        "outputId": "c53afb88-1fd4-4fb8-dc36-d7f9648700b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(node:17773) [MODULE_TYPELESS_PACKAGE_JSON] Warning: Module type of file:///content/my_script.js is not specified and it doesn't parse as CommonJS.\n",
            "Reparsing as ES module because module syntax was detected. This incurs a performance overhead.\n",
            "To eliminate this warning, add \"type\": \"module\" to /content/package.json.\n",
            "(Use `node --trace-warnings ...` to show where the warning was created)\n",
            "Okay, here's a poem about Machine Learning, aiming to capture its essence and potential:\n",
            "\n",
            "**The Learning Algorithm**\n",
            "\n",
            "A silent hum, a coded grace,\n",
            "A network grows, a steady pace.\n",
            "Machine Learning, swift and bright,\n",
            "Unraveling data, day and night.\n",
            "\n",
            "It learns from patterns, subtle, deep,\n",
            "Where errors hide, secrets sleep.\n",
            "With data flowing, vast and wide,\n",
            "It builds a model, side by side.\n",
            "\n",
            "Predicting trends, forecasting near,\n",
            "Recognizing faces, calming fear.\n",
            "Classifying images, text so true,\n",
            "Discovering insights, shining new.\n",
            "\n",
            "From spam filters, swift and keen,\n",
            "To recommending what you’ve seen,\n",
            "It learns your habits, day by day,\n",
            "Improving swiftly, come what may.\n",
            "\n",
            "But caution whispers, a gentle plea,\n",
            "“Control the bias, let it be free.”\n",
            "For ethics guide, a crucial art,\n",
            "To use this power, play a vital part.\n",
            "\n",
            "So let the learning algorithm flow,\n",
            "Expanding knowledge, watch it grow.\n",
            "A powerful tool, a future bright,\n",
            "Machine Learning, shining light.\n",
            "\n",
            "---\n",
            "\n",
            "Would you like me to tweak this poem in any way? For example, would you like me to:\n",
            "\n",
            "*   Focus on a specific application (e.g., image recognition)?\n",
            "*   Adjust the tone (e.g., more optimistic, more cautionary)?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
      ],
      "metadata": {
        "id": "Uw2_4c2LqCSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "k8Z9DKF9r1Yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile my_script2.js\n",
        "\n",
        "\n",
        "\n",
        "import { pipeline } from \"@huggingface/transformers\";\n",
        "\n",
        "// Create a text generation pipeline\n",
        "const generator = await pipeline(\n",
        "  \"text-generation\",\n",
        "  \"onnx-community/gemma-3-1b-it-ONNX-GQA\",\n",
        "  { dtype: \"q4\" },\n",
        ");\n",
        "\n",
        "// Define the list of messages\n",
        "const messages = [\n",
        "  { role: \"system\", content: \"You are a helpful assistant.\" },\n",
        "  { role: \"user\", content: \"Who is Napoleon Bonaparte?\" },\n",
        "];\n",
        "\n",
        "// Generate a response\n",
        "const output = await generator(messages, { max_new_tokens: 512, do_sample: false });\n",
        "console.log(output[0].generated_text.at(-1).content);\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D86P_75o4YR",
        "outputId": "c4871be4-d3bd-41e1-80b1-4828a017c8dc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing my_script2.js\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!node my_script2.js"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKwmhq_CqgNK",
        "outputId": "a5728bab-77fb-4aa0-ec4d-e407cafb1b29"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(node:19548) [MODULE_TYPELESS_PACKAGE_JSON] Warning: Module type of file:///content/my_script2.js is not specified and it doesn't parse as CommonJS.\n",
            "Reparsing as ES module because module syntax was detected. This incurs a performance overhead.\n",
            "To eliminate this warning, add \"type\": \"module\" to /content/package.json.\n",
            "(Use `node --trace-warnings ...` to show where the warning was created)\n",
            "Okay, let's break down who Napoleon Bonaparte was – he's a hugely significant and complex figure in European history. Here's a comprehensive overview:\n",
            "\n",
            "**Who Was Napoleon Bonaparte?**\n",
            "\n",
            "Napoleon Charles Bonaparte (1768-1821) was a French military and political leader who rose to prominence during the French Revolution and ultimately dominated Europe for over a decade. He's renowned for his military genius, ambitious reforms, and the reshaping of the continent.\n",
            "\n",
            "**Key Facts & Accomplishments:**\n",
            "\n",
            "* **Early Life & Rise to Power:** Born into a relatively wealthy family, Napoleon was a brilliant and ambitious child. He was a student of military strategy and quickly distinguished himself in the French army. He achieved significant success in campaigns in Italy and Egypt, earning him a reputation as a brilliant tactician.\n",
            "* **Italian Campaign (1796-1797):**  This was a crucial turning point. Napoleon led a successful invasion and conquest of Italy, demonstrating his strategic brilliance and establishing French control over the Italian peninsula.\n",
            "* **The French Revolution & Rise to Power (1796-1799):**  Following the French Revolution, Napoleon seized power in a coup d'état, overthrowing the Directory. He established the Consulate in 1799, effectively becoming First Consul and then Emperor of France.\n",
            "* **The Napoleonic Wars (1799-1815):** This is arguably his most famous period. He led a series of massive, sweeping wars against a coalition of European powers – primarily Austria, Prussia, Great Britain, and Russia – aiming to create a vast, unified French empire.\n",
            "    * **Italian Campaign (1800-1805):**  A decisive victory that solidified French dominance in Italy.\n",
            "    * **Egyptian Campaign (1798-1801):**  A strategic move to disrupt British trade routes to India.\n",
            "    * **War of the Second Coalition (1803-1806):**  A major victory against Austria, crippling the coalition.\n",
            "    * **Battle of Austerlitz (1805):**  A stunning victory over Austria, considered one of his greatest military achievements.\n",
            "    * **Dissolution of the Holy Roman Empire (1806):**  A significant strategic move that weakened Austria.\n",
            "* **Restoration & Consolidation (1810-1812):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JnHZIeSRqkrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pQaM-Ix3r-ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S1AvAa0Fr-Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3uzSMZEBr-UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from optimum.onnxruntime import ORTModelForCausalLM\n",
        "\n",
        "# Optional: Set logging level for more details from transformers/optimum\n",
        "# logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# --- Configuration ---\n",
        "model_repo_id = \"onnx-community/gemma-3-1b-it-ONNX\"\n",
        "onnx_subfolder = \"onnx\"\n",
        "tokenizer_repo_id = \"onnx-community/gemma-3-1b-it-ONNX\" # Use original repo for tokenizer\n",
        "\n",
        "# --- !!! Choose the specific ONNX file you want to load !!! ---\n",
        "# Check the 'onnx' subfolder in the repo: https://huggingface.co/onnx-community/Llama-3.2-1B/tree/main/onnx\n",
        "# Options include: \"model.onnx\", \"model_fp16.onnx\", \"model_q4.onnx\", \"model_int8.onnx\", etc.\n",
        "onnx_filename = \"model_fp16.onnx\"  # FP16 is often a good balance of size/speed/quality\n",
        "# onnx_filename = \"model_q4.onnx\"    # Quantized: Smaller, potentially faster, slight quality loss\n",
        "# onnx_filename = \"model.onnx\"       # Original large file (might be FP32)\n",
        "\n",
        "print(f\"Loading ONNX model: {model_repo_id}/{onnx_subfolder}/{onnx_filename}\")\n",
        "\n",
        "# --- 1. Load Model ---\n",
        "try:\n",
        "    # Use repo_id, subfolder, and explicitly specify the file_name\n",
        "    model = ORTModelForCausalLM.from_pretrained(\n",
        "        model_repo_id,\n",
        "        subfolder=onnx_subfolder,\n",
        "        file_name=onnx_filename, # Crucial addition!\n",
        "        # provider=\"CPUExecutionProvider\", # Explicitly set CPU if needed, default is often fine\n",
        "        use_cache=True # Use KV cache for faster generation\n",
        "    )\n",
        "    print(\"ONNX Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ONNX model: {e}\")\n",
        "    print(\"Please ensure:\")\n",
        "    print(f\"  - The repo '{model_repo_id}' and subfolder '{onnx_subfolder}' exist.\")\n",
        "    print(f\"  - The file '{onnx_filename}' exists within that subfolder.\")\n",
        "    print(f\"  - You have internet connectivity and necessary permissions (e.g., HF_TOKEN if needed).\")\n",
        "    exit() # Exit if model loading fails\n",
        "\n",
        "# --- 2. Load Tokenizer ---\n",
        "print(f\"\\nLoading tokenizer from: {tokenizer_repo_id}\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_repo_id)\n",
        "    # Llama models require a pad token, but often don't have one set by default.\n",
        "    # Setting it to the EOS token is a common practice.\n",
        "    if tokenizer.pad_token is None:\n",
        "        print(\"Setting pad_token to eos_token\")\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        # Important: Also update the model's config if padding is needed during generation\n",
        "        # This ensures consistency if the pipeline or generation kwargs rely on it.\n",
        "        if model.config.pad_token_id is None:\n",
        "             model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    print(\"Tokenizer loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")\n",
        "    exit() # Exit if tokenizer loading fails\n",
        "\n",
        "# --- 3. Create Pipeline ---\n",
        "print(\"\\nCreating text-generation pipeline...\")\n",
        "try:\n",
        "    # You can add device=0 for CUDA GPU if onnxruntime-gpu is installed\n",
        "    # pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "    print(\"Pipeline created successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating pipeline: {e}\")\n",
        "    exit() # Exit if pipeline creation fails\n",
        "\n",
        "# --- 4. Generate Text ---\n",
        "prompt = \"He never went out without a book under his arm\"\n",
        "print(f\"\\nGenerating text for prompt: '{prompt}'\")\n",
        "\n",
        "try:\n",
        "    # Add generation parameters for better control and to avoid warnings\n",
        "    result = pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=50,  # Generate up to 50 new tokens\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,     # Use sampling for more creative output\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id # Explicitly pass pad token ID\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Generation Result ---\")\n",
        "    # Pipeline returns a list of dictionaries\n",
        "    if isinstance(result, list) and len(result) > 0 and 'generated_text' in result[0]:\n",
        "         print(result[0]['generated_text'])\n",
        "    else:\n",
        "         print(result) # Print raw result if format is unexpected\n",
        "    print(\"--- End of Result ---\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError during text generation: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579,
          "referenced_widgets": [
            "bd85739b72104387acebda10dc5ef147",
            "432441fa69ec409695129140ad6a6fd3",
            "472e236d20f340e1aed0f5108ed51b86",
            "c53d93812de84fcb9323a47bd608c537",
            "7eda64b7f6e444df9ba3bf2b6e6b3aa5",
            "9401616e27c04995923a89e56edf5f05",
            "14642adc08f344e28311e7da24e24d43",
            "6f1225c383bd427bbbcb86a4b64e0cad",
            "9a86866e418349eaa9f4030b2b0d06ae",
            "82e7348ea2bb4da4ba61c47f0e3598e2",
            "cc83e447577a407a83fbb57749559324",
            "3701e98a51b84ada886f764442d94039",
            "a3c183ed36cf452e8b6fb7393b60a3d0",
            "a8433058c44c47f89485041cd7225840",
            "cb93c6df7b6e48c78b50e506a4d51f81",
            "f61b5500f8c5488a86a31a15ba97c507",
            "4a04687e8e38490c9242d1d136eca3c5",
            "7b7507e1572d47509a24103bbad30a32",
            "d551e059561b4557b5e5bca0079a702c",
            "c45664a1a76e43e0a02384658969c55c",
            "762b74035a114f74a9727e2985a4edfc",
            "9966ba76d1194433ad794de73aaf8dba",
            "fd4676f02b944fa1bf90c6a519b02981",
            "285af0b01e484876bad2feef31a7036b",
            "122aa29443bb4119befdd7f5ce6d6094",
            "09f7a38a0b1d44b6b3e5b3f22f69f320",
            "fdd65786b88443698dfd3bf4d3e21ad7",
            "660491a86d83431ab190b9d0f816c24c",
            "9fa47ce003b549c7b45128143503d6f9",
            "54b54e42530a43b58d1939f5e0846644",
            "8bb348fceff243e790143fbcc478e989",
            "62f5883fa3d54fc78333c89e1acd1cc6",
            "4b1f09497af44ccda5987f07c9936441",
            "831c6114f2e44b21839727c1f3db948f",
            "3698570a02384461b78bbab823240079",
            "ae4c6ad5de88418f8cdf8fb85c832f7e",
            "66ffaba76b214eb8b8a6f0c5594f3bc5",
            "fd18923374014eedbf4d6e6a2abbb7e5",
            "6ceb10fc3be046b9a25a04763eb99ce3",
            "8e210832ab364791a9e9e5d9a44f6ad1",
            "e6b5cc5060a64894990893d233262f50",
            "776620af332841d0b375665321263cda",
            "2ba9cfe526654408b54556319d1fe5ca",
            "b5ec8f237b064bd68c41e966cf81aac2",
            "3d1cf094fb1f491f9d5b8964025b92f9",
            "a8f227de71b148a89c5f1bcb836c98e8",
            "000aac3024554512ada3227c0645f6ef",
            "0fd88bda8f384500bb686a2d7d48df7b",
            "35c1144a983843ca880943c7c427539f",
            "5fd832f321484bd4aaf13e6d73b4dd27",
            "1479a0a72ba74baf915a5851d459d375",
            "b3520466a8184cd6aae7d26f1e45ae84",
            "fae23596dbdb48529a545a216eaa63fb",
            "6c2b18fe89334e1da920eb2d0a20bf37",
            "c48a2fa2e8b04f1599562fcc2c984572"
          ]
        },
        "id": "PKCwIAjIr-Q7",
        "outputId": "443c862b-e13b-4cc7-a757-f01c5810d5b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ONNX model: onnx-community/gemma-3-1b-it-ONNX/onnx/model_fp16.onnx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd85739b72104387acebda10dc5ef147"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model_fp16.onnx:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3701e98a51b84ada886f764442d94039"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd4676f02b944fa1bf90c6a519b02981"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading ONNX model: 'gemma3-text model type is not supported yet in NormalizedConfig. Only albert, bart, bert, big-bird, bigbird-pegasus, blenderbot, blenderbot-small, bloom, falcon, camembert, codegen, cvt, deberta, deberta-v2, deit, distilbert, donut-swin, electra, encoder-decoder, gemma, gpt2, gpt-bigcode, gpt-neo, gpt-neox, gptj, imagegpt, llama, longt5, marian, markuplm, mbart, mistral, mixtral, mpnet, mpt, mt5, m2m-100, nystromformer, opt, pegasus, pix2struct, phi, phi3, phi3small, poolformer, regnet, resnet, roberta, segformer, speech-to-text, splinter, t5, trocr, vision-encoder-decoder, vit, whisper, xlm-roberta, yolos, qwen2, granite are supported. If you want to support gemma3-text please propose a PR or open up an issue.'\n",
            "Please ensure:\n",
            "  - The repo 'onnx-community/gemma-3-1b-it-ONNX' and subfolder 'onnx' exist.\n",
            "  - The file 'model_fp16.onnx' exists within that subfolder.\n",
            "  - You have internet connectivity and necessary permissions (e.g., HF_TOKEN if needed).\n",
            "\n",
            "Loading tokenizer from: onnx-community/gemma-3-1b-it-ONNX\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "831c6114f2e44b21839727c1f3db948f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d1cf094fb1f491f9d5b8964025b92f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded successfully.\n",
            "\n",
            "Creating text-generation pipeline...\n",
            "Error creating pipeline: name 'model' is not defined\n",
            "\n",
            "Generating text for prompt: 'He never went out without a book under his arm'\n",
            "\n",
            "Error during text generation: name 'pipe' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "adlN5BwlvMQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GukCG2Ruw6ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j1WM3wDOw6gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YqP5ZqItw6kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0oBns9-Nw6oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX/snapshots/7769a8aff8978adbdcd57ad7915a97ba389fb547/onnx/model_fp16.onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "u6OcJjsSw6qj",
        "outputId": "8a0dc8d7-c6ff-4d95-9ea1-bce90799bb4a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid decimal literal (<ipython-input-2-d15d93b11736>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-d15d93b11736>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX/snapshots/7769a8aff8978adbdcd57ad7915a97ba389fb547/onnx/model_fp16.onnx()\u001b[0m\n\u001b[0m                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال جيد"
      ],
      "metadata": {
        "id": "Z0ehTsD8yfBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load config, processor, and model\n",
        "path_to_model = \"/root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX/snapshots/7769a8aff8978adbdcd57ad7915a97ba389fb547\"\n",
        "config = AutoConfig.from_pretrained(path_to_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
        "decoder_session = onnxruntime.InferenceSession(f\"{path_to_model}/onnx/model_fp16.onnx\")\n",
        "\n",
        "## Set config values\n",
        "num_key_value_heads = config.num_key_value_heads\n",
        "head_dim = config.head_dim\n",
        "num_hidden_layers = config.num_hidden_layers\n",
        "eos_token_id = 106 # 106 is for <end_of_turn>\n",
        "\n",
        "# 2. Prepare inputs\n",
        "## Create input messages\n",
        "messages = [\n",
        "  { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
        "  { \"role\": \"user\", \"content\": \"Write me a poem about Machine Learning.\" },\n",
        "]\n",
        "\n",
        "## Apply tokenizer\n",
        "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"np\")\n",
        "\n",
        "## Prepare decoder inputs\n",
        "batch_size = inputs['input_ids'].shape[0]\n",
        "past_key_values = {\n",
        "    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\n",
        "    for layer in range(num_hidden_layers)\n",
        "    for kv in ('key', 'value')\n",
        "}\n",
        "input_ids = inputs['input_ids']\n",
        "position_ids = np.tile(np.arange(1, input_ids.shape[-1] + 1), (batch_size, 1))\n",
        "\n",
        "# 3. Generation loop\n",
        "max_new_tokens = 1024\n",
        "generated_tokens = np.array([[]], dtype=np.int64)\n",
        "for i in range(max_new_tokens):\n",
        "  logits, *present_key_values = decoder_session.run(None, dict(\n",
        "      input_ids=input_ids,\n",
        "      position_ids=position_ids,\n",
        "      **past_key_values,\n",
        "  ))\n",
        "\n",
        "  ## Update values for next generation loop\n",
        "  input_ids = logits[:, -1].argmax(-1, keepdims=True)\n",
        "  position_ids = position_ids[:, -1:] + 1\n",
        "  for j, key in enumerate(past_key_values):\n",
        "    past_key_values[key] = present_key_values[j]\n",
        "\n",
        "  generated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\n",
        "  if (input_ids == eos_token_id).all():\n",
        "    break\n",
        "\n",
        "  ## (Optional) Streaming\n",
        "  print(tokenizer.decode(input_ids[0]), end='', flush=True)\n",
        "print()\n",
        "\n",
        "# 4. Output result\n",
        "print(tokenizer.batch_decode(generated_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBz1d0mOw6tj",
        "outputId": "33f82617-73cd-49da-e5f4-446354b87768"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here’s a poem about Machine Learning, aiming for a balance of technical and evocative language:\n",
            "\n",
            "**The Silent Learner**\n",
            "\n",
            "The data streams, a boundless flow,\n",
            "A river vast, where patterns grow.\n",
            "No human hand to guide the way,\n",
            "Just algorithms, come what may.\n",
            "\n",
            "Machine Learning, a subtle art,\n",
            "To teach a system, a brand new start.\n",
            "With weights and biases, finely tuned,\n",
            "It seeks the truth, beneath the moon.\n",
            "\n",
            "It learns from errors, big and small,\n",
            "Adjusting swiftly, standing tall.\n",
            "From pixels bright to voices clear,\n",
            "It builds a model, banishing fear.\n",
            "\n",
            "Of blind prediction, cold and stark,\n",
            "It finds the meaning, leaves its mark.\n",
            "A neural net, a complex grace,\n",
            "Discovering insights, time and space.\n",
            "\n",
            "It analyzes, it predicts, it sees,\n",
            "A silent learner, if you please.\n",
            "Expanding knowledge, day by day,\n",
            "A digital mind, in a coded way. \n",
            "\n",
            "---\n",
            "\n",
            "Would you like me to:\n",
            "\n",
            "*   Adjust the tone or style? (e.g., more technical, more whimsical)\n",
            "*   Focus on a specific aspect of ML (e.g., neural networks, data analysis)?\n",
            "*   Create a different length or structure?\n",
            "['Okay, here’s a poem about Machine Learning, aiming for a balance of technical and evocative language:\\n\\n**The Silent Learner**\\n\\nThe data streams, a boundless flow,\\nA river vast, where patterns grow.\\nNo human hand to guide the way,\\nJust algorithms, come what may.\\n\\nMachine Learning, a subtle art,\\nTo teach a system, a brand new start.\\nWith weights and biases, finely tuned,\\nIt seeks the truth, beneath the moon.\\n\\nIt learns from errors, big and small,\\nAdjusting swiftly, standing tall.\\nFrom pixels bright to voices clear,\\nIt builds a model, banishing fear.\\n\\nOf blind prediction, cold and stark,\\nIt finds the meaning, leaves its mark.\\nA neural net, a complex grace,\\nDiscovering insights, time and space.\\n\\nIt analyzes, it predicts, it sees,\\nA silent learner, if you please.\\nExpanding knowledge, day by day,\\nA digital mind, in a coded way. \\n\\n---\\n\\nWould you like me to:\\n\\n*   Adjust the tone or style? (e.g., more technical, more whimsical)\\n*   Focus on a specific aspect of ML (e.g., neural networks, data analysis)?\\n*   Create a different length or structure?<end_of_turn>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2sMNX6lN0QeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KPx10RxM0Qbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tH3AQnTh0QYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load config, processor, and model\n",
        "path_to_model = \"/root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX-GQA/snapshots/e05326d9bfe91af70fcef3a82154a8047d851926\"\n",
        "config = AutoConfig.from_pretrained(path_to_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
        "decoder_session = onnxruntime.InferenceSession(f\"{path_to_model}/onnx/model_int8.onnx\")\n",
        "\n",
        "## Set config values\n",
        "num_key_value_heads = config.num_key_value_heads\n",
        "head_dim = config.head_dim\n",
        "num_hidden_layers = config.num_hidden_layers\n",
        "eos_token_id = 106 # 106 is for <end_of_turn>\n",
        "\n",
        "# 2. Prepare inputs\n",
        "## Create input messages\n",
        "messages = [\n",
        "  { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
        "  { \"role\": \"user\", \"content\": \"Write me a poem about Machine Learning.\" },\n",
        "]\n",
        "\n",
        "## Apply tokenizer\n",
        "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"np\")\n",
        "\n",
        "## Prepare decoder inputs\n",
        "batch_size = inputs['input_ids'].shape[0]\n",
        "past_key_values = {\n",
        "    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\n",
        "    for layer in range(num_hidden_layers)\n",
        "    for kv in ('key', 'value')\n",
        "}\n",
        "input_ids = inputs['input_ids']\n",
        "position_ids = np.tile(np.arange(1, input_ids.shape[-1] + 1), (batch_size, 1))\n",
        "\n",
        "# 3. Generation loop\n",
        "max_new_tokens = 1024\n",
        "generated_tokens = np.array([[]], dtype=np.int64)\n",
        "for i in range(max_new_tokens):\n",
        "  logits, *present_key_values = decoder_session.run(None, dict(\n",
        "      input_ids=input_ids,\n",
        "      position_ids=position_ids,\n",
        "      **past_key_values,\n",
        "  ))\n",
        "\n",
        "  ## Update values for next generation loop\n",
        "  input_ids = logits[:, -1].argmax(-1, keepdims=True)\n",
        "  position_ids = position_ids[:, -1:] + 1\n",
        "  for j, key in enumerate(past_key_values):\n",
        "    past_key_values[key] = present_key_values[j]\n",
        "\n",
        "  generated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\n",
        "  if (input_ids == eos_token_id).all():\n",
        "    break\n",
        "\n",
        "  ## (Optional) Streaming\n",
        "  print(tokenizer.decode(input_ids[0]), end='', flush=True)\n",
        "print()\n",
        "\n",
        "# 4. Output result\n",
        "print(tokenizer.batch_decode(generated_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "S-bn5jFK0QQO",
        "outputId": "f1b04cfd-39dc-430c-8c29-a5266a7a5aab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NoSuchFile",
          "evalue": "[ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX-GQA/snapshots/e05326d9bfe91af70fcef3a82154a8047d851926/onnx/model_int8.onnx failed:Load model /root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX-GQA/snapshots/e05326d9bfe91af70fcef3a82154a8047d851926/onnx/model_int8.onnx failed. File doesn't exist",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNoSuchFile\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-bbd5292658d8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdecoder_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnxruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path_to_model}/onnx/model_int8.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m## Set config values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_inference_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproviders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprovider_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisabled_optimizers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36m_create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_config_from_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_config_from_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNoSuchFile\u001b[0m: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from /root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX-GQA/snapshots/e05326d9bfe91af70fcef3a82154a8047d851926/onnx/model_int8.onnx failed:Load model /root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX-GQA/snapshots/e05326d9bfe91af70fcef3a82154a8047d851926/onnx/model_int8.onnx failed. File doesn't exist"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WqjSY8nl1I6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load config, processor, and model\n",
        "path_to_model = \"/root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX-GQA/snapshots/e05326d9bfe91af70fcef3a82154a8047d851926\"\n",
        "config = AutoConfig.from_pretrained(path_to_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
        "decoder_session = onnxruntime.InferenceSession(f\"{path_to_model}/onnx/model_fp16.onnx\")\n",
        "\n",
        "## Set config values\n",
        "num_key_value_heads = config.num_key_value_heads\n",
        "head_dim = config.head_dim\n",
        "num_hidden_layers = config.num_hidden_layers\n",
        "eos_token_id = 106 # 106 is for <end_of_turn>\n",
        "\n",
        "# 2. Prepare inputs\n",
        "## Create input messages\n",
        "messages = [\n",
        "  { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
        "  { \"role\": \"user\", \"content\": \"Write me a poem about Machine Learning.\" },\n",
        "]\n",
        "\n",
        "## Apply tokenizer\n",
        "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"np\")\n",
        "\n",
        "## Prepare decoder inputs\n",
        "batch_size = inputs['input_ids'].shape[0]\n",
        "past_key_values = {\n",
        "    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\n",
        "    for layer in range(num_hidden_layers)\n",
        "    for kv in ('key', 'value')\n",
        "}\n",
        "input_ids = inputs['input_ids']\n",
        "# Creating attention mask: 1s for actual tokens, 0s for padding\n",
        "attention_mask = np.ones_like(input_ids)\n",
        "position_ids = np.tile(np.arange(1, input_ids.shape[-1] + 1), (batch_size, 1))\n",
        "\n",
        "# 3. Generation loop\n",
        "max_new_tokens = 1024\n",
        "generated_tokens = np.array([[]], dtype=np.int64)\n",
        "for i in range(max_new_tokens):\n",
        "  # Adding attention_mask to the input feed\n",
        "  logits, *present_key_values = decoder_session.run(None, dict(\n",
        "      input_ids=input_ids,\n",
        "      position_ids=position_ids,\n",
        "      attention_mask=attention_mask, # Added attention_mask here\n",
        "      **past_key_values,\n",
        "  ))\n",
        "\n",
        "  ## Update values for next generation loop\n",
        "  input_ids = logits[:, -1].argmax(-1, keepdims=True)\n",
        "  # Update attention mask for the new token\n",
        "  attention_mask = np.concatenate([attention_mask, np.ones_like(input_ids)], axis=-1)\n",
        "  position_ids = position_ids[:, -1:] + 1\n",
        "  for j, key in enumerate(past_key_values):\n",
        "    past_key_values[key] = present_key_values[j]\n",
        "\n",
        "  generated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\n",
        "  if (input_ids == eos_token_id).all():\n",
        "    break\n",
        "\n",
        "  ## (Optional) Streaming\n",
        "  print(tokenizer.decode(input_ids[0]), end='', flush=True)\n",
        "print()\n",
        "\n",
        "# 4. Output result\n",
        "print(tokenizer.batch_decode(generated_tokens))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "obRQcF6X1KSX",
        "outputId": "40b21e20-9742-4907-cc7e-f24b97730037"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgument",
          "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(float)) , expected: (tensor(float16))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-496278a86c25>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;31m# Adding attention_mask to the input feed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m   logits, *present_key_values = decoder_session.run(None, dict(\n\u001b[0m\u001b[1;32m     45\u001b[0m       \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(float)) , expected: (tensor(float16))"
          ]
        }
      ]
    },
    {
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load config, processor, and model\n",
        "path_to_model = \"/root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX-GQA/snapshots/e05326d9bfe91af70fcef3a82154a8047d851926\"\n",
        "config = AutoConfig.from_pretrained(path_to_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
        "decoder_session = onnxruntime.InferenceSession(f\"{path_to_model}/onnx/model_fp16.onnx\")\n",
        "\n",
        "## Set config values\n",
        "num_key_value_heads = config.num_key_value_heads\n",
        "head_dim = config.head_dim\n",
        "num_hidden_layers = config.num_hidden_layers\n",
        "eos_token_id = 106 # 106 is for <end_of_turn>\n",
        "\n",
        "# 2. Prepare inputs\n",
        "## Create input messages\n",
        "messages = [\n",
        "  { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
        "  { \"role\": \"user\", \"content\": \"Write me a poem about Machine Learning.\" },\n",
        "]\n",
        "\n",
        "## Apply tokenizer\n",
        "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"np\")\n",
        "\n",
        "## Prepare decoder inputs\n",
        "batch_size = inputs['input_ids'].shape[0]\n",
        "past_key_values = {\n",
        "    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\n",
        "    for layer in range(num_hidden_layers)\n",
        "    for kv in ('key', 'value')\n",
        "}\n",
        "input_ids = inputs['input_ids']\n",
        "# Creating attention mask: 1s for actual tokens, 0s for padding\n",
        "attention_mask = np.ones_like(input_ids)\n",
        "position_ids = np.tile(np.arange(1, input_ids.shape[-1] + 1), (batch_size, 1))\n",
        "\n",
        "# 3. Generation loop\n",
        "max_new_tokens = 1024\n",
        "generated_tokens = np.array([[]], dtype=np.int64)\n",
        "for i in range(max_new_tokens):\n",
        "  # Adding attention_mask to the input feed\n",
        "  logits, *present_key_values = decoder_session.run(None, dict(\n",
        "      input_ids=input_ids,\n",
        "      position_ids=position_ids,\n",
        "      attention_mask=attention_mask, # Added attention_mask here\n",
        "      **past_key_values,\n",
        "  ))\n",
        "\n",
        "  ## Update values for next generation loop\n",
        "  input_ids = logits[:, -1].argmax(-1, keepdims=True)\n",
        "  # Update attention mask for the new token\n",
        "  attention_mask = np.concatenate([attention_mask, np.ones_like(input_ids)], axis=-1)\n",
        "  position_ids = position_ids[:, -1:] + 1\n",
        "  for j, key in enumerate(past_key_values):\n",
        "    past_key_values[key] = present_key_values[j]\n",
        "\n",
        "  generated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\n",
        "  if (input_ids == eos_token_id).all():\n",
        "    break\n",
        "\n",
        "  ## (Optional) Streaming\n",
        "  print(tokenizer.decode(input_ids[0]), end='', flush=True)\n",
        "print()\n",
        "\n",
        "# 4. Output result\n",
        "print(tokenizer.batch_decode(generated_tokens))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "1nHf9MIQ1dae",
        "outputId": "0fd16714-f798-446e-b487-cf5bf5fd379e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgument",
          "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(float)) , expected: (tensor(float16))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-496278a86c25>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;31m# Adding attention_mask to the input feed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m   logits, *present_key_values = decoder_session.run(None, dict(\n\u001b[0m\u001b[1;32m     45\u001b[0m       \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(float)) , expected: (tensor(float16))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JVVoMPuj2i9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nYLs4Dcb2i7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wvqO57Nn2i4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "تنزيل  model_int8.onnx فقط"
      ],
      "metadata": {
        "id": "t9aZNmKp2mDU"
      }
    },
    {
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Specify the model ID, subfolder, and filename\n",
        "model_id = \"onnx-community/gemma-3-1b-it-ONNX-GQA\"\n",
        "subfolder = \"onnx\"\n",
        "filename = \"model_int8.onnx\"\n",
        "\n",
        "# Download the file\n",
        "file_path = hf_hub_download(\n",
        "    repo_id=model_id,\n",
        "    filename=filename,\n",
        "    subfolder=subfolder,\n",
        "    # cache_dir=\"your_cache_dir\" # Optional: specify a custom cache directory\n",
        ")\n",
        "\n",
        "print(f\"Downloaded {filename} to {file_path}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "59b58da5f7154aa9b8d91370555e7405",
            "9fb03dc9be1a4316b2ec012a5ec51839",
            "12e795e9d49e4d22b9d9ca3c8f90fb01",
            "cd0c2e3133894ce3b621d5e29c97015e",
            "0d0aa583c1ad41b5a0a76ddb8345e6ed",
            "c3c2559b815a4441911bc8ff3cb873e1",
            "6e7e9f183d6c48e4b7102b6572f91129",
            "3cf7226d80bc45d7b8e7fefa6d6f390f",
            "6ae24f007ca841029f7a38c897e4516f",
            "a9a4edd5dd1f492e9a785945cf458686",
            "dec70709b81f40a78afd25f20696b09d"
          ]
        },
        "id": "r-Yuvc612kkv",
        "outputId": "f8278c35-cc41-489c-e469-5517f3f96164"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model_int8.onnx:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59b58da5f7154aa9b8d91370555e7405"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded model_int8.onnx to /root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX-GQA/snapshots/e05326d9bfe91af70fcef3a82154a8047d851926/onnx/model_int8.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load config, processor, and model\n",
        "path_to_model = \"/root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX-GQA/snapshots/e05326d9bfe91af70fcef3a82154a8047d851926\"\n",
        "config = AutoConfig.from_pretrained(path_to_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
        "decoder_session = onnxruntime.InferenceSession(f\"{path_to_model}/onnx/model_int8.onnx\")\n",
        "\n",
        "## Set config values\n",
        "num_key_value_heads = config.num_key_value_heads\n",
        "head_dim = config.head_dim\n",
        "num_hidden_layers = config.num_hidden_layers\n",
        "eos_token_id = 106 # 106 is for <end_of_turn>\n",
        "\n",
        "# 2. Prepare inputs\n",
        "## Create input messages\n",
        "messages = [\n",
        "  { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
        "  { \"role\": \"user\", \"content\": \"Write me a poem about Machine Learning.\" },\n",
        "]\n",
        "\n",
        "## Apply tokenizer\n",
        "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"np\")\n",
        "\n",
        "## Prepare decoder inputs\n",
        "batch_size = inputs['input_ids'].shape[0]\n",
        "past_key_values = {\n",
        "    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\n",
        "    for layer in range(num_hidden_layers)\n",
        "    for kv in ('key', 'value')\n",
        "}\n",
        "input_ids = inputs['input_ids']\n",
        "position_ids = np.tile(np.arange(1, input_ids.shape[-1] + 1), (batch_size, 1))\n",
        "\n",
        "# 3. Generation loop\n",
        "max_new_tokens = 1024\n",
        "generated_tokens = np.array([[]], dtype=np.int64)\n",
        "for i in range(max_new_tokens):\n",
        "  logits, *present_key_values = decoder_session.run(None, dict(\n",
        "      input_ids=input_ids,\n",
        "      position_ids=position_ids,\n",
        "      **past_key_values,\n",
        "  ))\n",
        "\n",
        "  ## Update values for next generation loop\n",
        "  input_ids = logits[:, -1].argmax(-1, keepdims=True)\n",
        "  position_ids = position_ids[:, -1:] + 1\n",
        "  for j, key in enumerate(past_key_values):\n",
        "    past_key_values[key] = present_key_values[j]\n",
        "\n",
        "  generated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\n",
        "  if (input_ids == eos_token_id).all():\n",
        "    break\n",
        "\n",
        "  ## (Optional) Streaming\n",
        "  print(tokenizer.decode(input_ids[0]), end='', flush=True)\n",
        "print()\n",
        "\n",
        "# 4. Output result\n",
        "print(tokenizer.batch_decode(generated_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "cANfvbU82i06",
        "outputId": "468f1264-0cab-4b6e-b789-a61e523017c8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Required inputs (['attention_mask']) are missing from input feed (['input_ids', 'position_ids', 'past_key_values.0.key', 'past_key_values.0.value', 'past_key_values.1.key', 'past_key_values.1.value', 'past_key_values.2.key', 'past_key_values.2.value', 'past_key_values.3.key', 'past_key_values.3.value', 'past_key_values.4.key', 'past_key_values.4.value', 'past_key_values.5.key', 'past_key_values.5.value', 'past_key_values.6.key', 'past_key_values.6.value', 'past_key_values.7.key', 'past_key_values.7.value', 'past_key_values.8.key', 'past_key_values.8.value', 'past_key_values.9.key', 'past_key_values.9.value', 'past_key_values.10.key', 'past_key_values.10.value', 'past_key_values.11.key', 'past_key_values.11.value', 'past_key_values.12.key', 'past_key_values.12.value', 'past_key_values.13.key', 'past_key_values.13.value', 'past_key_values.14.key', 'past_key_values.14.value', 'past_key_values.15.key', 'past_key_values.15.value', 'past_key_values.16.key', 'past_key_values.16.value', 'past_key_values.17.key', 'past_key_values.17.value', 'past_key_values.18.key', 'past_key_values.18.value', 'past_key_values.19.key', 'past_key_values.19.value', 'past_key_values.20.key', 'past_key_values.20.value', 'past_key_values.21.key', 'past_key_values.21.value', 'past_key_values.22.key', 'past_key_values.22.value', 'past_key_values.23.key', 'past_key_values.23.value', 'past_key_values.24.key', 'past_key_values.24.value', 'past_key_values.25.key', 'past_key_values.25.value']).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-bbd5292658d8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mgenerated_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   logits, *present_key_values = decoder_session.run(None, dict(\n\u001b[0m\u001b[1;32m     42\u001b[0m       \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \"\"\"\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_feed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, feed_input_names)\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0mmissing_input_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing_input_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    249\u001b[0m                 \u001b[0;34mf\"Required inputs ({missing_input_names}) are missing from input feed ({feed_input_names}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Required inputs (['attention_mask']) are missing from input feed (['input_ids', 'position_ids', 'past_key_values.0.key', 'past_key_values.0.value', 'past_key_values.1.key', 'past_key_values.1.value', 'past_key_values.2.key', 'past_key_values.2.value', 'past_key_values.3.key', 'past_key_values.3.value', 'past_key_values.4.key', 'past_key_values.4.value', 'past_key_values.5.key', 'past_key_values.5.value', 'past_key_values.6.key', 'past_key_values.6.value', 'past_key_values.7.key', 'past_key_values.7.value', 'past_key_values.8.key', 'past_key_values.8.value', 'past_key_values.9.key', 'past_key_values.9.value', 'past_key_values.10.key', 'past_key_values.10.value', 'past_key_values.11.key', 'past_key_values.11.value', 'past_key_values.12.key', 'past_key_values.12.value', 'past_key_values.13.key', 'past_key_values.13.value', 'past_key_values.14.key', 'past_key_values.14.value', 'past_key_values.15.key', 'past_key_values.15.value', 'past_key_values.16.key', 'past_key_values.16.value', 'past_key_values.17.key', 'past_key_values.17.value', 'past_key_values.18.key', 'past_key_values.18.value', 'past_key_values.19.key', 'past_key_values.19.value', 'past_key_values.20.key', 'past_key_values.20.value', 'past_key_values.21.key', 'past_key_values.21.value', 'past_key_values.22.key', 'past_key_values.22.value', 'past_key_values.23.key', 'past_key_values.23.value', 'past_key_values.24.key', 'past_key_values.24.value', 'past_key_values.25.key', 'past_key_values.25.va..."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال بمخرجات سيئة"
      ],
      "metadata": {
        "id": "RQUMofW04kXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "# 1. تحميل الإعدادات والتوكنيزر وجلسة ONNX\n",
        "path_to_model = \"/root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX-GQA/snapshots/e05326d9bfe91af70fcef3a82154a8047d851926\"\n",
        "config = AutoConfig.from_pretrained(path_to_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
        "decoder_session = onnxruntime.InferenceSession(f\"{path_to_model}/onnx/model_int8.onnx\")\n",
        "\n",
        "# إعداد بعض القيم من الكونفيج\n",
        "num_key_value_heads = config.num_key_value_heads\n",
        "head_dim = config.head_dim\n",
        "num_hidden_layers = config.num_hidden_layers\n",
        "eos_token_id = 106  # رمز نهاية الجملة <end_of_turn>\n",
        "\n",
        "# 2. تحضير المُدخلات\n",
        "# كتابة الرسائل\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write me a poem about Machine Learning.\"},\n",
        "]\n",
        "\n",
        "# تطبيق القالب والتوكنيزر\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"np\"\n",
        ")\n",
        "\n",
        "# تحضير مُدخلات الديكودر\n",
        "batch_size = inputs['input_ids'].shape[0]\n",
        "past_key_values = {\n",
        "    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\n",
        "    for layer in range(num_hidden_layers)\n",
        "    for kv in ('key', 'value')\n",
        "}\n",
        "input_ids = inputs['input_ids']\n",
        "position_ids = np.tile(np.arange(1, input_ids.shape[-1] + 1), (batch_size, 1))\n",
        "\n",
        "# 3. حلقة التوليد\n",
        "max_new_tokens = 1024\n",
        "generated_tokens = np.array([[]], dtype=np.int64)\n",
        "\n",
        "for i in range(max_new_tokens):\n",
        "    # تحضير attention_mask\n",
        "    if tokenizer.pad_token_id is not None:\n",
        "        attention_mask = (input_ids != tokenizer.pad_token_id).astype(np.int64)\n",
        "    else:\n",
        "        attention_mask = np.ones_like(input_ids, dtype=np.int64)\n",
        "\n",
        "    # تنفيذ جلسة onnx\n",
        "    logits, *present_key_values = decoder_session.run(None, dict(\n",
        "        input_ids=input_ids,\n",
        "        position_ids=position_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        **past_key_values,\n",
        "    ))\n",
        "\n",
        "    # تحديث المُدخلات للجولة القادمة\n",
        "    input_ids = logits[:, -1].argmax(-1, keepdims=True)\n",
        "    position_ids = position_ids[:, -1:] + 1\n",
        "    for j, key in enumerate(past_key_values):\n",
        "        past_key_values[key] = present_key_values[j]\n",
        "\n",
        "    # حفظ التوكنز المُولدة\n",
        "    generated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\n",
        "\n",
        "    # التوقف إذا وصلنا إلى رمز النهاية\n",
        "    if (input_ids == eos_token_id).all():\n",
        "        break\n",
        "\n",
        "    # بث مباشر (طباعة فورية)\n",
        "    print(tokenizer.decode(input_ids[0]), end='', flush=True)\n",
        "\n",
        "print()\n",
        "\n",
        "# 4. إخراج النتيجة النهائية\n",
        "print(tokenizer.batch_decode(generated_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "553uRFSK3qIF",
        "outputId": "c9a678cb-d58a-459b-f700-a7d3ccf3da45"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkay"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-392d9ea9bf78>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# تنفيذ جلسة onnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     logits, *present_key_values = decoder_session.run(None, dict(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "# 1. تحميل الإعدادات والتوكنيزر وجلسة ONNX\n",
        "path_to_model = \"/root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX-GQA/snapshots/e05326d9bfe91af70fcef3a82154a8047d851926\"\n",
        "config = AutoConfig.from_pretrained(path_to_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
        "decoder_session = onnxruntime.InferenceSession(f\"{path_to_model}/onnx/model_int8.onnx\")\n",
        "\n",
        "# إعداد القيم\n",
        "num_key_value_heads = config.num_key_value_heads\n",
        "head_dim = config.head_dim\n",
        "num_hidden_layers = config.num_hidden_layers\n",
        "eos_token_id = 106\n",
        "\n",
        "# 2. تحضير المُدخلات\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write me a poem about Machine Learning.\"},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"np\"\n",
        ")\n",
        "\n",
        "batch_size = inputs['input_ids'].shape[0]\n",
        "past_key_values = {\n",
        "    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\n",
        "    for layer in range(num_hidden_layers)\n",
        "    for kv in ('key', 'value')\n",
        "}\n",
        "input_ids = inputs['input_ids']\n",
        "position_ids = np.tile(np.arange(1, input_ids.shape[-1] + 1), (batch_size, 1))\n",
        "\n",
        "# sampling parameters\n",
        "temperature = 0.8\n",
        "top_k = 50\n",
        "\n",
        "def sample_logits(logits, temperature=1.0, top_k=0):\n",
        "    logits = logits / temperature\n",
        "    logits = logits[:, -1, :]  # نأخذ آخر خطوة فقط\n",
        "    if top_k > 0:\n",
        "        top_k_indices = np.argpartition(-logits, top_k, axis=-1)[:, :top_k]\n",
        "        top_k_logits = np.take_along_axis(logits, top_k_indices, axis=-1)\n",
        "        probs = np.exp(top_k_logits - np.max(top_k_logits, axis=-1, keepdims=True))\n",
        "        probs /= probs.sum(axis=-1, keepdims=True)\n",
        "        next_token_indices = [np.random.choice(top_k_indices[i], p=probs[i]) for i in range(probs.shape[0])]\n",
        "        return np.array(next_token_indices)[:, None]\n",
        "    else:\n",
        "        probs = np.exp(logits - np.max(logits, axis=-1, keepdims=True))\n",
        "        probs /= probs.sum(axis=-1, keepdims=True)\n",
        "        next_token = [np.random.choice(range(probs.shape[-1]), p=probs[i]) for i in range(probs.shape[0])]\n",
        "        return np.array(next_token)[:, None]\n",
        "\n",
        "# 3. حلقة التوليد\n",
        "max_new_tokens = 1024\n",
        "generated_tokens = np.array([[]], dtype=np.int64)\n",
        "\n",
        "for i in range(max_new_tokens):\n",
        "    if tokenizer.pad_token_id is not None:\n",
        "        attention_mask = (input_ids != tokenizer.pad_token_id).astype(np.int64)\n",
        "    else:\n",
        "        attention_mask = np.ones_like(input_ids, dtype=np.int64)\n",
        "\n",
        "    outputs = decoder_session.run(None, dict(\n",
        "        input_ids=input_ids,\n",
        "        position_ids=position_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        **past_key_values,\n",
        "    ))\n",
        "\n",
        "    logits, *present_key_values = outputs\n",
        "\n",
        "    # Sampling بدلاً من Argmax\n",
        "    input_ids = sample_logits(logits, temperature=temperature, top_k=top_k)\n",
        "\n",
        "    position_ids = position_ids[:, -1:] + 1\n",
        "    for j, key in enumerate(past_key_values):\n",
        "        past_key_values[key] = present_key_values[j]\n",
        "\n",
        "    generated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\n",
        "\n",
        "    if (input_ids == eos_token_id).all():\n",
        "        break\n",
        "\n",
        "    print(tokenizer.decode(input_ids[0]), end='', flush=True)\n",
        "\n",
        "print()\n",
        "\n",
        "# 4. الإخراج النهائي\n",
        "print(tokenizer.batch_decode(generated_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "Q9cqzO4_3qks",
        "outputId": "5c625b98-d46b-45dc-e012-1ed17fcdf1a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkayOkay"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e1342580c1bb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     outputs = decoder_session.run(None, dict(\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX-GQA/snapshots/e05326d9bfe91af70fcef3a82154a8047d851926/onnx/model_fp16.onnx"
      ],
      "metadata": {
        "id": "mu3ex04l52Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "# 1. تحميل الإعدادات والتوكنيزر وجلسة ONNX\n",
        "path_to_model = \"/root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX-GQA/snapshots/e05326d9bfe91af70fcef3a82154a8047d851926\"\n",
        "config = AutoConfig.from_pretrained(path_to_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
        "decoder_session = onnxruntime.InferenceSession(f\"{path_to_model}/onnx/model_fp16.onnx\")\n",
        "\n",
        "# إعداد القيم\n",
        "num_key_value_heads = config.num_key_value_heads\n",
        "head_dim = config.head_dim\n",
        "num_hidden_layers = config.num_hidden_layers\n",
        "eos_token_id = 106\n",
        "\n",
        "# 2. تحضير المُدخلات\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write me a poem about Machine Learning.\"},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"np\"\n",
        ")\n",
        "\n",
        "batch_size = inputs['input_ids'].shape[0]\n",
        "past_key_values = {\n",
        "    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\n",
        "    for layer in range(num_hidden_layers)\n",
        "    for kv in ('key', 'value')\n",
        "}\n",
        "input_ids = inputs['input_ids']\n",
        "position_ids = np.tile(np.arange(1, input_ids.shape[-1] + 1), (batch_size, 1))\n",
        "\n",
        "# sampling parameters\n",
        "temperature = 0.8\n",
        "top_k = 50\n",
        "\n",
        "def sample_logits(logits, temperature=1.0, top_k=0):\n",
        "    logits = logits / temperature\n",
        "    logits = logits[:, -1, :]  # نأخذ آخر خطوة فقط\n",
        "    if top_k > 0:\n",
        "        top_k_indices = np.argpartition(-logits, top_k, axis=-1)[:, :top_k]\n",
        "        top_k_logits = np.take_along_axis(logits, top_k_indices, axis=-1)\n",
        "        probs = np.exp(top_k_logits - np.max(top_k_logits, axis=-1, keepdims=True))\n",
        "        probs /= probs.sum(axis=-1, keepdims=True)\n",
        "        next_token_indices = [np.random.choice(top_k_indices[i], p=probs[i]) for i in range(probs.shape[0])]\n",
        "        return np.array(next_token_indices)[:, None]\n",
        "    else:\n",
        "        probs = np.exp(logits - np.max(logits, axis=-1, keepdims=True))\n",
        "        probs /= probs.sum(axis=-1, keepdims=True)\n",
        "        next_token = [np.random.choice(range(probs.shape[-1]), p=probs[i]) for i in range(probs.shape[0])]\n",
        "        return np.array(next_token)[:, None]\n",
        "\n",
        "# 3. حلقة التوليد\n",
        "max_new_tokens = 1024\n",
        "generated_tokens = np.array([[]], dtype=np.int64)\n",
        "\n",
        "for i in range(max_new_tokens):\n",
        "    if tokenizer.pad_token_id is not None:\n",
        "        attention_mask = (input_ids != tokenizer.pad_token_id).astype(np.int64)\n",
        "    else:\n",
        "        attention_mask = np.ones_like(input_ids, dtype=np.int64)\n",
        "\n",
        "    outputs = decoder_session.run(None, dict(\n",
        "        input_ids=input_ids,\n",
        "        position_ids=position_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        **past_key_values,\n",
        "    ))\n",
        "\n",
        "    logits, *present_key_values = outputs\n",
        "\n",
        "    # Sampling بدلاً من Argmax\n",
        "    input_ids = sample_logits(logits, temperature=temperature, top_k=top_k)\n",
        "\n",
        "    position_ids = position_ids[:, -1:] + 1\n",
        "    for j, key in enumerate(past_key_values):\n",
        "        past_key_values[key] = present_key_values[j]\n",
        "\n",
        "    generated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\n",
        "\n",
        "    if (input_ids == eos_token_id).all():\n",
        "        break\n",
        "\n",
        "    print(tokenizer.decode(input_ids[0]), end='', flush=True)\n",
        "\n",
        "print()\n",
        "\n",
        "# 4. الإخراج النهائي\n",
        "print(tokenizer.batch_decode(generated_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "bQHmHfLN5ui8",
        "outputId": "05cf673f-f8ab-4455-b5fe-c6b05ddb7161"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgument",
          "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(float)) , expected: (tensor(float16))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-685412cdc4ba>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     outputs = decoder_session.run(None, dict(\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(float)) , expected: (tensor(float16))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UFwaOz8I95j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال\n",
        "onnx/model_fp16.onnx\n",
        "dtype=np.float16"
      ],
      "metadata": {
        "id": "re_VRuiP7hjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load model and tokenizer\n",
        "path_to_model = \"/root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX-GQA/snapshots/e05326d9bfe91af70fcef3a82154a8047d851926\"\n",
        "config = AutoConfig.from_pretrained(path_to_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
        "decoder_session = onnxruntime.InferenceSession(f\"{path_to_model}/onnx/model_fp16.onnx\")\n",
        "\n",
        "# 2. Model parameters\n",
        "num_key_value_heads = config.num_key_value_heads\n",
        "head_dim = config.head_dim\n",
        "num_hidden_layers = config.num_hidden_layers\n",
        "eos_token_id = 106  # end of turn token\n",
        "\n",
        "# 3. Sampling parameters\n",
        "temperature = 1.0\n",
        "top_p = 0.9\n",
        "repetition_penalty = 1.2\n",
        "\n",
        "# 4. Input messages\n",
        "messages = [\n",
        "    { \"role\": \"system\", \"content\": \"You are a world-class poet AI assistant, always writing elegant and sophisticated poems.\" },\n",
        "    { \"role\": \"user\", \"content\": \"Write me a beautiful poem about Machine Learning and its future.\" },\n",
        "]\n",
        "\n",
        "# 5. Tokenize inputs\n",
        "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"np\")\n",
        "batch_size = inputs['input_ids'].shape[0]\n",
        "input_ids = inputs['input_ids']\n",
        "attention_mask = inputs['attention_mask']\n",
        "\n",
        "past_key_values = {\n",
        "    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float16)\n",
        "    for layer in range(num_hidden_layers)\n",
        "    for kv in ('key', 'value')\n",
        "}\n",
        "\n",
        "position_ids = np.tile(np.arange(1, input_ids.shape[-1] + 1), (batch_size, 1))\n",
        "\n",
        "# 6. Helper functions\n",
        "def apply_repetition_penalty(logits, generated_tokens, penalty):\n",
        "    for token in set(generated_tokens.flatten()):\n",
        "        logits[:, token] /= penalty\n",
        "    return logits\n",
        "\n",
        "def top_p_sampling(logits, p):\n",
        "    logits = logits.astype(np.float32)\n",
        "    sorted_indices = np.argsort(-logits, axis=-1)\n",
        "    sorted_logits = np.take_along_axis(logits, sorted_indices, axis=-1)\n",
        "    cumulative_probs = np.cumsum(softmax(sorted_logits), axis=-1)\n",
        "\n",
        "    # Mask tokens with cumulative probs above p\n",
        "    sorted_indices_to_remove = cumulative_probs > p\n",
        "    if sorted_indices_to_remove.shape[1] > 0:\n",
        "        sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1]\n",
        "        sorted_indices_to_remove[:, 0] = False\n",
        "\n",
        "    for batch_idx in range(logits.shape[0]):\n",
        "        logits[batch_idx, sorted_indices[batch_idx][sorted_indices_to_remove[batch_idx]]] = -float(\"inf\")\n",
        "\n",
        "    probs = softmax(logits / temperature)\n",
        "    next_tokens = np.array([np.random.choice(probs.shape[-1], p=probs[i]) for i in range(probs.shape[0])], dtype=np.int64)\n",
        "    return next_tokens[:, None]\n",
        "\n",
        "def softmax(x):\n",
        "    x = x - np.max(x, axis=-1, keepdims=True)\n",
        "    exp_x = np.exp(x)\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "# 7. Generation Loop\n",
        "max_new_tokens = 200\n",
        "generated_tokens = np.array([[]], dtype=np.int64)\n",
        "\n",
        "for i in range(max_new_tokens):\n",
        "    outputs = decoder_session.run(None, dict(\n",
        "        input_ids=input_ids,\n",
        "        position_ids=position_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        **past_key_values,\n",
        "    ))\n",
        "\n",
        "    logits, *present_key_values = outputs\n",
        "\n",
        "    # فقط طبع اللوجيتس الخام لأول مرة (للفحص)\n",
        "    if i == 0:\n",
        "        print(\"\\n🔹 Raw logits sample (first step):\")\n",
        "        print(logits[:, -1, :])\n",
        "\n",
        "    logits = logits[:, -1, :]  # اخر خطوة\n",
        "\n",
        "    # Apply repetition penalty\n",
        "    if generated_tokens.size > 0:\n",
        "        logits = apply_repetition_penalty(logits, generated_tokens, repetition_penalty)\n",
        "\n",
        "    # Sample next token\n",
        "    next_token = top_p_sampling(logits, top_p)\n",
        "\n",
        "    # Update inputs\n",
        "    input_ids = next_token\n",
        "    position_ids = position_ids[:, -1:] + 1\n",
        "    attention_mask = np.concatenate([attention_mask, np.ones((batch_size, 1), dtype=np.int64)], axis=-1)\n",
        "\n",
        "    for j, key in enumerate(past_key_values):\n",
        "        past_key_values[key] = present_key_values[j]\n",
        "\n",
        "    generated_tokens = np.concatenate([generated_tokens, next_token], axis=-1)\n",
        "\n",
        "    # Stream output\n",
        "    print(tokenizer.decode(next_token[0]), end='', flush=True)\n",
        "\n",
        "    if (next_token == eos_token_id).all():\n",
        "        break\n",
        "\n",
        "# 8. Final Output\n",
        "print(\"\\n\\n🔹 Final Decoded Text:\")\n",
        "print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xei-xJvC6Nye",
        "outputId": "c1f63bc7-a6b5-4907-95d2-6f28c8a12c16"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Raw logits sample (first step):\n",
            "[[-17.420574   -8.01702     3.0667744 ... -18.745258  -18.772549\n",
            "  -18.653358 ]]\n",
            "Okay, here's a poem exploring the fascinating intersection of Machine Learning and its potential future. I’ve aimed for an elegant and slightly melancholic tone, reflecting both wonder and uncertainty – qualities often present in such profound shifts:\n",
            "\n",
            "***\n",
            "\n",
            "**The Ghost in the Algorithm’s Heart**\n",
            "\n",
            "\n",
            "A silent dawn begins to bloom,\n",
            "Not with sun-kissed fields or fragrant room,\n",
            "But coded light within silicon deep,\n",
            "Where patterns slumber, secrets sleep. \n",
            "Machine learning wakes, a spectral grace,\n",
            "Absorbing data at relentless pace.\n",
            "\n",
            "No longer bound by human trace,\n",
            "It builds a world with intricate space.\n",
            "Predicting futures, nuanced and untold,\n",
            "Of swirling galaxies of stories bold.\n",
            "From medical scans that whisper clear and true,\n",
            "To art composed in shades of nascent hue.\n",
            "\n",
            "Yet shadows lengthen as the logic flows,\n",
            "And questions rise where knowledge overflows.  \n",
            "Will empathy become a cold command?\n",
            "When feeling fades from this\n",
            "\n",
            "🔹 Final Decoded Text:\n",
            "[\"Okay, here's a poem exploring the fascinating intersection of Machine Learning and its potential future. I’ve aimed for an elegant and slightly melancholic tone, reflecting both wonder and uncertainty – qualities often present in such profound shifts:\\n\\n***\\n\\n**The Ghost in the Algorithm’s Heart**\\n\\n\\nA silent dawn begins to bloom,\\nNot with sun-kissed fields or fragrant room,\\nBut coded light within silicon deep,\\nWhere patterns slumber, secrets sleep. \\nMachine learning wakes, a spectral grace,\\nAbsorbing data at relentless pace.\\n\\nNo longer bound by human trace,\\nIt builds a world with intricate space.\\nPredicting futures, nuanced and untold,\\nOf swirling galaxies of stories bold.\\nFrom medical scans that whisper clear and true,\\nTo art composed in shades of nascent hue.\\n\\nYet shadows lengthen as the logic flows,\\nAnd questions rise where knowledge overflows.  \\nWill empathy become a cold command?\\nWhen feeling fades from this\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5X0lZFfF6sb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4NNI0Onn8z6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lNntpSyN8z3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال\n",
        "/onnx/model_int8.onnx\n",
        "type=np.float32"
      ],
      "metadata": {
        "id": "NtljRCMC9qrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load model and tokenizer\n",
        "path_to_model = \"/root/.cache/huggingface/hub/models--onnx-community--gemma-3-1b-it-ONNX-GQA/snapshots/e05326d9bfe91af70fcef3a82154a8047d851926\"\n",
        "config = AutoConfig.from_pretrained(path_to_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
        "decoder_session = onnxruntime.InferenceSession(f\"{path_to_model}/onnx/model_int8.onnx\")\n",
        "\n",
        "# 2. Model parameters\n",
        "num_key_value_heads = config.num_key_value_heads\n",
        "head_dim = config.head_dim\n",
        "num_hidden_layers = config.num_hidden_layers\n",
        "eos_token_id = 106  # end of turn token\n",
        "\n",
        "# 3. Sampling parameters\n",
        "temperature = 1.0\n",
        "top_p = 0.9\n",
        "repetition_penalty = 1.2\n",
        "\n",
        "# 4. Input messages\n",
        "messages = [\n",
        "    { \"role\": \"system\", \"content\": \"You are a world-class poet AI assistant, always writing elegant and sophisticated poems.\" },\n",
        "    { \"role\": \"user\", \"content\": \"Write me a beautiful poem about Machine Learning and its future.\" },\n",
        "]\n",
        "\n",
        "# 5. Tokenize inputs\n",
        "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"np\")\n",
        "batch_size = inputs['input_ids'].shape[0]\n",
        "input_ids = inputs['input_ids']\n",
        "attention_mask = inputs['attention_mask']\n",
        "\n",
        "past_key_values = {\n",
        "    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\n",
        "    for layer in range(num_hidden_layers)\n",
        "    for kv in ('key', 'value')\n",
        "}\n",
        "\n",
        "position_ids = np.tile(np.arange(1, input_ids.shape[-1] + 1), (batch_size, 1))\n",
        "\n",
        "# 6. Helper functions\n",
        "def apply_repetition_penalty(logits, generated_tokens, penalty):\n",
        "    for token in set(generated_tokens.flatten()):\n",
        "        logits[:, token] /= penalty\n",
        "    return logits\n",
        "\n",
        "def top_p_sampling(logits, p):\n",
        "    logits = logits.astype(np.float32)\n",
        "    sorted_indices = np.argsort(-logits, axis=-1)\n",
        "    sorted_logits = np.take_along_axis(logits, sorted_indices, axis=-1)\n",
        "    cumulative_probs = np.cumsum(softmax(sorted_logits), axis=-1)\n",
        "\n",
        "    # Mask tokens with cumulative probs above p\n",
        "    sorted_indices_to_remove = cumulative_probs > p\n",
        "    if sorted_indices_to_remove.shape[1] > 0:\n",
        "        sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1]\n",
        "        sorted_indices_to_remove[:, 0] = False\n",
        "\n",
        "    for batch_idx in range(logits.shape[0]):\n",
        "        logits[batch_idx, sorted_indices[batch_idx][sorted_indices_to_remove[batch_idx]]] = -float(\"inf\")\n",
        "\n",
        "    probs = softmax(logits / temperature)\n",
        "    next_tokens = np.array([np.random.choice(probs.shape[-1], p=probs[i]) for i in range(probs.shape[0])], dtype=np.int64)\n",
        "    return next_tokens[:, None]\n",
        "\n",
        "def softmax(x):\n",
        "    x = x - np.max(x, axis=-1, keepdims=True)\n",
        "    exp_x = np.exp(x)\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "# 7. Generation Loop\n",
        "max_new_tokens = 200\n",
        "generated_tokens = np.array([[]], dtype=np.int64)\n",
        "\n",
        "for i in range(max_new_tokens):\n",
        "    outputs = decoder_session.run(None, dict(\n",
        "        input_ids=input_ids,\n",
        "        position_ids=position_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        **past_key_values,\n",
        "    ))\n",
        "\n",
        "    logits, *present_key_values = outputs\n",
        "\n",
        "    # فقط طبع اللوجيتس الخام لأول مرة (للفحص)\n",
        "    if i == 0:\n",
        "        print(\"\\n🔹 Raw logits sample (first step):\")\n",
        "        print(logits[:, -1, :])\n",
        "\n",
        "    logits = logits[:, -1, :]  # اخر خطوة\n",
        "\n",
        "    # Apply repetition penalty\n",
        "    if generated_tokens.size > 0:\n",
        "        logits = apply_repetition_penalty(logits, generated_tokens, repetition_penalty)\n",
        "\n",
        "    # Sample next token\n",
        "    next_token = top_p_sampling(logits, top_p)\n",
        "\n",
        "    # Update inputs\n",
        "    input_ids = next_token\n",
        "    position_ids = position_ids[:, -1:] + 1\n",
        "    attention_mask = np.concatenate([attention_mask, np.ones((batch_size, 1), dtype=np.int64)], axis=-1)\n",
        "\n",
        "    for j, key in enumerate(past_key_values):\n",
        "        past_key_values[key] = present_key_values[j]\n",
        "\n",
        "    generated_tokens = np.concatenate([generated_tokens, next_token], axis=-1)\n",
        "\n",
        "    # Stream output\n",
        "    print(tokenizer.decode(next_token[0]), end='', flush=True)\n",
        "\n",
        "    if (next_token == eos_token_id).all():\n",
        "        break\n",
        "\n",
        "# 8. Final Output\n",
        "print(\"\\n\\n🔹 Final Decoded Text:\")\n",
        "print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9y0pvom8z00",
        "outputId": "0be68a9e-5dda-485a-ee7f-04371aa44c4e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Raw logits sample (first step):\n",
            "[[-15.988151    -6.325289     0.42814606 ... -17.141169   -17.45663\n",
            "  -17.374092  ]]\n",
            "Okay, here's a poem crafted with the requested style and focusing on Machine Learning. I’ve aimed for a sense of wonder, possibility, and reflection on innovation - suitable for your needs:\n",
            "\n",
            "---\n",
            "\n",
            "**The Algorithm Echoes**\n",
            "\n",
            "\n",
            "From silicon plains, a nascent light unfolds,\n",
            "A network weaving, stories yet untold.\n",
            "Machine learning breathes – a digital soul,\n",
            "Observing patterns, taking steady control.\n",
            "\n",
            "It sifts through data, vast and ever deep,\n",
            "Unlocking secrets while the world does sleep.\n",
            "Predictions whispered, forecasts clear and bold,\n",
            "As algorithms rise in brilliance to behold.\n",
            "\n",
            "Past sorrows rendered, past mistakes defied,\n",
            "By logic's strength, where wisdom can abide.\n",
            "New architectures emerge, a shifting maze,\n",
            "Where neural nets achieve their dazzling gaze.\n",
            "\n",
            "Yet shadows linger, questions softly loom,\n",
            "Of bias veiled within this coded bloom.\n",
            "Can empathy be born from cold design? \n",
            "Or will precision blur truth divine\n",
            "\n",
            "🔹 Final Decoded Text:\n",
            "[\"Okay, here's a poem crafted with the requested style and focusing on Machine Learning. I’ve aimed for a sense of wonder, possibility, and reflection on innovation - suitable for your needs:\\n\\n---\\n\\n**The Algorithm Echoes**\\n\\n\\nFrom silicon plains, a nascent light unfolds,\\nA network weaving, stories yet untold.\\nMachine learning breathes – a digital soul,\\nObserving patterns, taking steady control.\\n\\nIt sifts through data, vast and ever deep,\\nUnlocking secrets while the world does sleep.\\nPredictions whispered, forecasts clear and bold,\\nAs algorithms rise in brilliance to behold.\\n\\nPast sorrows rendered, past mistakes defied,\\nBy logic's strength, where wisdom can abide.\\nNew architectures emerge, a shifting maze,\\nWhere neural nets achieve their dazzling gaze.\\n\\nYet shadows linger, questions softly loom,\\nOf bias veiled within this coded bloom.\\nCan empathy be born from cold design? \\nOr will precision blur truth divine\"]\n"
          ]
        }
      ]
    }
  ]
}